Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {
  "attention_probs_dropout_prob": 0.0,
  "depths": [
    2,
    2,
    14,
    2
  ],
  "drop_path_rate": 0.1,
  "embed_dim": 128,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "image_size": [
    2560,
    1920
  ],
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-05,
  "mlp_ratio": 4.0,
  "model_type": "donut-swin",
  "num_channels": 3,
  "num_heads": [
    4,
    8,
    16,
    32
  ],
  "num_layers": 4,
  "patch_size": 4,
  "path_norm": true,
  "qkv_bias": true,
  "torch_dtype": "float32",
  "transformers_version": "4.51.3",
  "use_absolute_embeddings": false,
  "window_size": 10
}

Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_cross_attention": true,
  "add_final_layer_norm": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "init_std": 0.02,
  "is_decoder": true,
  "is_encoder_decoder": false,
  "max_position_embeddings": 1536,
  "model_type": "mbart",
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "scale_embedding": true,
  "torch_dtype": "float32",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 57525
}

Zero-shot inference:   0%|          | 0/24 [00:00<?, ?it/s]Zero-shot inference:   4%|▍         | 1/24 [00:15<05:56, 15.52s/it]Zero-shot inference:   8%|▊         | 2/24 [00:21<03:43, 10.18s/it]Zero-shot inference:  12%|█▎        | 3/24 [00:26<02:40,  7.64s/it]Zero-shot inference:  17%|█▋        | 4/24 [00:35<02:42,  8.12s/it]Zero-shot inference:  21%|██        | 5/24 [00:42<02:26,  7.72s/it]Zero-shot inference:  25%|██▌       | 6/24 [00:53<02:39,  8.84s/it]Zero-shot inference:  29%|██▉       | 7/24 [00:55<01:53,  6.68s/it]Zero-shot inference:  33%|███▎      | 8/24 [01:04<01:58,  7.42s/it]Zero-shot inference:  38%|███▊      | 9/24 [01:10<01:41,  6.77s/it]Zero-shot inference:  42%|████▏     | 10/24 [01:17<01:38,  7.00s/it]Zero-shot inference:  46%|████▌     | 11/24 [01:23<01:26,  6.68s/it]Zero-shot inference:  50%|█████     | 12/24 [01:29<01:18,  6.52s/it]Zero-shot inference:  54%|█████▍    | 13/24 [01:44<01:38,  8.99s/it]Zero-shot inference:  58%|█████▊    | 14/24 [01:50<01:20,  8.08s/it]Zero-shot inference:  62%|██████▎   | 15/24 [01:52<00:57,  6.34s/it]Zero-shot inference:  67%|██████▋   | 16/24 [01:58<00:49,  6.23s/it]Zero-shot inference:  71%|███████   | 17/24 [02:04<00:43,  6.22s/it]Zero-shot inference:  75%|███████▌  | 18/24 [02:14<00:43,  7.21s/it]Zero-shot inference:  79%|███████▉  | 19/24 [02:19<00:33,  6.63s/it]Zero-shot inference:  83%|████████▎ | 20/24 [02:25<00:25,  6.29s/it]Zero-shot inference:  88%|████████▊ | 21/24 [02:28<00:16,  5.54s/it]Zero-shot inference:  92%|█████████▏| 22/24 [02:37<00:12,  6.33s/it]Zero-shot inference:  96%|█████████▌| 23/24 [02:42<00:06,  6.17s/it]Zero-shot inference: 100%|██████████| 24/24 [02:44<00:00,  4.74s/it]Zero-shot inference: 100%|██████████| 24/24 [02:44<00:00,  6.84s/it]
