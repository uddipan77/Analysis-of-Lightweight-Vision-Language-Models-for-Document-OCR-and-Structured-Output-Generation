### Starting TaskPrologue of job 1198773 on tg080 at Tue Sep  2 01:57:11 PM CEST 2025
Running on cores 14-15,30-31,46-47,62-63 with governor powersave
Tue Sep  2 13:57:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:DB:00.0 Off |                  N/A |
| 30%   35C    P8             14W /  300W |       1MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

===================================
DONUT Fine-tuning Configuration:
===================================
Job ID: 1198773
Log Directory: /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/logs/DONUT/1198773
Data Directory: /home/woody/iwi5/iwi5298h/json_inven
Image Directory: /home/woody/iwi5/iwi5298h/inventory_images
Model Cache Directory: /home/vault/iwi5/iwi5298h/models/donut_base
Output Directory: /home/vault/iwi5/iwi5298h/models_image/donut_base/inventory/inventory_data_base_20250902_135713
Num Epochs: 50
Batch Size: 2
Learning Rate: 1e-5
Max Length: 512
Prediction On: test
Prediction Batch Size: 2
Early Stopping Patience: 5
Early Stopping Threshold: 0.00
===================================
[2025-09-02 13:57:29,711] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/iwi5/iwi5298h/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-02 13:57:32,920] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using device: cuda
All output will be saved in: /home/vault/iwi5/iwi5298h/models_image/donut_base/inventory/autoregressive_inventory_data_20250902_135734
Checking for model at: /home/vault/iwi5/iwi5298h/models/donut_base
Model already exists at /home/vault/iwi5/iwi5298h/models/donut_base, loading from local cache...
Loaded 213 samples from /home/woody/iwi5/iwi5298h/json_inven/train.jsonl
Loaded 47 samples from /home/woody/iwi5/iwi5298h/json_inven/val.jsonl
Loaded 48 samples from /home/woody/iwi5/iwi5298h/json_inven/test.jsonl
Starting AUTOREGRESSIVE training...
================================================================================
DONUT AUTOREGRESSIVE TRAINING MODE (FIXED)
================================================================================
✅ Training uses NO teacher forcing - decoder uses only its own predictions
✅ Ground truth text used ONLY for loss computation, not as decoder input
✅ 100% reliance on image features during training
✅ Fixed input handling for Donut encoder/decoder separation
✅ Model selection based on: eval_cer_string (lower is better)
================================================================================
===================================
Training failed with exit code: 1
Check logs for errors.
Output directory: /home/vault/iwi5/iwi5298h/models_image/donut_base/inventory/inventory_data_base_20250902_135713
Log directory: /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/logs/DONUT/1198773
Job completed at: Tue Sep  2 01:58:32 PM CEST 2025
=== JOB_STATISTICS ===
=== current date     : Tue Sep  2 01:58:32 PM CEST 2025
= Job-ID             : 1198773 on tinygpu
= Job-Name           : donut_base
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_donut2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : rtx3080
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 15:00:00
= Elapsed runtime    : 00:01:48
= Total RAM usage    : 3.5 GiB of requested  GiB (%)   
= Node list          : tg080
= Subm/Elig/Start/End: 2025-09-02T13:56:43 / 2025-09-02T13:56:43 / 2025-09-02T13:56:44 / 2025-09-02T13:58:32
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              26.0G   104.9G   209.7G        N/A  32,290      500K   1,000K        N/A    
    /home/woody            46.7G  1000.0G  1500.0G        N/A     238K   5,000K   7,500K        N/A    
    /home/vault           767.8G  1048.6G  2097.2G        N/A   1,993      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 1949444, 56 %, 16 %, 9860 MiB, 62727 ms
