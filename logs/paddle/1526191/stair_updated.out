### Starting TaskPrologue of job 1526191 on tg072 at Sat Feb  7 03:03:41 PM CET 2026
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Sat Feb  7 15:03:41 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.211.01             Driver Version: 570.211.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   44C    P0             29W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Configured torch.backends.cuda.sdp_kernel(mem_efficient=True, math=True, flash=False)

============================================================
PADDLEOCR-VL FINETUNING - STAIRCASE DATASET
BEST MODEL SELECTION: CER(GT JSON string vs prediction_stripped_for_cer)
============================================================
Model: PaddlePaddle/PaddleOCR-VL
Local cache: /home/vault/iwi5/iwi5298h/models/PaddleOCR-VL
Output run dir: /home/vault/iwi5/iwi5298h/models_image_text/paddleocr/stair/run_20260207_150404_cer
Prompt task: ocr -> 'OCR:'
Attention impl override: None
============================================================

Using device: cuda
Loading PaddleOCR-VL from local cache: /home/vault/iwi5/iwi5298h/models/PaddleOCR-VL
Enabled gradient checkpointing (use_reentrant=False).
Disabled use_cache for training.
Loaded 115 samples from /home/woody/iwi5/iwi5298h/json_staircase/train.jsonl
Loaded 25 samples from /home/woody/iwi5/iwi5298h/json_staircase/val.jsonl
Loaded 24 samples from /home/woody/iwi5/iwi5298h/json_staircase/test.jsonl

============================================================
Epoch 1/10
============================================================

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.

⚠️ CUDA OOM during backward. Skipping this batch and clearing cache.
Epoch 1 - train loss: 0.0000

Running validation (CER-based model selection)...
Paddle training completed at: Sat Feb  7 03:08:39 PM CET 2026
=== JOB_STATISTICS ===
=== current date     : Sat Feb  7 03:08:39 PM CET 2026
= Job-ID             : 1526191 on tinygpu
= Job-Name           : stair_updated
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_paddle2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:04:59
= Total RAM usage    : 6.5 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2026-02-07T12:56:51 / 2026-02-07T12:56:51 / 2026-02-07T15:03:40 / 2026-02-07T15:08:39
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           408.1G  1000.0G  1500.0G        N/A   1,018K   5,000K   7,500K        N/A    
    /home/hpc              91.3G   104.9G   209.7G        N/A  29,981      500K   1,000K        N/A    
    /home/vault           910.0G  1048.6G  2097.2G        N/A   9,300      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 3057563, 60 %, 10 %, 32490 MiB, 274771 ms
