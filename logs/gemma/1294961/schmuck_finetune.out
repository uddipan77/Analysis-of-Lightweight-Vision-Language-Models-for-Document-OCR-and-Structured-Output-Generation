### Starting TaskPrologue of job 1294961 on tg090 at Sun Nov  2 07:04:38 PM CET 2025
Running on cores 0-31 with governor ondemand
Sun Nov  2 19:04:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   45C    P0             58W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/gemma/schmuck/finetune/run_A100_20251102_190506

============================================================
GEMMA-3 SCHMUCK - A100 OPTIMIZED
============================================================
Loading Gemma-3 vision model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Gemma3 does not support SDPA - switching to eager!
Model loaded - Unsloth auto-configured dtype and attention
Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients
Gemma-3 vision model loaded with A100-optimized LoRA config

============================================================
STARTING TRAINING (A100-OPTIMIZED)
============================================================
Preparing training and validation datasets...
Training: 413 samples, Validation: 88 samples
GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.494 GB.
4.809 GB of memory reserved.
Starting training with CER-based evaluation (A100-optimized)...
{'loss': 14.1868, 'grad_norm': 257.10748291015625, 'learning_rate': 0.0, 'epoch': 0.04}
{'loss': 14.078, 'grad_norm': 400.4818115234375, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.19}
{'loss': 13.6162, 'grad_norm': 331.91387939453125, 'learning_rate': 2.7e-06, 'epoch': 0.39}
{'loss': 10.7605, 'grad_norm': 201.92642211914062, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.58}
{'loss': 7.5958, 'grad_norm': 83.60795593261719, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.77}
{'loss': 5.3417, 'grad_norm': 47.82322311401367, 'learning_rate': 7.2e-06, 'epoch': 0.97}
Gemma-3 training completed at: Sun Nov  2 07:13:52 PM CET 2025
=== JOB_STATISTICS ===
=== current date     : Sun Nov  2 07:13:52 PM CET 2025
= Job-ID             : 1294961 on tinygpu
= Job-Name           : schmuck
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_gemma2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 15:00:00
= Elapsed runtime    : 00:09:17
= Total RAM usage    : 7.9 GiB of requested  GiB (%)   
= Node list          : tg090
= Subm/Elig/Start/End: 2025-11-02T16:59:31 / 2025-11-02T16:59:31 / 2025-11-02T19:04:35 / 2025-11-02T19:13:52
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc             102.3G   104.9G   209.7G        N/A  30,466      500K   1,000K        N/A    
    /home/woody           184.1G  1000.0G  1500.0G        N/A     841K   5,000K   7,500K        N/A    
    /home/vault           850.9G  1048.6G  2097.2G        N/A   4,692      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 2466966, 35 %, 14 %, 12530 MiB, 546906 ms
