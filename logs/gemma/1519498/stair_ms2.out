### Starting TaskPrologue of job 1519498 on tg097 at Fri Jan 30 03:50:02 PM CET 2026
Running on cores 96-127 with governor ondemand
Fri Jan 30 15:50:02 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   37C    P0             56W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035

============================================================
GEMMA-3 STAIRCASE - MULTI-STAGE + BEST BY AUTOREGRESSIVE GEN-CER (SAVE ONLY ONE)
============================================================
Loading Gemma-3 vision model with Unsloth for STAIRCASE...
Temporary directory for augmented images: /tmp/1519498.tinygpu/stair_gemma_aug_oqdfjorx
Augmentation factor (per original image): 1
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.493 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Gemma3 does not support SDPA - switching to eager!
Model loaded - Unsloth auto-configured dtype and attention
Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients
Gemma-3 vision model loaded with A100-optimized LoRA config

============================================================
STARTING MULTI-STAGE TRAINING
============================================================
Preparing training and validation datasets for STAIRCASE...
Preparing staircase TRAIN data with augmentation factor = 1
Training: 230 samples (original: 115, augmented: 115)
Validation/Test: 25 valid samples out of 25 total (no augmentation)
Training: 230 samples, Validation: 25 samples

======================================================================
STAGE 1: Warm-up training (no eval, teacher forcing)
======================================================================

{'loss': 27.6028, 'grad_norm': 282.18310546875, 'learning_rate': 0.0, 'epoch': 0.07}
{'loss': 27.0313, 'grad_norm': 292.2550354003906, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.35}
{'loss': 21.2164, 'grad_norm': 108.50241088867188, 'learning_rate': 1.62e-05, 'epoch': 0.7}
{'loss': 14.4812, 'grad_norm': 18.219640731811523, 'learning_rate': 2.5200000000000003e-05, 'epoch': 1.0}
{'loss': 13.189, 'grad_norm': 26.18245506286621, 'learning_rate': 3.4200000000000005e-05, 'epoch': 1.35}
{'loss': 10.581, 'grad_norm': 17.670888900756836, 'learning_rate': 4.32e-05, 'epoch': 1.7}
{'loss': 7.8593, 'grad_norm': 9.78054428100586, 'learning_rate': 5.22e-05, 'epoch': 2.0}
{'loss': 8.06, 'grad_norm': 11.624561309814453, 'learning_rate': 6.120000000000001e-05, 'epoch': 2.35}
{'loss': 7.7126, 'grad_norm': 7.555846691131592, 'learning_rate': 7.020000000000001e-05, 'epoch': 2.7}
{'loss': 6.5721, 'grad_norm': 9.418025970458984, 'learning_rate': 7.92e-05, 'epoch': 3.0}
{'loss': 7.2712, 'grad_norm': 6.505249977111816, 'learning_rate': 8.82e-05, 'epoch': 3.35}
{'loss': 7.1815, 'grad_norm': 3.180363178253174, 'learning_rate': 2.250000000000001e-05, 'epoch': 3.7}
{'train_runtime': 1347.2484, 'train_samples_per_second': 0.683, 'train_steps_per_second': 0.042, 'train_loss': 11.847387620380946, 'epoch': 3.77}

======================================================================
STAGE 2: Main training (eval each epoch, BEST by autoregressive gen-CER)
======================================================================

[GenCER Callback] Fixed val subset size = 25
GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.493 GB.
23.309 GB of memory reserved.
Starting Stage 2 training (Trainer selects BEST by eval_gen_cer)...
{'loss': 7.1409, 'grad_norm': 6.5385870933532715, 'learning_rate': 0.0, 'epoch': 0.07}
{'loss': 7.0624, 'grad_norm': 4.4161152839660645, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.35}
{'loss': 7.076, 'grad_norm': 5.351962089538574, 'learning_rate': 5.4e-06, 'epoch': 0.7}
{'loss': 6.1733, 'grad_norm': 2.3583672046661377, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.0}
{'eval_loss': 0.902915358543396, 'eval_runtime': 23.9382, 'eval_samples_per_second': 1.044, 'eval_steps_per_second': 1.044, 'epoch': 1.0}

[GenCER Callback] Epoch 1.00 gen_cer = 0.7966 (79.66%)
{'loss': 7.0248, 'grad_norm': 8.735763549804688, 'learning_rate': 1.1400000000000001e-05, 'epoch': 1.35}
{'loss': 7.0106, 'grad_norm': 3.5817601680755615, 'learning_rate': 1.44e-05, 'epoch': 1.7}
{'loss': 6.1202, 'grad_norm': 1.860846757888794, 'learning_rate': 1.74e-05, 'epoch': 2.0}
{'eval_loss': 0.9000135660171509, 'eval_runtime': 21.3099, 'eval_samples_per_second': 1.173, 'eval_steps_per_second': 1.173, 'epoch': 2.0}

[GenCER Callback] Epoch 2.00 gen_cer = 0.7953 (79.53%)
{'loss': 6.9551, 'grad_norm': 3.6405560970306396, 'learning_rate': 2.04e-05, 'epoch': 2.35}
{'loss': 6.9276, 'grad_norm': 4.5059309005737305, 'learning_rate': 2.3400000000000003e-05, 'epoch': 2.7}
{'loss': 6.0591, 'grad_norm': 4.024653434753418, 'learning_rate': 2.64e-05, 'epoch': 3.0}
{'eval_loss': 0.9008574485778809, 'eval_runtime': 21.2485, 'eval_samples_per_second': 1.177, 'eval_steps_per_second': 1.177, 'epoch': 3.0}

[GenCER Callback] Epoch 3.00 gen_cer = 0.7967 (79.67%)
{'loss': 6.8782, 'grad_norm': 3.5613434314727783, 'learning_rate': 2.94e-05, 'epoch': 3.35}
{'loss': 6.8682, 'grad_norm': 3.517524480819702, 'learning_rate': 2.969294911878742e-05, 'epoch': 3.7}
{'loss': 5.9887, 'grad_norm': 1.309836983680725, 'learning_rate': 2.8467068093561125e-05, 'epoch': 4.0}
{'eval_loss': 0.9023732542991638, 'eval_runtime': 24.2756, 'eval_samples_per_second': 1.03, 'eval_steps_per_second': 1.03, 'epoch': 4.0}

[GenCER Callback] Epoch 4.00 gen_cer = 0.7999 (79.99%)
{'train_runtime': 10359.775, 'train_samples_per_second': 0.178, 'train_steps_per_second': 0.011, 'train_loss': 6.679982701937358, 'epoch': 4.0}

=== BEST CHECKPOINT INFO (Stage 2) ===
Best checkpoint: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035/stage2/checkpoint-30
Best metric (gen_cer): 0.7953268072039205

Saving ONLY best model to: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035/best_model_gen_cer

Cleaning up: removing stage2 checkpoint-* folders (keeping only best_model_gen_cer)...
10359.775 seconds used for training.
Peak reserved memory = 24.469 GB (61.958%).

Best model already saved (ONLY ONE) at: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035/best_model_gen_cer

============================================================
STARTING EVALUATION ON TEST.JSONL (IN-MEMORY BEST MODEL)
============================================================
Starting evaluation on STAIRCASE test.jsonl with aggressive chunking...
Loaded 24 test samples

============================================================
Chunk 1/5 (5 images)
============================================================

[1/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (83).jpg
  âœ… CER: 0.0082 (0.82%)

[2/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (45).jpg
  âœ… CER: 0.0264 (2.64%)

[3/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (27).jpg
  âœ… CER: 0.0236 (2.36%)

[4/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (130).jpg
  âœ… CER: 0.0152 (1.52%)

[5/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (129).jpg
  âœ… CER: 0.0181 (1.81%)

============================================================
Chunk 2/5 (5 images)
============================================================

[6/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (88).jpg
  âœ… CER: 0.0438 (4.38%)

[7/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (102).jpg
  âœ… CER: 0.0315 (3.15%)

[8/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (103).jpg
  âœ… CER: 0.0162 (1.62%)

[9/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (165).jpg
  âœ… CER: 0.0011 (0.11%)

[10/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (43).jpg
  âœ… CER: 0.0324 (3.24%)

============================================================
Chunk 3/5 (5 images)
============================================================

[11/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (11).jpg
  âœ… CER: 0.0094 (0.94%)

[12/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (36).jpg
  âœ… CER: 0.1044 (10.44%)

[13/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (87).jpg
  âœ… CER: 0.0185 (1.85%)

[14/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (94).jpg
  âœ… CER: 0.6116 (61.16%)

[15/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (58).jpg
  âœ… CER: 0.0748 (7.48%)

============================================================
Chunk 4/5 (5 images)
============================================================

[16/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (111).jpg
  âœ… CER: 0.0230 (2.30%)

[17/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (99).jpg
  âœ… CER: 0.0201 (2.01%)

[18/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (116).jpg
  âœ… CER: 0.0122 (1.22%)

[19/24] Processing: FMIS_FormblaÌˆtterMielke_Vorlage3.jpg
  âœ… CER: 0.0817 (8.17%)

[20/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (132).jpg
  âœ… CER: 0.0096 (0.96%)

============================================================
Chunk 5/5 (4 images)
============================================================

[21/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (139).jpg
  âœ… CER: 0.0455 (4.55%)

[22/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (66).jpg
  âœ… CER: 0.0397 (3.97%)

[23/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (105).jpg
  âœ… CER: 0.5928 (59.28%)

[24/24] Processing: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (13).jpg
  âœ… CER: 0.6123 (61.23%)

CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035/cer_evaluation_results.txt

ðŸŽ‰ Multi-stage training completed!
All outputs saved to: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035

Final TEST CER: 0.1030 (10.30%)
Best VALIDATION gen-CER (Stage 2): 0.7953 (79.53%)
ONLY saved best model at: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_multistage_genCER_20260130_155035/best_model_gen_cer
Cleaned up temporary directory: /tmp/1519498.tinygpu/stair_gemma_aug_oqdfjorx
Gemma-3 training completed at: Fri Jan 30 08:13:58 PM CET 2026
=== JOB_STATISTICS ===
=== current date     : Fri Jan 30 08:13:58 PM CET 2026
= Job-ID             : 1519498 on tinygpu
= Job-Name           : stair_ms2
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_gemma.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 04:23:57
= Total RAM usage    : 9.6 GiB of requested  GiB (%)   
= Node list          : tg097
= Subm/Elig/Start/End: 2026-01-28T15:53:46 / 2026-01-28T15:53:46 / 2026-01-30T15:50:01 / 2026-01-30T20:13:58
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           219.6G  1000.0G  1500.0G        N/A   1,017K   5,000K   7,500K        N/A    
    /home/hpc              88.2G   104.9G   209.7G        N/A  29,334      500K   1,000K        N/A    
!!! /home/vault          1123.6G  1048.6G  2097.2G  -29221days   8,736      200K     400K        N/A !!!
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:C1:00.0, 244548, 25 %, 6 %, 25596 MiB, 15828546 ms
