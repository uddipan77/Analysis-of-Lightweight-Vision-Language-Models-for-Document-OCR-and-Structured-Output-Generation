### Starting TaskPrologue of job 1517477 on tg097 at Mon Jan 26 01:28:00 AM CET 2026
Running on cores 0-7,40-63 with governor ondemand
Mon Jan 26 01:28:00 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   44C    P0             55W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/gemma/stair/finetune/run_A100_genCER_20260126_012824

======================================================================
GEMMA-3 STAIRCASE - GEN-CER BEST CHECKPOINT TRAINING
======================================================================
Loading Gemma-3 vision model with Unsloth for STAIRCASE...
Temporary directory for augmented images: /tmp/1517477.tinygpu/stair_gemma_aug_kz0gi7c0
Augmentation factor (per original image): 1
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Gemma3 does not support SDPA - switching to eager!
Model loaded - Unsloth auto-configured dtype and attention
Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients
Gemma-3 vision model loaded with LoRA config: r=32, alpha=32, dropout=0.05, rslora=True

======================================================================
STARTING TRAINING (best checkpoint selected by generation CER)
======================================================================
Preparing training and validation datasets for STAIRCASE...
Preparing TRAIN data with augmentation factor = 1
Training: 230 samples (original: 115, augmented: 115)
Val/Test: 25 valid samples out of 25 total (no augmentation)
Training: 230 samples, Validation: 25 samples
[GenCER Callback] Fixed val subset size = 25
GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.494 GB.
4.809 GB of memory reserved.
Starting training (best checkpoint selected by gen_cer)...
{'loss': 37.6351, 'grad_norm': 524.9535522460938, 'learning_rate': 0.0, 'epoch': 0.07}
{'loss': 37.8105, 'grad_norm': 9604.380859375, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.35}
{'loss': 37.1633, 'grad_norm': 1177.341064453125, 'learning_rate': 2.7e-06, 'epoch': 0.7}
{'loss': 28.6713, 'grad_norm': 326.0860595703125, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.0}
{'eval_loss': 3.614293098449707, 'eval_cer': 0.3245812584880036, 'eval_cer_percentage': 32.458125848800364, 'eval_runtime': 23.8255, 'eval_samples_per_second': 1.049, 'eval_steps_per_second': 1.049, 'epoch': 1.0}

[GenCER Callback] gen_cer = 0.7817 (78.17%)
Cleaned up temporary directory: /tmp/1517477.tinygpu/stair_gemma_aug_kz0gi7c0
Gemma-3 training completed at: Mon Jan 26 02:11:27 AM CET 2026
=== JOB_STATISTICS ===
=== current date     : Mon Jan 26 02:11:27 AM CET 2026
= Job-ID             : 1517477 on tinygpu
= Job-Name           : stair_hpo2
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_gemma2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:43:44
= Total RAM usage    : 9.3 GiB of requested  GiB (%)   
= Node list          : tg097
= Subm/Elig/Start/End: 2026-01-25T16:29:21 / 2026-01-25T16:29:21 / 2026-01-26T01:27:43 / 2026-01-26T02:11:27
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              88.1G   104.9G   209.7G        N/A  29,267      500K   1,000K        N/A    
    /home/woody           219.6G  1000.0G  1500.0G        N/A   1,017K   5,000K   7,500K        N/A    
    /home/vault          1039.5G  1048.6G  2097.2G        N/A   8,520      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 773603, 22 %, 4 %, 17888 MiB, 2597512 ms
