### Starting TaskPrologue of job 1235644 on tg090 at Wed Oct  8 06:18:46 AM CEST 2025
Running on cores 96-127 with governor ondemand
Wed Oct  8 06:18:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   43C    P0             56W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Run directory: /home/vault/iwi5/iwi5298h/models_image_text/gemma/inventory_dataset/run_SIMPLIFIED_20251008_061914
============================================================
GEMMA-3 SIMPLIFIED & FOCUSED VERSION
============================================================

Key Improvements:
1. Clean label preparation (no regex during training)
2. Explicit instruction with all field requirements
3. Optimized generation: temp=0.2, rep_penalty=1.3, no_repeat_ngram=5
4. min_new_tokens=100 to prevent empty outputs
============================================================
Loading Gemma-3 vision model with Unsloth...
Clearing Unsloth compiled cache: /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/gemma3/unsloth_compiled_cache
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Gemma3 does not support SDPA - switching to eager!
Model loaded successfully
Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients
LoRA applied successfully

============================================================
TRAINING
============================================================
Preparing datasets...
Found 213 valid samples
Found 47 valid samples
Training: 213, Validation: 47
GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.494 GB.
4.809 GB of memory reserved.
Starting training...
{'loss': 49.6408, 'grad_norm': 492.88470458984375, 'learning_rate': 0.0, 'epoch': 0.08}
{'loss': 50.4324, 'grad_norm': 583.2283325195312, 'learning_rate': 5.333333333333335e-07, 'epoch': 0.38}
{'loss': 50.8348, 'grad_norm': 447.6945495605469, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.75}
{'eval_loss': 2.952662706375122, 'eval_runtime': 20.2014, 'eval_samples_per_second': 2.327, 'eval_steps_per_second': 2.327, 'epoch': 1.0}
{'loss': 42.5011, 'grad_norm': 511.4781188964844, 'learning_rate': 1.8666666666666669e-06, 'epoch': 1.08}
{'loss': 43.8964, 'grad_norm': 366.1200256347656, 'learning_rate': 2.5333333333333338e-06, 'epoch': 1.45}
{'loss': 38.672, 'grad_norm': 224.90609741210938, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.83}
{'eval_loss': 2.0765795707702637, 'eval_runtime': 20.6254, 'eval_samples_per_second': 2.279, 'eval_steps_per_second': 2.279, 'epoch': 2.0}
{'loss': 29.4819, 'grad_norm': 216.66603088378906, 'learning_rate': 3.866666666666667e-06, 'epoch': 2.15}
{'loss': 29.9703, 'grad_norm': 102.45368957519531, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.53}
{'loss': 26.7568, 'grad_norm': 98.04283142089844, 'learning_rate': 5.2e-06, 'epoch': 2.9}
{'eval_loss': 1.458583116531372, 'eval_runtime': 19.9611, 'eval_samples_per_second': 2.355, 'eval_steps_per_second': 2.355, 'epoch': 3.0}
{'loss': 20.0197, 'grad_norm': 110.97529602050781, 'learning_rate': 5.8666666666666675e-06, 'epoch': 3.23}
{'loss': 19.3748, 'grad_norm': 86.54676818847656, 'learning_rate': 6.533333333333334e-06, 'epoch': 3.6}
{'loss': 16.0294, 'grad_norm': 72.62718200683594, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.98}
{'eval_loss': 0.8704550862312317, 'eval_runtime': 19.9656, 'eval_samples_per_second': 2.354, 'eval_steps_per_second': 2.354, 'epoch': 4.0}
{'loss': 11.4292, 'grad_norm': 82.40647888183594, 'learning_rate': 7.866666666666667e-06, 'epoch': 4.3}
{'loss': 11.8561, 'grad_norm': 46.028480529785156, 'learning_rate': 8.533333333333335e-06, 'epoch': 4.68}
{'loss': 9.6787, 'grad_norm': 23.51368522644043, 'learning_rate': 9.200000000000002e-06, 'epoch': 5.0}
{'eval_loss': 0.720844566822052, 'eval_runtime': 20.0068, 'eval_samples_per_second': 2.349, 'eval_steps_per_second': 2.349, 'epoch': 5.0}
{'loss': 10.6721, 'grad_norm': 137.25440979003906, 'learning_rate': 9.866666666666668e-06, 'epoch': 5.38}
{'loss': 10.6127, 'grad_norm': 34.60142135620117, 'learning_rate': 1.0533333333333333e-05, 'epoch': 5.75}
{'eval_loss': 0.6746543645858765, 'eval_runtime': 19.9768, 'eval_samples_per_second': 2.353, 'eval_steps_per_second': 2.353, 'epoch': 6.0}
{'loss': 8.3833, 'grad_norm': 24.126501083374023, 'learning_rate': 1.1200000000000001e-05, 'epoch': 6.08}
{'loss': 9.9743, 'grad_norm': 35.28903579711914, 'learning_rate': 1.186666666666667e-05, 'epoch': 6.45}
{'loss': 9.6765, 'grad_norm': 34.396820068359375, 'learning_rate': 1.2533333333333336e-05, 'epoch': 6.83}
{'eval_loss': 0.6356679797172546, 'eval_runtime': 19.9871, 'eval_samples_per_second': 2.352, 'eval_steps_per_second': 2.352, 'epoch': 7.0}
{'loss': 7.5848, 'grad_norm': 28.308908462524414, 'learning_rate': 1.3200000000000002e-05, 'epoch': 7.15}
{'loss': 8.3779, 'grad_norm': 52.51923751831055, 'learning_rate': 1.3866666666666669e-05, 'epoch': 7.53}
{'loss': 8.7392, 'grad_norm': 29.8670654296875, 'learning_rate': 1.4533333333333335e-05, 'epoch': 7.9}
{'eval_loss': 0.6107180714607239, 'eval_runtime': 20.0288, 'eval_samples_per_second': 2.347, 'eval_steps_per_second': 2.347, 'epoch': 8.0}
{'loss': 6.9614, 'grad_norm': 27.628469467163086, 'learning_rate': 1.5200000000000002e-05, 'epoch': 8.23}
{'loss': 7.8978, 'grad_norm': 37.14423751831055, 'learning_rate': 1.586666666666667e-05, 'epoch': 8.6}
{'loss': 6.9918, 'grad_norm': 95.2158203125, 'learning_rate': 1.6533333333333333e-05, 'epoch': 8.98}
{'eval_loss': 0.6105887293815613, 'eval_runtime': 19.9843, 'eval_samples_per_second': 2.352, 'eval_steps_per_second': 2.352, 'epoch': 9.0}
{'loss': 5.9445, 'grad_norm': 30.50892448425293, 'learning_rate': 1.72e-05, 'epoch': 9.3}
{'loss': 6.4456, 'grad_norm': 44.79279327392578, 'learning_rate': 1.7866666666666666e-05, 'epoch': 9.68}
{'loss': 5.9691, 'grad_norm': 21.8883056640625, 'learning_rate': 1.8533333333333334e-05, 'epoch': 10.0}
{'eval_loss': 0.6195068955421448, 'eval_runtime': 19.9474, 'eval_samples_per_second': 2.356, 'eval_steps_per_second': 2.356, 'epoch': 10.0}
{'loss': 5.8854, 'grad_norm': 46.313961029052734, 'learning_rate': 1.9200000000000003e-05, 'epoch': 10.38}
{'loss': 6.2823, 'grad_norm': 45.12594223022461, 'learning_rate': 1.9866666666666667e-05, 'epoch': 10.75}
{'eval_loss': 0.6604732871055603, 'eval_runtime': 20.0067, 'eval_samples_per_second': 2.349, 'eval_steps_per_second': 2.349, 'epoch': 11.0}
{'loss': 4.7149, 'grad_norm': 53.21173095703125, 'learning_rate': 1.961261695938319e-05, 'epoch': 11.08}
{'loss': 4.5037, 'grad_norm': 53.613502502441406, 'learning_rate': 1.8090169943749477e-05, 'epoch': 11.45}
{'loss': 4.9626, 'grad_norm': 48.89668273925781, 'learning_rate': 1.5591929034707468e-05, 'epoch': 11.83}
{'eval_loss': 0.6771288514137268, 'eval_runtime': 19.974, 'eval_samples_per_second': 2.353, 'eval_steps_per_second': 2.353, 'epoch': 12.0}
{'loss': 3.9095, 'grad_norm': 55.1364860534668, 'learning_rate': 1.2419218955996677e-05, 'epoch': 12.15}
{'loss': 3.9125, 'grad_norm': 71.1019515991211, 'learning_rate': 8.954715367323468e-06, 'epoch': 12.53}
{'loss': 3.5927, 'grad_norm': 68.00077056884766, 'learning_rate': 5.616288532109225e-06, 'epoch': 12.9}
{'eval_loss': 0.7428640723228455, 'eval_runtime': 20.0343, 'eval_samples_per_second': 2.346, 'eval_steps_per_second': 2.346, 'epoch': 13.0}
{'train_runtime': 4185.644, 'train_samples_per_second': 0.763, 'train_steps_per_second': 0.047, 'train_loss': 15.217607851866838, 'epoch': 13.0}
4185.644 seconds used for training.
69.76 minutes used for training.
Peak reserved memory = 8.812 GB.
Peak reserved memory for training = 4.003 GB.
Peak reserved memory % of max memory = 22.312 %.
Peak reserved memory for training % of max memory = 10.136 %.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/gemma/inventory_dataset/run_SIMPLIFIED_20251008_061914...
Model saved successfully!

============================================================
EVALUATION
============================================================
Starting evaluation on test.jsonl...
Loaded 48 test samples
Processing test image 1/48: inventarbuch-022.jpg
  CER: 0.299
Processing test image 2/48: inventarbuch-099.jpg
  CER: 1.000
Processing test image 3/48: inventarbuch-245.jpg
  CER: 0.321
Processing test image 4/48: inventarbuch-263.jpg
  CER: 0.197
Processing test image 5/48: inventarbuch-033.jpg
  CER: 1.000
Processing test image 6/48: inventarbuch-143.jpg
  CER: 0.194
Processing test image 7/48: inventarbuch-244.jpg
  CER: 0.404
Processing test image 8/48: inventarbuch-024.jpg
  CER: 1.000
Processing test image 9/48: inventarbuch-141.jpg
  CER: 1.000
Processing test image 10/48: inventarbuch-183.jpg
  CER: 0.474
Processing test image 11/48: inventarbuch-191.jpg
  CER: 0.295
Processing test image 12/48: inventarbuch-051.jpg
  CER: 1.000
Processing test image 13/48: inventarbuch-203.jpg
  CER: 0.219
Processing test image 14/48: inventarbuch-296.jpg
  CER: 0.252
Processing test image 15/48: inventarbuch-302.jpg
  CER: 0.154
Processing test image 16/48: inventarbuch-179.jpg
  CER: 0.337
Processing test image 17/48: inventarbuch-114.jpg
  CER: 1.000
Processing test image 18/48: inventarbuch-082.jpg
  CER: 0.376
Processing test image 19/48: inventarbuch-287.jpg
  CER: 1.000
Processing test image 20/48: inventarbuch-181.jpg
  CER: 1.000
Processing test image 21/48: inventarbuch-299.jpg
  CER: 0.249
Processing test image 22/48: inventarbuch-084.jpg
  CER: 1.000
Processing test image 23/48: inventarbuch-004.jpg
  CER: 0.216
Processing test image 24/48: inventarbuch-148.jpg
  CER: 0.133
Processing test image 25/48: inventarbuch-238.jpg
  CER: 0.257
Processing test image 26/48: inventarbuch-116.jpg
  CER: 0.468
Processing test image 27/48: inventarbuch-223.jpg
  CER: 0.186
Processing test image 28/48: inventarbuch-104.jpg
  CER: 0.355
Processing test image 29/48: inventarbuch-015.jpg
  CER: 0.326
Processing test image 30/48: inventarbuch-272.jpg
  CER: 0.277
Processing test image 31/48: inventarbuch-124.jpg
  CER: 0.232
Processing test image 32/48: inventarbuch-115.jpg
  CER: 0.398
Processing test image 33/48: inventarbuch-049.jpg
  CER: 1.000
Processing test image 34/48: inventarbuch-017.jpg
  CER: 0.152
Processing test image 35/48: inventarbuch-018.jpg
  CER: 0.224
Processing test image 36/48: inventarbuch-225.jpg
  CER: 0.341
Processing test image 37/48: inventarbuch-046.jpg
  CER: 0.319
Processing test image 38/48: inventarbuch-294.jpg
  CER: 0.205
Processing test image 39/48: inventarbuch-054.jpg
  CER: 0.384
Processing test image 40/48: inventarbuch-073.jpg
  CER: 0.337
Processing test image 41/48: inventarbuch-118.jpg
  CER: 0.238
Processing test image 42/48: inventarbuch-130.jpg
  CER: 0.199
Processing test image 43/48: inventarbuch-146.jpg
  CER: 0.312
Processing test image 44/48: inventarbuch-014.jpg
  CER: 0.293
Processing test image 45/48: inventarbuch-059.jpg
  CER: 0.763
Processing test image 46/48: inventarbuch-256.jpg
  CER: 0.220
Processing test image 47/48: inventarbuch-260.jpg
  CER: 0.187
Processing test image 48/48: inventarbuch-279.jpg
  CER: 0.366
CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/gemma/inventory_dataset/run_SIMPLIFIED_20251008_061914/cer_evaluation_results.txt

============================================================
RESULTS
============================================================
Average CER: 0.4408 (44.08%)
Median CER: 0.3198
Perfect matches: 0/48

All outputs: /home/vault/iwi5/iwi5298h/models_image_text/gemma/inventory_dataset/run_SIMPLIFIED_20251008_061914
Gemma-3 training completed at: Wed Oct  8 08:00:41 AM CEST 2025
=== JOB_STATISTICS ===
=== current date     : Wed Oct  8 08:00:42 AM CEST 2025
= Job-ID             : 1235644 on tinygpu
= Job-Name           : gemma_inven
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_gemma.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 15:00:00
= Elapsed runtime    : 01:41:56
= Total RAM usage    : 9.5 GiB of requested  GiB (%)   
= Node list          : tg090
= Subm/Elig/Start/End: 2025-10-07T16:42:44 / 2025-10-07T16:42:44 / 2025-10-08T06:18:45 / 2025-10-08T08:00:41
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
!!! /home/hpc             135.2G   104.9G   209.7G  -29335days  37,244      500K   1,000K        N/A !!!
    /home/woody            77.8G  1000.0G  1500.0G        N/A     441K   5,000K   7,500K        N/A    
    /home/vault           904.2G  1048.6G  2097.2G        N/A   4,788      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:C1:00.0, 3846935, 31 %, 8 %, 9564 MiB, 6107963 ms
