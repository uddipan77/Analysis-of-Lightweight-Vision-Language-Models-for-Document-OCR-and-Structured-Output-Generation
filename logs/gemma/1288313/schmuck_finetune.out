### Starting TaskPrologue of job 1288313 on tg072 at Fri Oct 24 02:04:02 PM CEST 2025
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Fri Oct 24 14:04:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!
üìÅ Output: /home/vault/iwi5/iwi5298h/models_image_text/gemma/schmuck/finetune/run_FIXED_20251024_140435

============================================================
GEMMA-3 SCHMUCK (MEMORY OPTIMIZED)
============================================================

============================================================
Loading Gemma-3 (Memory Optimized for V100)
============================================================
GPU: Tesla V100-PCIE-32GB (Compute 7.x)
‚è≥ Loading model with memory optimizations...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Using float16 precision for gemma3 won't work! Using float32.
Unsloth: Gemma3 does not support SDPA - switching to eager!
‚úÖ Model loaded - applying lightweight LoRA...
Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients
‚úÖ Gemma-3 ready (lightweight config)!
============================================================


============================================================
PREPARING DATASETS
============================================================
üìÇ Loading data from /home/woody/iwi5/iwi5298h/json_schmuck/train.jsonl...
‚úÖ Found 413 valid samples
üîÑ Converting to conversation format...
üìÇ Loading data from /home/woody/iwi5/iwi5298h/json_schmuck/val.jsonl...
‚úÖ Found 88 valid samples
üîÑ Converting to conversation format...
‚úÖ Training: 413 samples
‚úÖ Validation: 88 samples

Unsloth: Switching to float32 training since model cannot work with float16
============================================================
STARTING TRAINING (MEMORY OPTIMIZED)
============================================================
GPU: Tesla V100-PCIE-32GB
Max memory: 31.733 GB
Reserved: 5.59 GB
LoRA rank: 8 (lightweight)
Gradient accum: 8
Max seq length: 1024
Learning rate: 1e-05 (low to prevent NaN)
============================================================

{'loss': nan, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.02}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.1}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.19}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.29}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.39}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.48}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 5.8e-06, 'epoch': 0.58}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.68}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.77}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 8.8e-06, 'epoch': 0.87}
{'loss': nan, 'grad_norm': nan, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.97}
Gemma-3 training completed at: Fri Oct 24 02:25:23 PM CEST 2025
=== JOB_STATISTICS ===
=== current date     : Fri Oct 24 02:25:24 PM CEST 2025
= Job-ID             : 1288313 on tinygpu
= Job-Name           : schmuck
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_gemma2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 12:00:00
= Elapsed runtime    : 00:21:23
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2025-10-24T14:03:04 / 2025-10-24T14:03:04 / 2025-10-24T14:03:05 / 2025-10-24T14:24:28
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           183.8G  1000.0G  1500.0G        N/A     833K   5,000K   7,500K        N/A    
    /home/hpc             101.7G   104.9G   209.7G        N/A  29,777      500K   1,000K        N/A    
    /home/vault           847.1G  1048.6G  2097.2G        N/A   4,438      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 1637875, 38 %, 14 %, 8008 MiB, 1272450 ms
