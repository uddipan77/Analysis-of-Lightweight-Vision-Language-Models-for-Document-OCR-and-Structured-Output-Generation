INFO:     Started server process [3412067]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
/home/woody/iwi5/iwi5298h/software/private/conda/envs/phi_ocr_backup/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.09s/it]
/home/woody/iwi5/iwi5298h/software/private/conda/envs/phi_ocr_backup/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48
You are not running the flash-attention implementation, expect numerical differences.
slurmstepd: error: *** JOB 1460667 ON tg082 CANCELLED AT 2025-12-08T13:36:57 ***
