### Starting TaskPrologue of job 1295782 on tg071 at Mon Nov  3 01:45:28 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Mon Nov  3 13:45:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:AF:00.0 Off |                    0 |
| N/A   30C    P0             25W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/multi/inven/run_20251103_135702
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
============================================================
STARTING MULTI-STAGE TRAINING WITH TEACHER FORCING + CER SELECTION
============================================================
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples

===== STAGE 1: Warm-up training (no eval, teacher forcing) =====

Unsloth: Model does not have a default image size - using 512
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.4805, 'grad_norm': 8.540557861328125, 'learning_rate': 1.8e-05, 'epoch': 0.37}
{'loss': 2.0595, 'grad_norm': 6.2120680809021, 'learning_rate': 4.800000000000001e-05, 'epoch': 0.75}
{'loss': 1.2947, 'grad_norm': 2.96156907081604, 'learning_rate': 7.800000000000001e-05, 'epoch': 1.15}
{'loss': 1.0609, 'grad_norm': 3.6779673099517822, 'learning_rate': 0.00010800000000000001, 'epoch': 1.52}
{'loss': 1.0657, 'grad_norm': 16.54805564880371, 'learning_rate': 0.00013800000000000002, 'epoch': 1.9}
{'train_runtime': 574.3268, 'train_samples_per_second': 0.742, 'train_steps_per_second': 0.091, 'train_loss': 1.7591013610363007, 'epoch': 1.97}

===== STAGE 2: Main training (eval each epoch, CER-based best model) =====

Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
9.631 GB of memory reserved.
{'loss': 0.7697, 'grad_norm': 3.6717231273651123, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.37}
{'loss': 0.7797, 'grad_norm': 3.4365620613098145, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.75}
{'eval_loss': 1.0283336639404297, 'eval_runtime': 25.3664, 'eval_samples_per_second': 1.853, 'eval_steps_per_second': 0.946, 'epoch': 0.97}

[ValidationCERCallback] Computing CER on 47 validation samples...

[ValidationCERCallback] Epoch 0.97 CER: 0.2297 (22.97%)
[ValidationCERCallback] New best CER: 0.2297 (improved by inf)
[ValidationCERCallback] Saving best CER model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/multi/inven/run_20251103_135702/best_model_cer
{'loss': 0.7412, 'grad_norm': 2.8443853855133057, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.15}
{'loss': 0.6015, 'grad_norm': 2.673334836959839, 'learning_rate': 3.6e-05, 'epoch': 1.52}
{'loss': 0.7512, 'grad_norm': 4.219703674316406, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.9}
{'eval_loss': 1.0435715913772583, 'eval_runtime': 23.7936, 'eval_samples_per_second': 1.975, 'eval_steps_per_second': 1.009, 'epoch': 1.97}

[ValidationCERCallback] Computing CER on 47 validation samples...

[ValidationCERCallback] Epoch 1.97 CER: 0.2284 (22.84%)
[ValidationCERCallback] New best CER: 0.2284 (improved by 0.0013)
[ValidationCERCallback] Saving best CER model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/multi/inven/run_20251103_135702/best_model_cer
{'loss': 0.6056, 'grad_norm': 5.793880939483643, 'learning_rate': 4.998234994371135e-05, 'epoch': 2.3}
{'loss': 0.4843, 'grad_norm': 3.305772304534912, 'learning_rate': 4.9841298989965984e-05, 'epoch': 2.67}
{'eval_loss': 1.111507534980774, 'eval_runtime': 23.7205, 'eval_samples_per_second': 1.981, 'eval_steps_per_second': 1.012, 'epoch': 2.97}

[ValidationCERCallback] Computing CER on 47 validation samples...

[ValidationCERCallback] Epoch 2.97 CER: 0.2347 (23.47%)
[ValidationCERCallback] No improvement. Best CER remains 0.2284
{'loss': 0.5255, 'grad_norm': 11.096839904785156, 'learning_rate': 4.9559993459581375e-05, 'epoch': 3.07}
{'loss': 0.2878, 'grad_norm': 3.4958531856536865, 'learning_rate': 4.9140021610405326e-05, 'epoch': 3.45}
{'loss': 0.312, 'grad_norm': 9.748828887939453, 'learning_rate': 4.858375461368499e-05, 'epoch': 3.82}
{'eval_loss': 1.2543704509735107, 'eval_runtime': 23.834, 'eval_samples_per_second': 1.972, 'eval_steps_per_second': 1.007, 'epoch': 3.97}

[ValidationCERCallback] Computing CER on 47 validation samples...

[ValidationCERCallback] Epoch 3.97 CER: 0.2432 (24.32%)
[ValidationCERCallback] No improvement. Best CER remains 0.2284
{'loss': 0.2556, 'grad_norm': 3.4319920539855957, 'learning_rate': 4.789433316637644e-05, 'epoch': 4.22}
{'loss': 0.1804, 'grad_norm': 2.6426875591278076, 'learning_rate': 4.7075649758693565e-05, 'epoch': 4.6}
{'loss': 0.1818, 'grad_norm': 5.657556056976318, 'learning_rate': 4.613232669701384e-05, 'epoch': 4.97}
{'eval_loss': 1.364378571510315, 'eval_runtime': 24.4185, 'eval_samples_per_second': 1.925, 'eval_steps_per_second': 0.983, 'epoch': 4.97}

[ValidationCERCallback] Computing CER on 47 validation samples...

[ValidationCERCallback] Epoch 4.97 CER: 0.2417 (24.17%)
[ValidationCERCallback] No improvement. Best CER remains 0.2284
{'train_runtime': 7696.5536, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.061, 'train_loss': 0.49845552450189223, 'epoch': 4.97}
7696.5536 seconds used for training.
128.28 minutes used for training.
Peak reserved memory = 10.043 GB.
Peak reserved memory for training = 0.412 GB.
Peak reserved memory % of max memory = 31.648 %.
Peak reserved memory for training % of max memory = 1.298 %.

[train_model] Reloading best CER model from /home/vault/iwi5/iwi5298h/models_image_text/qwen/multi/inven/run_20251103_135702/best_model_cer (CER=0.2284) for downstream evaluation.
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
=== JOB_STATISTICS ===
=== current date     : Mon Nov  3 04:25:40 PM CET 2025
= Job-ID             : 1295782 on tinygpu
= Job-Name           : qwen_inven_ms
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 02:40:18
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg071
= Subm/Elig/Start/End: 2025-11-03T13:40:57 / 2025-11-03T13:40:57 / 2025-11-03T13:44:57 / 2025-11-03T16:25:15
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc             102.2G   104.9G   209.7G        N/A  30,528      500K   1,000K        N/A    
    /home/woody           184.1G  1000.0G  1500.0G        N/A     841K   5,000K   7,500K        N/A    
    /home/vault           864.2G  1048.6G  2097.2G        N/A   5,020      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 2079816, 26 %, 11 %, 15794 MiB, 9335499 ms
