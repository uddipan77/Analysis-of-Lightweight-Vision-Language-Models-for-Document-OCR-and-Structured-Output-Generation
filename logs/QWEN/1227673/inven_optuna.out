### Starting TaskPrologue of job 1227673 on tg073 at Fri Sep 26 09:21:22 PM CEST 2025
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Fri Sep 26 21:21:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   28C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152
Loading Qwen2.5-VL model with Unsloth...
Using dtype: torch.float16
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with LoRA adapters!
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.3999, 'grad_norm': 12.832582473754883, 'learning_rate': 2.418512339984261e-05, 'epoch': 0.19}
{'loss': 2.2047, 'grad_norm': 7.567900657653809, 'learning_rate': 2.8068581172658988e-05, 'epoch': 0.37}
{'loss': 1.564, 'grad_norm': 30.743221282958984, 'learning_rate': 2.735404548896667e-05, 'epoch': 0.56}
{'loss': 1.3266, 'grad_norm': 3.980945110321045, 'learning_rate': 2.607567405225768e-05, 'epoch': 0.75}
{'loss': 1.2308, 'grad_norm': 4.063521862030029, 'learning_rate': 2.428788241656516e-05, 'epoch': 0.93}
{'eval_loss': 1.1104387044906616, 'eval_runtime': 19.1406, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 1.254, 'epoch': 1.0}
{'loss': 1.0556, 'grad_norm': 3.614105224609375, 'learning_rate': 2.2066770273698922e-05, 'epoch': 1.11}
{'loss': 0.9662, 'grad_norm': 3.535228729248047, 'learning_rate': 1.9506882170222786e-05, 'epoch': 1.3}
{'loss': 0.9231, 'grad_norm': 4.120912551879883, 'learning_rate': 1.6717183095038655e-05, 'epoch': 1.49}
{'loss': 0.9637, 'grad_norm': 3.1288650035858154, 'learning_rate': 1.3816420241961084e-05, 'epoch': 1.67}
{'loss': 1.1021, 'grad_norm': 3.7138781547546387, 'learning_rate': 1.0928068379931504e-05, 'epoch': 1.86}
{'eval_loss': 1.0425220727920532, 'eval_runtime': 19.128, 'eval_samples_per_second': 2.457, 'eval_steps_per_second': 1.255, 'epoch': 2.0}
{'loss': 0.9344, 'grad_norm': 8.792343139648438, 'learning_rate': 8.175073987807108e-06, 'epoch': 2.04}
{'loss': 0.886, 'grad_norm': 3.6658213138580322, 'learning_rate': 5.674621876508923e-06, 'epoch': 2.22}
{'loss': 0.9126, 'grad_norm': 21.32730484008789, 'learning_rate': 3.5331470641095533e-06, 'epoch': 2.41}
{'loss': 0.9068, 'grad_norm': 4.063093185424805, 'learning_rate': 1.841804229928732e-06, 'epoch': 2.6}
{'loss': 0.8041, 'grad_norm': 3.45888352394104, 'learning_rate': 6.7258759625515e-07, 'epoch': 2.79}
{'eval_loss': 1.0373587608337402, 'eval_runtime': 19.1216, 'eval_samples_per_second': 2.458, 'eval_steps_per_second': 1.255, 'epoch': 2.95}
{'train_runtime': 858.4063, 'train_samples_per_second': 0.744, 'train_steps_per_second': 0.185, 'train_loss': 1.2620549951709292, 'epoch': 2.95}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.2037
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=8, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.5891, 'grad_norm': 15.68415355682373, 'learning_rate': 6.300502089313286e-06, 'epoch': 0.19}
{'loss': 3.0267, 'grad_norm': 22.520952224731445, 'learning_rate': 1.728378299427733e-05, 'epoch': 0.37}
{'loss': 2.1699, 'grad_norm': 8.26814079284668, 'learning_rate': 1.6538087036061202e-05, 'epoch': 0.56}
{'loss': 1.8391, 'grad_norm': 8.907439231872559, 'learning_rate': 1.4939023832769778e-05, 'epoch': 0.75}
{'loss': 1.5555, 'grad_norm': 7.20100212097168, 'learning_rate': 1.29118225497879e-05, 'epoch': 0.93}
{'eval_loss': 1.3315054178237915, 'eval_runtime': 19.0515, 'eval_samples_per_second': 2.467, 'eval_steps_per_second': 1.26, 'epoch': 1.0}
{'loss': 1.3027, 'grad_norm': 5.063112735748291, 'learning_rate': 1.0230192876459667e-05, 'epoch': 1.11}
{'loss': 1.1595, 'grad_norm': 9.186347007751465, 'learning_rate': 7.378754167311472e-06, 'epoch': 1.3}
{'loss': 1.0929, 'grad_norm': 4.669546604156494, 'learning_rate': 4.6665040511097845e-06, 'epoch': 1.49}
{'loss': 1.1273, 'grad_norm': 4.3696465492248535, 'learning_rate': 2.387356912841759e-06, 'epoch': 1.67}
{'loss': 1.2592, 'grad_norm': 5.025728225708008, 'learning_rate': 7.882937095503343e-07, 'epoch': 1.86}
{'eval_loss': 1.1656299829483032, 'eval_runtime': 19.0325, 'eval_samples_per_second': 2.469, 'eval_steps_per_second': 1.261, 'epoch': 1.97}
{'train_runtime': 568.8144, 'train_samples_per_second': 0.749, 'train_steps_per_second': 0.186, 'train_loss': 1.7752740563086744, 'epoch': 1.97}
Training finished.
Evaluating on validation set at epoch 2...
Validation CER at epoch 2: 0.2356
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=8, alpha=8
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'eval_loss': 3.5608205795288086, 'eval_runtime': 16.4329, 'eval_samples_per_second': 2.86, 'eval_steps_per_second': 0.73, 'epoch': 0.89}
{'loss': 3.8384, 'grad_norm': 3.1568422317504883, 'learning_rate': 6.855840007164662e-06, 'epoch': 1.59}
{'eval_loss': 3.3758256435394287, 'eval_runtime': 16.4727, 'eval_samples_per_second': 2.853, 'eval_steps_per_second': 0.728, 'epoch': 1.89}
{'eval_loss': 3.320098876953125, 'eval_runtime': 16.3294, 'eval_samples_per_second': 2.878, 'eval_steps_per_second': 0.735, 'epoch': 2.89}
{'train_runtime': 730.5418, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.025, 'train_loss': 3.7634651396009655, 'epoch': 2.89}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.6630
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=8
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.7719, 'grad_norm': 3.2446069717407227, 'learning_rate': 1.6484504731695834e-05, 'epoch': 0.38}
{'loss': 3.139, 'grad_norm': 2.894522190093994, 'learning_rate': 1.3711446912575123e-05, 'epoch': 0.75}
{'eval_loss': 2.4526474475860596, 'eval_runtime': 24.9032, 'eval_samples_per_second': 1.887, 'eval_steps_per_second': 1.887, 'epoch': 1.0}
{'loss': 2.5693, 'grad_norm': 4.730107307434082, 'learning_rate': 8.899586105337856e-06, 'epoch': 1.11}
{'loss': 2.3049, 'grad_norm': 3.1629767417907715, 'learning_rate': 3.88688958921516e-06, 'epoch': 1.49}
{'loss': 2.2632, 'grad_norm': 2.8970606327056885, 'learning_rate': 5.880370580777257e-07, 'epoch': 1.86}
{'eval_loss': 2.2285871505737305, 'eval_runtime': 24.8817, 'eval_samples_per_second': 1.889, 'eval_steps_per_second': 1.889, 'epoch': 1.94}
{'train_runtime': 736.5942, 'train_samples_per_second': 0.578, 'train_steps_per_second': 0.071, 'train_loss': 2.794339418411255, 'epoch': 1.94}
Training finished.
Evaluating on validation set at epoch 2...
Validation CER at epoch 2: 0.7862
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=8, alpha=8
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.6377, 'grad_norm': 5.625295162200928, 'learning_rate': 1.2872085635846807e-05, 'epoch': 0.37}
{'loss': 3.2947, 'grad_norm': 3.2033557891845703, 'learning_rate': 1.6776332989153588e-05, 'epoch': 0.74}
{'eval_loss': 2.5467886924743652, 'eval_runtime': 16.6482, 'eval_samples_per_second': 2.823, 'eval_steps_per_second': 0.721, 'epoch': 1.0}
{'loss': 2.7362, 'grad_norm': 2.3530561923980713, 'learning_rate': 1.4966836847894645e-05, 'epoch': 1.11}
{'loss': 2.2943, 'grad_norm': 2.4935262203216553, 'learning_rate': 1.2328465124388848e-05, 'epoch': 1.48}
{'loss': 2.1561, 'grad_norm': 1.819193720817566, 'learning_rate': 8.766028456396724e-06, 'epoch': 1.85}
{'eval_loss': 2.0441067218780518, 'eval_runtime': 16.3114, 'eval_samples_per_second': 2.881, 'eval_steps_per_second': 0.736, 'epoch': 2.0}
{'loss': 2.0544, 'grad_norm': 1.733969807624817, 'learning_rate': 5.169920324324137e-06, 'epoch': 2.22}
{'loss': 1.9248, 'grad_norm': 1.7529752254486084, 'learning_rate': 2.195943999901102e-06, 'epoch': 2.59}
{'loss': 1.9854, 'grad_norm': 1.7988612651824951, 'learning_rate': 3.864478586421586e-07, 'epoch': 2.96}
{'eval_loss': 1.916479229927063, 'eval_runtime': 16.3336, 'eval_samples_per_second': 2.877, 'eval_steps_per_second': 0.735, 'epoch': 3.0}
{'train_runtime': 761.1065, 'train_samples_per_second': 0.84, 'train_steps_per_second': 0.106, 'train_loss': 2.5027693083256852, 'epoch': 3.0}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.7646
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=32, alpha=16
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.437, 'grad_norm': nan, 'learning_rate': 8.308768775392415e-06, 'epoch': 0.19}
{'loss': 2.9138, 'grad_norm': 7.623723983764648, 'learning_rate': 7.994247946635103e-06, 'epoch': 0.37}
{'loss': 2.4094, 'grad_norm': 7.3617377281188965, 'learning_rate': 7.323833312363093e-06, 'epoch': 0.56}
{'loss': 2.25, 'grad_norm': 11.641000747680664, 'learning_rate': 6.3738722774048404e-06, 'epoch': 0.75}
{'loss': 2.0061, 'grad_norm': 5.015118598937988, 'learning_rate': 5.228773048551728e-06, 'epoch': 0.93}
{'eval_loss': 1.7702152729034424, 'eval_runtime': 19.1665, 'eval_samples_per_second': 2.452, 'eval_steps_per_second': 1.252, 'epoch': 1.0}
{'loss': 1.763, 'grad_norm': 3.0383732318878174, 'learning_rate': 4.115219059139399e-06, 'epoch': 1.11}
{'loss': 1.5939, 'grad_norm': 3.8437557220458984, 'learning_rate': 2.8867302063902915e-06, 'epoch': 1.3}
{'loss': 1.5092, 'grad_norm': 2.0215344429016113, 'learning_rate': 1.7729510358268881e-06, 'epoch': 1.49}
{'loss': 1.4897, 'grad_norm': 7.232793807983398, 'learning_rate': 8.728457144931318e-07, 'epoch': 1.67}
{'loss': 1.6047, 'grad_norm': 7.285492420196533, 'learning_rate': 2.6639255023666707e-07, 'epoch': 1.86}
{'eval_loss': 1.5047391653060913, 'eval_runtime': 19.0658, 'eval_samples_per_second': 2.465, 'eval_steps_per_second': 1.259, 'epoch': 1.97}
{'train_runtime': 581.8686, 'train_samples_per_second': 0.732, 'train_steps_per_second': 0.182, 'train_loss': 2.065890717056562, 'epoch': 1.97}
Training finished.
Evaluating on validation set at epoch 2...
Validation CER at epoch 2: 0.7638
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=32, alpha=64
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.4877, 'grad_norm': 19.52726173400879, 'learning_rate': 8.485430273693564e-06, 'epoch': 0.74}
{'eval_loss': 2.242478847503662, 'eval_runtime': 16.5643, 'eval_samples_per_second': 2.837, 'eval_steps_per_second': 0.724, 'epoch': 0.96}
{'loss': 2.2328, 'grad_norm': 7.8874006271362305, 'learning_rate': 6.593285413236402e-06, 'epoch': 1.52}
{'eval_loss': 1.6114130020141602, 'eval_runtime': 16.7892, 'eval_samples_per_second': 2.799, 'eval_steps_per_second': 0.715, 'epoch': 1.96}
{'loss': 1.7026, 'grad_norm': 9.926971435546875, 'learning_rate': 3.305275235266043e-06, 'epoch': 2.3}
{'eval_loss': 1.4653140306472778, 'eval_runtime': 16.4726, 'eval_samples_per_second': 2.853, 'eval_steps_per_second': 0.728, 'epoch': 2.96}
{'train_runtime': 764.169, 'train_samples_per_second': 0.836, 'train_steps_per_second': 0.051, 'train_loss': 2.2473962734907103, 'epoch': 2.96}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.7352
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=32, alpha=64
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.2256, 'grad_norm': 22.711763381958008, 'learning_rate': 2.5038426541027535e-05, 'epoch': 0.37}
{'loss': 1.6449, 'grad_norm': 9.32789134979248, 'learning_rate': 1.439860885801658e-05, 'epoch': 0.74}
{'eval_loss': 1.2120481729507446, 'eval_runtime': 16.4775, 'eval_samples_per_second': 2.852, 'eval_steps_per_second': 0.728, 'epoch': 1.0}
{'train_runtime': 259.3185, 'train_samples_per_second': 0.821, 'train_steps_per_second': 0.104, 'train_loss': 2.1448450088500977, 'epoch': 1.0}
Training finished.
Evaluating on validation set at epoch 1...
Validation CER at epoch 1: 0.2434
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=64
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.5331, 'grad_norm': 15.469764709472656, 'learning_rate': 1.3815927128712877e-05, 'epoch': 0.37}
{'loss': 2.2087, 'grad_norm': 14.369463920593262, 'learning_rate': 7.027695014715878e-06, 'epoch': 0.75}
{'eval_loss': 1.7018115520477295, 'eval_runtime': 19.1627, 'eval_samples_per_second': 2.453, 'eval_steps_per_second': 1.252, 'epoch': 0.97}
{'train_runtime': 281.8854, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.092, 'train_loss': 2.6347440572885366, 'epoch': 0.97}
Training finished.
Evaluating on validation set at epoch 1...
Validation CER at epoch 1: 0.7236
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=32, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'eval_loss': 3.1284899711608887, 'eval_runtime': 16.5649, 'eval_samples_per_second': 2.837, 'eval_steps_per_second': 0.724, 'epoch': 0.89}
{'loss': 3.5445, 'grad_norm': 7.15927267074585, 'learning_rate': 3.147743675185616e-06, 'epoch': 1.59}
{'eval_loss': 2.733975648880005, 'eval_runtime': 16.4932, 'eval_samples_per_second': 2.85, 'eval_steps_per_second': 0.728, 'epoch': 1.89}
{'train_runtime': 489.7116, 'train_samples_per_second': 0.87, 'train_steps_per_second': 0.025, 'train_loss': 3.4142523209253945, 'epoch': 1.89}
Training finished.
Evaluating on validation set at epoch 2...
Validation CER at epoch 2: 0.6584
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.8555, 'grad_norm': nan, 'learning_rate': 3.848049174170776e-06, 'epoch': 0.09}
{'loss': 3.1706, 'grad_norm': 17.337854385375977, 'learning_rate': 5.3805374850543676e-06, 'epoch': 0.19}
{'loss': 2.6269, 'grad_norm': 25.731277465820312, 'learning_rate': 5.352162793566173e-06, 'epoch': 0.28}
{'loss': 2.5621, 'grad_norm': 12.111891746520996, 'learning_rate': 5.294897683196176e-06, 'epoch': 0.38}
{'loss': 2.1123, 'grad_norm': nan, 'learning_rate': 5.211111325788957e-06, 'epoch': 0.47}
{'loss': 2.0045, 'grad_norm': 10.283552169799805, 'learning_rate': 5.113728517499471e-06, 'epoch': 0.56}
{'loss': 1.8181, 'grad_norm': 7.7723236083984375, 'learning_rate': 4.982121747282153e-06, 'epoch': 0.66}
{'loss': 1.6896, 'grad_norm': 17.081890106201172, 'learning_rate': 4.827182645193239e-06, 'epoch': 0.75}
{'loss': 1.466, 'grad_norm': 8.17724895477295, 'learning_rate': 4.650490896866016e-06, 'epoch': 0.85}
{'loss': 1.4493, 'grad_norm': 5.457385063171387, 'learning_rate': 4.453847967602583e-06, 'epoch': 0.94}
{'eval_loss': 1.238990306854248, 'eval_runtime': 27.5499, 'eval_samples_per_second': 1.706, 'eval_steps_per_second': 1.706, 'epoch': 1.0}
{'loss': 1.1528, 'grad_norm': 6.008660316467285, 'learning_rate': 4.23925873548766e-06, 'epoch': 1.03}
{'loss': 1.2363, 'grad_norm': 9.231392860412598, 'learning_rate': 4.008911050602079e-06, 'epoch': 1.12}
{'loss': 1.0787, 'grad_norm': 3.68422794342041, 'learning_rate': 3.7651534287405478e-06, 'epoch': 1.22}
{'loss': 1.1366, 'grad_norm': 22.176504135131836, 'learning_rate': 3.5104711070579654e-06, 'epoch': 1.31}
{'loss': 1.0349, 'grad_norm': 9.441128730773926, 'learning_rate': 3.247460705769556e-06, 'epoch': 1.4}
{'loss': 1.0198, 'grad_norm': 9.909908294677734, 'learning_rate': 2.978803754242136e-06, 'epoch': 1.5}
{'loss': 0.9119, 'grad_norm': 5.329232692718506, 'learning_rate': 2.7072393513919092e-06, 'epoch': 1.59}
{'loss': 1.1961, 'grad_norm': 7.419901371002197, 'learning_rate': 2.435536239130439e-06, 'epoch': 1.69}
{'loss': 1.2069, 'grad_norm': 5.947399139404297, 'learning_rate': 2.1664645735846883e-06, 'epoch': 1.78}
{'loss': 1.2004, 'grad_norm': 6.257460117340088, 'learning_rate': 1.9027676818983898e-06, 'epoch': 1.87}
{'loss': 0.9349, 'grad_norm': 3.684077024459839, 'learning_rate': 1.6471340925690133e-06, 'epoch': 1.97}
{'eval_loss': 1.0952872037887573, 'eval_runtime': 26.3331, 'eval_samples_per_second': 1.785, 'eval_steps_per_second': 1.785, 'epoch': 2.0}
{'loss': 1.0416, 'grad_norm': 5.46237325668335, 'learning_rate': 1.402170124485744e-06, 'epoch': 2.06}
{'loss': 1.0267, 'grad_norm': 5.025351047515869, 'learning_rate': 1.1703733141376687e-06, 'epoch': 2.15}
{'loss': 0.972, 'grad_norm': 5.223273277282715, 'learning_rate': 9.54106951915728e-07, 'epoch': 2.24}
{'loss': 0.9052, 'grad_norm': 5.503695487976074, 'learning_rate': 7.555759871242505e-07, 'epoch': 2.34}
{'loss': 0.9977, 'grad_norm': 5.53771448135376, 'learning_rate': 5.768045473631433e-07, 'epoch': 2.43}
{'loss': 1.1167, 'grad_norm': 4.949117660522461, 'learning_rate': 4.1961530148245413e-07, 'epoch': 2.53}
{'loss': 0.971, 'grad_norm': 5.054075241088867, 'learning_rate': 2.8561087651484714e-07, 'epoch': 2.62}
{'loss': 0.9694, 'grad_norm': 6.6714396476745605, 'learning_rate': 1.7615751805012964e-07, 'epoch': 2.71}
{'loss': 0.948, 'grad_norm': 5.947375297546387, 'learning_rate': 9.237116064291075e-08, 'epoch': 2.81}
{'loss': 1.1177, 'grad_norm': 3.4505844116210938, 'learning_rate': 3.510605027291376e-08, 'epoch': 2.9}
{'eval_loss': 1.0846201181411743, 'eval_runtime': 27.3944, 'eval_samples_per_second': 1.716, 'eval_steps_per_second': 1.716, 'epoch': 2.98}
{'train_runtime': 1213.0439, 'train_samples_per_second': 0.527, 'train_steps_per_second': 0.262, 'train_loss': 1.4401960222976014, 'epoch': 2.98}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.2324
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.8717, 'grad_norm': 35.7923469543457, 'learning_rate': 2.3704563908839293e-06, 'epoch': 0.09}
{'loss': 3.2485, 'grad_norm': 15.374796867370605, 'learning_rate': 5.330081922806007e-06, 'epoch': 0.19}
{'loss': 2.6796, 'grad_norm': 9.111368179321289, 'learning_rate': 5.306558180370047e-06, 'epoch': 0.28}
{'loss': 2.5901, 'grad_norm': 11.079604148864746, 'learning_rate': 5.2545310998153595e-06, 'epoch': 0.38}
{'loss': 2.1502, 'grad_norm': 24.958688735961914, 'learning_rate': 5.175778020927074e-06, 'epoch': 0.47}
{'loss': 2.0248, 'grad_norm': 7.247786998748779, 'learning_rate': 5.071112291320439e-06, 'epoch': 0.56}
{'loss': 1.8212, 'grad_norm': 7.054705619812012, 'learning_rate': 4.941614879800424e-06, 'epoch': 0.66}
{'loss': 1.6868, 'grad_norm': 11.93414306640625, 'learning_rate': 4.788623212310593e-06, 'epoch': 0.75}
{'loss': 1.4703, 'grad_norm': 7.004266262054443, 'learning_rate': 4.613717359238824e-06, 'epoch': 0.85}
{'loss': 1.4522, 'grad_norm': 6.578916072845459, 'learning_rate': 4.418703716734918e-06, 'epoch': 0.94}
{'eval_loss': 1.2397724390029907, 'eval_runtime': 26.5468, 'eval_samples_per_second': 1.77, 'eval_steps_per_second': 1.77, 'epoch': 1.0}
{'loss': 1.1558, 'grad_norm': 7.158748626708984, 'learning_rate': 4.205596350576498e-06, 'epoch': 1.03}
{'loss': 1.2388, 'grad_norm': 10.996858596801758, 'learning_rate': 4.000145159616631e-06, 'epoch': 1.12}
{'loss': 1.0765, 'grad_norm': 6.211605548858643, 'learning_rate': 3.7588594507265237e-06, 'epoch': 1.22}
{'loss': 1.1348, 'grad_norm': 15.932886123657227, 'learning_rate': 3.506294770922606e-06, 'epoch': 1.31}
{'loss': 1.0327, 'grad_norm': 16.176454544067383, 'learning_rate': 3.2450595627307363e-06, 'epoch': 1.4}
{'loss': 1.0185, 'grad_norm': 15.983992576599121, 'learning_rate': 2.9778518163323893e-06, 'epoch': 1.5}
{'loss': 0.9112, 'grad_norm': 22.65129852294922, 'learning_rate': 2.707431205207978e-06, 'epoch': 1.59}
{'loss': 1.1915, 'grad_norm': 6.334339141845703, 'learning_rate': 2.4365905847261667e-06, 'epoch': 1.69}
{'loss': 1.2079, 'grad_norm': 9.670564651489258, 'learning_rate': 2.168127148036615e-06, 'epoch': 1.78}
{'loss': 1.2018, 'grad_norm': 180.70933532714844, 'learning_rate': 1.9048135371629137e-06, 'epoch': 1.87}
{'loss': 0.9289, 'grad_norm': 4.281246662139893, 'learning_rate': 1.6493692076551131e-06, 'epoch': 1.97}
{'eval_loss': 1.0948224067687988, 'eval_runtime': 27.3374, 'eval_samples_per_second': 1.719, 'eval_steps_per_second': 1.719, 'epoch': 2.0}
{'loss': 1.0414, 'grad_norm': 7.117819786071777, 'learning_rate': 1.4044323425425846e-06, 'epoch': 2.06}
{'loss': 1.0267, 'grad_norm': 3.8974151611328125, 'learning_rate': 1.1725326056548291e-06, 'epoch': 2.15}
{'loss': 0.9674, 'grad_norm': 5.612857818603516, 'learning_rate': 9.560650157090425e-07, 'epoch': 2.24}
{'loss': 0.9047, 'grad_norm': 10.47951889038086, 'learning_rate': 7.572652109881483e-07, 'epoch': 2.34}
{'loss': 0.9987, 'grad_norm': 4.329596042633057, 'learning_rate': 5.781863600712577e-07, 'epoch': 2.43}
{'loss': 1.1105, 'grad_norm': 13.492545127868652, 'learning_rate': 4.20677957078352e-07, 'epoch': 2.53}
{'loss': 0.9816, 'grad_norm': 4.440316677093506, 'learning_rate': 2.8636672042811915e-07, 'epoch': 2.62}
{'loss': 0.975, 'grad_norm': 4.518215179443359, 'learning_rate': 1.7663979238308954e-07, 'epoch': 2.71}
{'loss': 0.9508, 'grad_norm': 6.23776912689209, 'learning_rate': 9.263041289418084e-08, 'epoch': 2.81}
{'loss': 1.1202, 'grad_norm': 4.793262958526611, 'learning_rate': 3.520621570257787e-08, 'epoch': 2.9}
{'eval_loss': 1.0847866535186768, 'eval_runtime': 26.1321, 'eval_samples_per_second': 1.799, 'eval_steps_per_second': 1.799, 'epoch': 2.98}
{'train_runtime': 1210.5901, 'train_samples_per_second': 0.528, 'train_steps_per_second': 0.263, 'train_loss': 1.4477543141107139, 'epoch': 2.98}
Training finished.
Evaluating on validation set at epoch 3...
Validation CER at epoch 3: 0.2224

=== HPO DONE ===
Best trial #: 0
Best params: {'lora_r': 16, 'lora_alpha': 32, 'learning_rate': 2.821597729981638e-05, 'batch_size': 2, 'grad_accum_steps': 2, 'warmup_ratio': 0.03838535052564136, 'weight_decay': 0.06956514659802214, 'num_train_epochs': 3, 'gen_temperature': 0.1315753593583161}
Best val CER: 0.2037

=== FINAL TRAIN (train âˆª val) WITH BEST PARAMS ===
Re-loading base model [/home/vault/iwi5/iwi5298h/models/qwen7b] with LoRA r=16, alpha=32
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Preparing training/validation datasets...
Training: 260 samples, Validation: 0 samples
Unsloth: Model does not have a default image size - using 512
Starting training...
{'loss': 3.6757, 'grad_norm': 13.207633972167969, 'learning_rate': 2.8215977299816383e-06, 'epoch': 0.15}
{'loss': 3.3132, 'grad_norm': 10.492205619812012, 'learning_rate': 7.900473643948588e-06, 'epoch': 0.31}
{'loss': 2.4801, 'grad_norm': 28.41885757446289, 'learning_rate': 1.3543669103911863e-05, 'epoch': 0.46}
{'loss': 1.9, 'grad_norm': 5.972558975219727, 'learning_rate': 1.8622545017878812e-05, 'epoch': 0.62}
{'loss': 1.6374, 'grad_norm': 4.641227722167969, 'learning_rate': 2.3701420931845758e-05, 'epoch': 0.77}
{'loss': 1.2794, 'grad_norm': 7.22886848449707, 'learning_rate': 2.821579907265005e-05, 'epoch': 0.92}
{'loss': 1.1456, 'grad_norm': 4.5509490966796875, 'learning_rate': 2.820956159464237e-05, 'epoch': 1.08}
{'loss': 1.0389, 'grad_norm': 3.8097996711730957, 'learning_rate': 2.8194417260918144e-05, 'epoch': 1.23}
{'loss': 1.0936, 'grad_norm': 4.532280445098877, 'learning_rate': 2.8170375636963144e-05, 'epoch': 1.38}
{'loss': 0.975, 'grad_norm': 5.934581279754639, 'learning_rate': 2.8137451907982354e-05, 'epoch': 1.54}
{'loss': 1.0595, 'grad_norm': 3.992870807647705, 'learning_rate': 2.8095666869308663e-05, 'epoch': 1.69}
{'loss': 1.0213, 'grad_norm': 4.975340366363525, 'learning_rate': 2.80450469132681e-05, 'epoch': 1.85}
{'loss': 0.9637, 'grad_norm': 7.997402667999268, 'learning_rate': 2.7985624012509858e-05, 'epoch': 2.0}
{'loss': 0.8547, 'grad_norm': 6.139052391052246, 'learning_rate': 2.791743569981168e-05, 'epoch': 2.15}
{'loss': 0.8867, 'grad_norm': 3.7661936283111572, 'learning_rate': 2.784052504437338e-05, 'epoch': 2.31}
{'loss': 0.8681, 'grad_norm': 4.342703342437744, 'learning_rate': 2.775494062461339e-05, 'epoch': 2.46}
{'loss': 0.8575, 'grad_norm': 4.185973167419434, 'learning_rate': 2.76607364974856e-05, 'epoch': 2.62}
{'loss': 0.8455, 'grad_norm': 5.070373058319092, 'learning_rate': 2.7557972164335813e-05, 'epoch': 2.77}
{'loss': 0.7354, 'grad_norm': 9.666056632995605, 'learning_rate': 2.744671253331943e-05, 'epoch': 2.92}
{'loss': 0.7746, 'grad_norm': 4.595054626464844, 'learning_rate': 2.7327027878404085e-05, 'epoch': 3.08}
{'loss': 0.7097, 'grad_norm': 5.682513236999512, 'learning_rate': 2.7198993794983063e-05, 'epoch': 3.23}
{'loss': 0.6064, 'grad_norm': 4.450924396514893, 'learning_rate': 2.7062691152127666e-05, 'epoch': 3.38}
{'loss': 0.6915, 'grad_norm': 15.092231750488281, 'learning_rate': 2.6918206041508544e-05, 'epoch': 3.54}
{'loss': 0.6147, 'grad_norm': 15.289817810058594, 'learning_rate': 2.676562972301837e-05, 'epoch': 3.69}
{'loss': 0.6887, 'grad_norm': 8.579402923583984, 'learning_rate': 2.660505856713014e-05, 'epoch': 3.85}
{'loss': 0.6475, 'grad_norm': 5.214366912841797, 'learning_rate': 2.6436593994027493e-05, 'epoch': 4.0}
{'loss': 0.3981, 'grad_norm': 5.833795547485352, 'learning_rate': 2.6260342409545568e-05, 'epoch': 4.15}
{'loss': 0.4723, 'grad_norm': 6.4929094314575195, 'learning_rate': 2.6076415137962796e-05, 'epoch': 4.31}
{'loss': 0.4501, 'grad_norm': 7.602312088012695, 'learning_rate': 2.58849283516861e-05, 'epoch': 4.46}
{'loss': 0.4906, 'grad_norm': 4.564793109893799, 'learning_rate': 2.5686002997873896e-05, 'epoch': 4.62}
{'loss': 0.3956, 'grad_norm': 8.821756362915039, 'learning_rate': 2.5479764722043303e-05, 'epoch': 4.77}
{'loss': 0.5043, 'grad_norm': 8.021865844726562, 'learning_rate': 2.526634378870973e-05, 'epoch': 4.92}
{'loss': 0.4007, 'grad_norm': 6.153326511383057, 'learning_rate': 2.5045874999109e-05, 'epoch': 5.08}
{'loss': 0.2639, 'grad_norm': 6.385464191436768, 'learning_rate': 2.4818497606054017e-05, 'epoch': 5.23}
{'loss': 0.2521, 'grad_norm': 9.062529563903809, 'learning_rate': 2.4584355225979715e-05, 'epoch': 5.38}
{'loss': 0.2983, 'grad_norm': 18.43766212463379, 'learning_rate': 2.4367965163764146e-05, 'epoch': 5.54}
{'loss': 0.2723, 'grad_norm': 5.3056111335754395, 'learning_rate': 2.4121380199830743e-05, 'epoch': 5.69}
{'loss': 0.2553, 'grad_norm': 7.019289016723633, 'learning_rate': 2.3868470563173377e-05, 'epoch': 5.85}
{'loss': 0.3307, 'grad_norm': 7.157144546508789, 'learning_rate': 2.360939599693928e-05, 'epoch': 6.0}
{'loss': 0.1402, 'grad_norm': 5.318406581878662, 'learning_rate': 2.334432013817734e-05, 'epoch': 6.15}
{'loss': 0.1232, 'grad_norm': 1.895493984222412, 'learning_rate': 2.3073410414481416e-05, 'epoch': 6.31}
{'loss': 0.1565, 'grad_norm': 9.265043258666992, 'learning_rate': 2.279683793823952e-05, 'epoch': 6.46}
{'loss': 0.2003, 'grad_norm': 9.735255241394043, 'learning_rate': 2.2514777398555524e-05, 'epoch': 6.62}
{'loss': 0.1599, 'grad_norm': 5.904176235198975, 'learning_rate': 2.2227406950911824e-05, 'epoch': 6.77}
{'loss': 0.1883, 'grad_norm': 5.123623371124268, 'learning_rate': 2.1934908104642465e-05, 'epoch': 6.92}
{'loss': 0.1055, 'grad_norm': 3.5119729042053223, 'learning_rate': 2.1637465608288002e-05, 'epoch': 7.08}
{'loss': 0.0606, 'grad_norm': 10.869818687438965, 'learning_rate': 2.1335267332904337e-05, 'epoch': 7.23}
{'loss': 0.0836, 'grad_norm': 3.137173652648926, 'learning_rate': 2.102850415339933e-05, 'epoch': 7.38}
{'loss': 0.0722, 'grad_norm': 6.3159098625183105, 'learning_rate': 2.071736982797215e-05, 'epoch': 7.54}
{'loss': 0.0976, 'grad_norm': 3.820222854614258, 'learning_rate': 2.0402060875731385e-05, 'epoch': 7.69}
{'loss': 0.0857, 'grad_norm': 26.73219108581543, 'learning_rate': 2.0082776452569427e-05, 'epoch': 7.85}
{'loss': 0.0986, 'grad_norm': 4.647565841674805, 'learning_rate': 1.9759718225371285e-05, 'epoch': 8.0}
{'loss': 0.0473, 'grad_norm': 4.952218532562256, 'learning_rate': 1.9433090244637506e-05, 'epoch': 8.15}
{'loss': 0.047, 'grad_norm': 5.033793926239014, 'learning_rate': 1.9103098815601467e-05, 'epoch': 8.31}
{'loss': 0.0413, 'grad_norm': 3.690150737762451, 'learning_rate': 1.8769952367922578e-05, 'epoch': 8.46}
{'loss': 0.05, 'grad_norm': 5.36082649230957, 'learning_rate': 1.8433861324037655e-05, 'epoch': 8.62}
{'loss': 0.0478, 'grad_norm': 3.997884511947632, 'learning_rate': 1.8095037966253593e-05, 'epoch': 8.77}
{'loss': 0.0445, 'grad_norm': 5.433896541595459, 'learning_rate': 1.77536963026653e-05, 'epoch': 8.92}
{'loss': 0.0395, 'grad_norm': 1.9685895442962646, 'learning_rate': 1.7410051931983624e-05, 'epoch': 9.08}
{'loss': 0.0246, 'grad_norm': 4.084749698638916, 'learning_rate': 1.7064321907358538e-05, 'epoch': 9.23}
{'loss': 0.0309, 'grad_norm': 1.765900731086731, 'learning_rate': 1.6716724599283777e-05, 'epoch': 9.38}
{'loss': 0.0375, 'grad_norm': 7.92549991607666, 'learning_rate': 1.6367479557669313e-05, 'epoch': 9.54}
{'loss': 0.0264, 'grad_norm': 3.011361598968506, 'learning_rate': 1.6016807373168928e-05, 'epoch': 9.69}
{'loss': 0.0253, 'grad_norm': 2.977080821990967, 'learning_rate': 1.566492953785046e-05, 'epoch': 9.85}
{'loss': 0.0246, 'grad_norm': 4.8141093254089355, 'learning_rate': 1.5312068305296632e-05, 'epoch': 10.0}
{'loss': 0.0091, 'grad_norm': 1.478650450706482, 'learning_rate': 1.4958446550224937e-05, 'epoch': 10.15}
{'loss': 0.0191, 'grad_norm': 5.604259967803955, 'learning_rate': 1.4604287627715197e-05, 'epoch': 10.31}
{'loss': 0.0156, 'grad_norm': 3.81449556350708, 'learning_rate': 1.42498152321337e-05, 'epoch': 10.46}
{'loss': 0.0132, 'grad_norm': 1.3621222972869873, 'learning_rate': 1.3895253255843044e-05, 'epoch': 10.62}
{'loss': 0.0183, 'grad_norm': 4.070272445678711, 'learning_rate': 1.354082564778692e-05, 'epoch': 10.77}
{'loss': 0.02, 'grad_norm': 3.013453722000122, 'learning_rate': 1.318675627203917e-05, 'epoch': 10.92}
{'loss': 0.0124, 'grad_norm': 2.93851637840271, 'learning_rate': 1.2833268766406424e-05, 'epoch': 11.08}
{'loss': 0.0081, 'grad_norm': 2.183311700820923, 'learning_rate': 1.248058640117368e-05, 'epoch': 11.23}
{'loss': 0.0093, 'grad_norm': 0.7330896258354187, 'learning_rate': 1.2128931938082001e-05, 'epoch': 11.38}
{'loss': 0.0101, 'grad_norm': 5.9332475662231445, 'learning_rate': 1.1778527489627424e-05, 'epoch': 11.54}
{'loss': 0.0088, 'grad_norm': 3.4977972507476807, 'learning_rate': 1.1429594378769954e-05, 'epoch': 11.69}
{'loss': 0.0119, 'grad_norm': 1.6194604635238647, 'learning_rate': 1.1082352999141228e-05, 'epoch': 11.85}
{'loss': 0.0099, 'grad_norm': 1.4790440797805786, 'learning_rate': 1.0737022675839187e-05, 'epoch': 12.0}
{'loss': 0.0054, 'grad_norm': 3.9634041786193848, 'learning_rate': 1.039382152689763e-05, 'epoch': 12.15}
{'loss': 0.0063, 'grad_norm': 2.7660579681396484, 'learning_rate': 1.0052966325518198e-05, 'epoch': 12.31}
{'loss': 0.0047, 'grad_norm': 1.709822177886963, 'learning_rate': 9.714672363151763e-06, 'epoch': 12.46}
{'loss': 0.0055, 'grad_norm': 0.3283061385154724, 'learning_rate': 9.37915331351575e-06, 'epoch': 12.62}
{'loss': 0.0055, 'grad_norm': 1.7967983484268188, 'learning_rate': 9.046621097633224e-06, 'epoch': 12.77}
{'loss': 0.0065, 'grad_norm': 2.0576260089874268, 'learning_rate': 8.717285749979027e-06, 'epoch': 12.92}
{'loss': 0.0032, 'grad_norm': 0.3163781464099884, 'learning_rate': 8.391355285817522e-06, 'epoch': 13.08}
{'loss': 0.0045, 'grad_norm': 0.7503535151481628, 'learning_rate': 8.069035569815658e-06, 'epoch': 13.23}
{'loss': 0.0033, 'grad_norm': 0.2853717505931854, 'learning_rate': 7.750530186014488e-06, 'epoch': 13.38}
{'loss': 0.0033, 'grad_norm': 3.6146271228790283, 'learning_rate': 7.436040309241056e-06, 'epoch': 13.54}
{'loss': 0.002, 'grad_norm': 0.34208929538726807, 'learning_rate': 7.125764578042134e-06, 'epoch': 13.69}
{'loss': 0.0032, 'grad_norm': 4.236904621124268, 'learning_rate': 6.819898969219801e-06, 'epoch': 13.85}
{'loss': 0.0028, 'grad_norm': 0.25965240597724915, 'learning_rate': 6.51863667404833e-06, 'epoch': 14.0}
{'loss': 0.0023, 'grad_norm': 0.16999518871307373, 'learning_rate': 6.222167976250432e-06, 'epoch': 14.15}
{'loss': 0.0018, 'grad_norm': 0.23271383345127106, 'learning_rate': 5.930680131809962e-06, 'epoch': 14.31}
{'loss': 0.0018, 'grad_norm': 0.2332826405763626, 'learning_rate': 5.644357250697052e-06, 'epoch': 14.46}
{'loss': 0.0015, 'grad_norm': 0.2162310630083084, 'learning_rate': 5.363380180580276e-06, 'epoch': 14.62}
{'loss': 0.0015, 'grad_norm': 0.15370658040046692, 'learning_rate': 5.087926392599419e-06, 'epoch': 14.77}
{'loss': 0.0018, 'grad_norm': 2.8075027465820312, 'learning_rate': 4.818169869270886e-06, 'epoch': 14.92}
{'loss': 0.0014, 'grad_norm': 0.3993624746799469, 'learning_rate': 4.554280994596601e-06, 'epoch': 15.08}
{'loss': 0.0012, 'grad_norm': 0.17026081681251526, 'learning_rate': 4.296426446445854e-06, 'epoch': 15.23}
{'loss': 0.0011, 'grad_norm': 0.12664537131786346, 'learning_rate': 4.044769091277945e-06, 'epoch': 15.38}
{'loss': 0.0014, 'grad_norm': 0.13084089756011963, 'learning_rate': 3.799467881272284e-06, 'epoch': 15.54}
{'loss': 0.0013, 'grad_norm': 0.11328762024641037, 'learning_rate': 3.560677753930738e-06, 'epoch': 15.69}
{'loss': 0.0012, 'grad_norm': 0.16139523684978485, 'learning_rate': 3.328549534215824e-06, 'epoch': 15.85}
{'loss': 0.0013, 'grad_norm': 0.3017568290233612, 'learning_rate': 3.103229839286417e-06, 'epoch': 16.0}
{'loss': 0.0011, 'grad_norm': 0.10526147484779358, 'learning_rate': 2.8848609858911963e-06, 'epoch': 16.15}
{'loss': 0.001, 'grad_norm': 0.1113201305270195, 'learning_rate': 2.673580900478354e-06, 'epoch': 16.31}
{'loss': 0.001, 'grad_norm': 0.14060941338539124, 'learning_rate': 2.4695230320782823e-06, 'epoch': 16.46}
{'loss': 0.001, 'grad_norm': 0.12986628711223602, 'learning_rate': 2.2728162680143214e-06, 'epoch': 16.62}
{'loss': 0.0011, 'grad_norm': 0.18075141310691833, 'learning_rate': 2.0835848524947536e-06, 'epoch': 16.77}
{'loss': 0.0012, 'grad_norm': 0.14590470492839813, 'learning_rate': 1.901948308137484e-06, 'epoch': 16.92}
{'loss': 0.0012, 'grad_norm': 0.1225975751876831, 'learning_rate': 1.7280213604770254e-06, 'epoch': 17.08}
{'loss': 0.001, 'grad_norm': 0.12770450115203857, 'learning_rate': 1.5619138655013607e-06, 'epoch': 17.23}
{'loss': 0.0011, 'grad_norm': 0.14785915613174438, 'learning_rate': 1.40373074026457e-06, 'epoch': 17.38}
{'loss': 0.001, 'grad_norm': 0.10992498695850372, 'learning_rate': 1.2535718966189497e-06, 'epoch': 17.54}
{'loss': 0.0011, 'grad_norm': 0.13228167593479156, 'learning_rate': 1.111532178108562e-06, 'epoch': 17.69}
{'loss': 0.001, 'grad_norm': 0.16381096839904785, 'learning_rate': 9.77701300064017e-07, 'epoch': 17.85}
{'loss': 0.0012, 'grad_norm': 0.13341857492923737, 'learning_rate': 8.521637929363331e-07, 'epoch': 18.0}
{'loss': 0.001, 'grad_norm': 0.15502677857875824, 'learning_rate': 7.349989489057116e-07, 'epoch': 18.15}
{'loss': 0.0011, 'grad_norm': 0.14191024005413055, 'learning_rate': 6.262807717988775e-07, 'epoch': 18.31}
{'loss': 0.001, 'grad_norm': 0.1580575853586197, 'learning_rate': 5.260779303466984e-07, 'epoch': 18.46}
{'loss': 0.0011, 'grad_norm': 0.16246838867664337, 'learning_rate': 4.3445371481151956e-07, 'epoch': 18.62}
{'loss': 0.0009, 'grad_norm': 0.11079823225736618, 'learning_rate': 3.514659970116916e-07, 'epoch': 18.77}
{'loss': 0.0011, 'grad_norm': 0.13558515906333923, 'learning_rate': 2.771671937684855e-07, 'epoch': 18.92}
{'loss': 0.0009, 'grad_norm': 0.17854900658130646, 'learning_rate': 2.1160423379850666e-07, 'epoch': 19.08}
{'loss': 0.001, 'grad_norm': 0.17097793519496918, 'learning_rate': 1.5481852807250942e-07, 'epoch': 19.23}
{'loss': 0.001, 'grad_norm': 0.11313788592815399, 'learning_rate': 1.0684594365934023e-07, 'epoch': 19.38}
{'loss': 0.001, 'grad_norm': 0.14764650166034698, 'learning_rate': 6.77167810715294e-08, 'epoch': 19.54}
{'loss': 0.001, 'grad_norm': 0.15381397306919098, 'learning_rate': 3.745575512684072e-08, 'epoch': 19.69}
{'loss': 0.0011, 'grad_norm': 0.11129633337259293, 'learning_rate': 1.6081979337855725e-08, 'epoch': 19.85}
{'loss': 0.001, 'grad_norm': 0.12894277274608612, 'learning_rate': 3.6089538394754887e-09, 'epoch': 20.0}
{'train_runtime': 6781.8278, 'train_samples_per_second': 0.767, 'train_steps_per_second': 0.192, 'train_loss': 0.29597554185356084, 'epoch': 20.0}
Training finished.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152/final_model...
Model saved successfully!

=== TEST EVALUATION ===
Starting evaluation on test.jsonl...
Loaded 48 test samples
Processing test image 1/48: inventarbuch-022.jpg
  Processed successfully. CER: 0.267
Processing test image 2/48: inventarbuch-099.jpg
  Processed successfully. CER: 0.141
Processing test image 3/48: inventarbuch-245.jpg
  Processed successfully. CER: 0.293
Processing test image 4/48: inventarbuch-263.jpg
  Processed successfully. CER: 0.169
Processing test image 5/48: inventarbuch-033.jpg
  Processed successfully. CER: 0.336
Processing test image 6/48: inventarbuch-143.jpg
  Processed successfully. CER: 0.182
Processing test image 7/48: inventarbuch-244.jpg
  Processed successfully. CER: 0.464
Processing test image 8/48: inventarbuch-024.jpg
  Processed successfully. CER: 0.062
Processing test image 9/48: inventarbuch-141.jpg
  Processed successfully. CER: 0.089
Processing test image 10/48: inventarbuch-183.jpg
  Processed successfully. CER: 0.393
Processing test image 11/48: inventarbuch-191.jpg
  Processed successfully. CER: 0.203
Processing test image 12/48: inventarbuch-051.jpg
  Processed successfully. CER: 0.058
Processing test image 13/48: inventarbuch-203.jpg
  Processed successfully. CER: 0.184
Processing test image 14/48: inventarbuch-296.jpg
  Processed successfully. CER: 0.243
Processing test image 15/48: inventarbuch-302.jpg
  Processed successfully. CER: 0.085
Processing test image 16/48: inventarbuch-179.jpg
  Processed successfully. CER: 0.218
Processing test image 17/48: inventarbuch-114.jpg
  Processed successfully. CER: 0.261
Processing test image 18/48: inventarbuch-082.jpg
  Processed successfully. CER: 0.315
Processing test image 19/48: inventarbuch-287.jpg
  Processed successfully. CER: 0.320
Processing test image 20/48: inventarbuch-181.jpg
  Processed successfully. CER: 0.276
Processing test image 21/48: inventarbuch-299.jpg
  Processed successfully. CER: 0.296
Processing test image 22/48: inventarbuch-084.jpg
  Processed successfully. CER: 0.413
Processing test image 23/48: inventarbuch-004.jpg
  Processed successfully. CER: 0.172
Processing test image 24/48: inventarbuch-148.jpg
  Processed successfully. CER: 0.100
Processing test image 25/48: inventarbuch-238.jpg
  Processed successfully. CER: 0.295
Processing test image 26/48: inventarbuch-116.jpg
  Processed successfully. CER: 0.144
Processing test image 27/48: inventarbuch-223.jpg
  Processed successfully. CER: 0.163
Processing test image 28/48: inventarbuch-104.jpg
  Processed successfully. CER: 0.254
Processing test image 29/48: inventarbuch-015.jpg
  Processed successfully. CER: 0.234
Processing test image 30/48: inventarbuch-272.jpg
  Processed successfully. CER: 0.264
Processing test image 31/48: inventarbuch-124.jpg
  Processed successfully. CER: 0.145
Processing test image 32/48: inventarbuch-115.jpg
  Processed successfully. CER: 0.193
Processing test image 33/48: inventarbuch-049.jpg
  Processed successfully. CER: 0.028
Processing test image 34/48: inventarbuch-017.jpg
  Processed successfully. CER: 0.047
Processing test image 35/48: inventarbuch-018.jpg
  Processed successfully. CER: 0.193
Processing test image 36/48: inventarbuch-225.jpg
  Processed successfully. CER: 0.370
Processing test image 37/48: inventarbuch-046.jpg
  Processed successfully. CER: 0.246
Processing test image 38/48: inventarbuch-294.jpg
  Processed successfully. CER: 0.152
Processing test image 39/48: inventarbuch-054.jpg
  Processed successfully. CER: 0.336
Processing test image 40/48: inventarbuch-073.jpg
  Processed successfully. CER: 0.329
Processing test image 41/48: inventarbuch-118.jpg
  Processed successfully. CER: 0.199
Processing test image 42/48: inventarbuch-130.jpg
  Processed successfully. CER: 1.000
Processing test image 43/48: inventarbuch-146.jpg
  Processed successfully. CER: 0.258
Processing test image 44/48: inventarbuch-014.jpg
  Processed successfully. CER: 0.216
Processing test image 45/48: inventarbuch-059.jpg
  Processed successfully. CER: 0.223
Processing test image 46/48: inventarbuch-256.jpg
  Processed successfully. CER: 0.150
Processing test image 47/48: inventarbuch-260.jpg
  Processed successfully. CER: 0.148
Processing test image 48/48: inventarbuch-279.jpg
  Processed successfully. CER: 0.276
CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152/final_model/cer_evaluation_results.txt

Evaluation completed!
Predictions saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152/final_model/test_predictions.jsonl
CER results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152/final_model/cer_evaluation_results.txt
All files saved in: /home/vault/iwi5/iwi5298h/models_image_text/qwen/inventory_dataset/run_20250926_212152

============================================================
FINAL RESULTS SUMMARY
============================================================
Average CER: 0.2375 (23.75%)
Median CER: 0.2204 (22.04%)
Perfect matches: 0/48 (0.00%)
Total images processed: 48

Training, HPO, and evaluation complete!
=== JOB_STATISTICS ===
=== current date     : Sat Sep 27 05:29:48 AM CEST 2025
= Job-ID             : 1227673 on tinygpu
= Job-Name           : qwen_inven
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 18:00:00
= Elapsed runtime    : 08:08:26
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg073
= Subm/Elig/Start/End: 2025-09-26T21:21:19 / 2025-09-26T21:21:19 / 2025-09-26T21:21:20 / 2025-09-27T05:29:46
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody            70.7G  1000.0G  1500.0G        N/A     392K   5,000K   7,500K        N/A    
    /home/hpc              65.8G   104.9G   209.7G        N/A  36,310      500K   1,000K        N/A    
    /home/vault           640.0G  1048.6G  2097.2G        N/A   3,685      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 2586740, 41 %, 18 %, 15200 MiB, 29296542 ms
