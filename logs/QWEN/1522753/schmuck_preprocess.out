### Starting TaskPrologue of job 1522753 on tg072 at Mon Feb  2 05:16:12 PM CET 2026
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Mon Feb  2 17:16:12 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.211.01             Driver Version: 570.211.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             28W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20260202_171711
Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
============================================================
STARTING TRAINING FOR SCHMUCK DATASET
============================================================
Preparing training and validation datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
7.854 GB of memory reserved.
Starting training with early stopping...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.9593, 'grad_norm': 32.30121994018555, 'learning_rate': 3e-06, 'epoch': 0.19}
{'loss': 2.4881, 'grad_norm': 5.862329959869385, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.39}
{'loss': 1.1777, 'grad_norm': 2.529088258743286, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.58}
{'loss': 0.2841, 'grad_norm': 3.0269927978515625, 'learning_rate': 3.3e-05, 'epoch': 0.77}
{'loss': 0.0432, 'grad_norm': 1.1821081638336182, 'learning_rate': 4.3e-05, 'epoch': 0.97}
{'eval_loss': 0.03569898381829262, 'eval_runtime': 62.2369, 'eval_samples_per_second': 1.414, 'eval_steps_per_second': 0.707, 'epoch': 0.99}
{'loss': 0.0375, 'grad_norm': 0.5849218368530273, 'learning_rate': 4.999782812965259e-05, 'epoch': 1.17}
{'loss': 0.028, 'grad_norm': 1.045673131942749, 'learning_rate': 4.995922759815339e-05, 'epoch': 1.37}
{'loss': 0.0266, 'grad_norm': 0.5324095487594604, 'learning_rate': 4.987244904838563e-05, 'epoch': 1.56}
{'loss': 0.0231, 'grad_norm': 0.6538407206535339, 'learning_rate': 4.973765998627628e-05, 'epoch': 1.75}
{'loss': 0.0196, 'grad_norm': 0.5435512065887451, 'learning_rate': 4.9555120590946336e-05, 'epoch': 1.95}
{'eval_loss': 0.023188244551420212, 'eval_runtime': 51.9622, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 0.847, 'epoch': 1.99}
{'loss': 0.0199, 'grad_norm': 0.5338481068611145, 'learning_rate': 4.9325183212495095e-05, 'epoch': 2.15}
{'loss': 0.0211, 'grad_norm': 2.343980312347412, 'learning_rate': 4.904829169186982e-05, 'epoch': 2.35}
{'loss': 0.0168, 'grad_norm': 0.6649322509765625, 'learning_rate': 4.872498050413334e-05, 'epoch': 2.54}
{'loss': 0.0161, 'grad_norm': 0.5292678475379944, 'learning_rate': 4.835587372678357e-05, 'epoch': 2.73}
{'loss': 0.019, 'grad_norm': 0.38198134303092957, 'learning_rate': 4.794168383511616e-05, 'epoch': 2.93}
{'eval_loss': 0.02676737867295742, 'eval_runtime': 51.7826, 'eval_samples_per_second': 1.699, 'eval_steps_per_second': 0.85, 'epoch': 2.99}
{'loss': 0.0294, 'grad_norm': 0.9561973810195923, 'learning_rate': 4.7483210326955673e-05, 'epoch': 3.14}
{'loss': 0.0264, 'grad_norm': 0.5688309669494629, 'learning_rate': 4.698133817940986e-05, 'epoch': 3.33}
{'loss': 0.0251, 'grad_norm': 0.665668785572052, 'learning_rate': 4.643703614062601e-05, 'epoch': 3.52}
{'loss': 0.0287, 'grad_norm': 1.0318578481674194, 'learning_rate': 4.585135485984656e-05, 'epoch': 3.71}
{'loss': 0.023, 'grad_norm': 0.4184770882129669, 'learning_rate': 4.522542485937369e-05, 'epoch': 3.91}
{'eval_loss': 0.04020959138870239, 'eval_runtime': 50.8527, 'eval_samples_per_second': 1.73, 'eval_steps_per_second': 0.865, 'epoch': 3.99}
{'loss': 0.0383, 'grad_norm': 1.0452631711959839, 'learning_rate': 4.4560454352357394e-05, 'epoch': 4.12}
{'loss': 0.0452, 'grad_norm': 0.9947757124900818, 'learning_rate': 4.3857726910619314e-05, 'epoch': 4.31}
{'loss': 0.0374, 'grad_norm': 1.9335947036743164, 'learning_rate': 4.311859898701413e-05, 'epoch': 4.5}
{'loss': 0.0314, 'grad_norm': 1.682611107826233, 'learning_rate': 4.234449729711091e-05, 'epoch': 4.7}
{'loss': 0.0357, 'grad_norm': 0.7387794256210327, 'learning_rate': 4.153691606524857e-05, 'epoch': 4.89}
{'eval_loss': 0.05876682698726654, 'eval_runtime': 53.0096, 'eval_samples_per_second': 1.66, 'eval_steps_per_second': 0.83, 'epoch': 4.99}
{'train_runtime': 3864.5829, 'train_samples_per_second': 1.603, 'train_steps_per_second': 0.198, 'train_loss': 0.33420991248944226, 'epoch': 4.99}
3864.5829 seconds used for training.
64.41 minutes used for training.
Peak reserved memory = 10.99 GB.
Peak reserved memory for training = 3.136 GB.
Peak reserved memory % of max memory = 34.633 %.
Peak reserved memory for training % of max memory = 9.882 %.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20260202_171711...
Model saved successfully!

============================================================
TRAINING COMPLETED
============================================================
All outputs saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20260202_171711
=== JOB_STATISTICS ===
=== current date     : Mon Feb  2 06:25:46 PM CET 2026
= Job-ID             : 1522753 on tinygpu
= Job-Name           : schmuck_preprocess
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 01:09:40
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2026-02-02T15:53:05 / 2026-02-02T15:53:05 / 2026-02-02T17:16:06 / 2026-02-02T18:25:46
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           408.0G  1000.0G  1500.0G        N/A   1,017K   5,000K   7,500K        N/A    
    /home/hpc              88.2G   104.9G   209.7G        N/A  29,434      500K   1,000K        N/A    
    /home/vault           764.2G  1048.6G  2097.2G        N/A   8,684      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 3721179, 55 %, 24 %, 11664 MiB, 4159405 ms
