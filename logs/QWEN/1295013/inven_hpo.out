### Starting TaskPrologue of job 1295013 on tg074 at Sun Nov  2 06:56:49 PM CET 2025
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Sun Nov  2 18:56:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!

============================================================
QWEN2.5-VL INVENTORY FINE-TUNING + OPTUNA HPO
============================================================
Config saved to /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/training_config.json

============================================================
PHASE 1: OPTUNA HPO (minimize validation CER)
============================================================
Completed trials: 0/20
Remaining trials: 20

================================================================================
OPTUNA TRIAL 1 (qwen_inven)
================================================================================
  â€¢ num_epochs:    5
  â€¢ learning_rate: 1.83e-04
  â€¢ batch_size:    1
  â€¢ weight_decay:  0.0408
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
7.854 GB of memory reserved.
Starting training with early stopping...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.5145, 'grad_norm': 14.31373119354248, 'learning_rate': 1.8305438737784603e-05, 'epoch': 0.19}
{'loss': 2.0501, 'grad_norm': 20.99287223815918, 'learning_rate': 5.49163162133538e-05, 'epoch': 0.38}
{'loss': 1.1516, 'grad_norm': 12.126303672790527, 'learning_rate': 9.152719368892301e-05, 'epoch': 0.56}
{'loss': 1.182, 'grad_norm': 3.2784626483917236, 'learning_rate': 0.0001281380711644922, 'epoch': 0.75}
{'loss': 1.0787, 'grad_norm': 3.863828659057617, 'learning_rate': 0.0001647489486400614, 'epoch': 0.94}
{'eval_loss': 1.0024285316467285, 'eval_runtime': 25.1561, 'eval_samples_per_second': 1.868, 'eval_steps_per_second': 1.868, 'epoch': 1.0}
{'loss': 0.8099, 'grad_norm': 4.20518159866333, 'learning_rate': 0.0001828102187880128, 'epoch': 1.11}
{'loss': 0.7695, 'grad_norm': 3.0037779808044434, 'learning_rate': 0.00018086467958906987, 'epoch': 1.3}
{'loss': 0.7387, 'grad_norm': 4.064018249511719, 'learning_rate': 0.0001770150669814611, 'epoch': 1.49}
{'loss': 0.7907, 'grad_norm': 2.435441255569458, 'learning_rate': 0.00017134342877441196, 'epoch': 1.68}
{'loss': 0.8997, 'grad_norm': 6.3366923332214355, 'learning_rate': 0.00016397064608948976, 'epoch': 1.86}
{'eval_loss': 0.9822254776954651, 'eval_runtime': 26.4843, 'eval_samples_per_second': 1.775, 'eval_steps_per_second': 1.775, 'epoch': 2.0}
{'loss': 0.7015, 'grad_norm': 4.880253791809082, 'learning_rate': 0.00015505385698936532, 'epoch': 2.04}
{'loss': 0.4464, 'grad_norm': 2.9890806674957275, 'learning_rate': 0.00014478310735282246, 'epoch': 2.23}
{'loss': 0.48, 'grad_norm': 2.042828321456909, 'learning_rate': 0.0001333773003767998, 'epoch': 2.41}
{'loss': 0.4831, 'grad_norm': 3.125844717025757, 'learning_rate': 0.00012107953103479702, 'epoch': 2.6}
{'loss': 0.3528, 'grad_norm': 1.796687126159668, 'learning_rate': 0.00010815190492956393, 'epoch': 2.79}
{'loss': 0.5082, 'grad_norm': 2.6713998317718506, 'learning_rate': 9.486995196723268e-05, 'epoch': 2.98}
{'eval_loss': 1.0283235311508179, 'eval_runtime': 25.8265, 'eval_samples_per_second': 1.82, 'eval_steps_per_second': 1.82, 'epoch': 3.0}
{'loss': 0.269, 'grad_norm': 4.558155059814453, 'learning_rate': 8.151675391572976e-05, 'epoch': 3.15}
{'loss': 0.1479, 'grad_norm': 3.523015022277832, 'learning_rate': 6.83769110083652e-05, 'epoch': 3.34}
{'loss': 0.1589, 'grad_norm': 1.7193132638931274, 'learning_rate': 5.573047618396841e-05, 'epoch': 3.53}
{'loss': 0.2083, 'grad_norm': 2.5938446521759033, 'learning_rate': 4.384698624471072e-05, 'epoch': 3.71}
{'loss': 0.1783, 'grad_norm': 8.912108421325684, 'learning_rate': 3.297971714712376e-05, 'epoch': 3.9}
{'eval_loss': 1.1747527122497559, 'eval_runtime': 24.4903, 'eval_samples_per_second': 1.919, 'eval_steps_per_second': 1.919, 'epoch': 4.0}
{'loss': 0.1306, 'grad_norm': 1.4401479959487915, 'learning_rate': 2.336028586481092e-05, 'epoch': 4.08}
{'loss': 0.058, 'grad_norm': 0.480620801448822, 'learning_rate': 1.5193713874777022e-05, 'epoch': 4.26}
{'loss': 0.0603, 'grad_norm': 1.651080846786499, 'learning_rate': 8.654057480593817e-06, 'epoch': 4.45}
{'loss': 0.0535, 'grad_norm': 1.6394013166427612, 'learning_rate': 3.880698104477478e-06, 'epoch': 4.64}
{'loss': 0.0782, 'grad_norm': 1.262674331665039, 'learning_rate': 9.753716142609443e-07, 'epoch': 4.83}
{'eval_loss': 1.2899664640426636, 'eval_runtime': 25.679, 'eval_samples_per_second': 1.83, 'eval_steps_per_second': 1.83, 'epoch': 4.92}
{'train_runtime': 1953.9978, 'train_samples_per_second': 0.545, 'train_steps_per_second': 0.136, 'train_loss': 0.6539088817137593, 'epoch': 4.92}
1953.9978 seconds used for training.
32.57 minutes used for training.
Peak reserved memory = 8.277 GB.
Peak reserved memory for training = 0.423 GB.
Peak reserved memory % of max memory = 26.083 %.
Peak reserved memory for training % of max memory = 1.333 %.
Evaluating on validation set at epoch 5...
Validation CER at epoch 5: 0.2357
  âœ… Trial 1 validation CER = 0.2357
[I 2025-11-02 19:52:22,797] Trial 1 finished with value: 0.235685153665821 and parameters: {'num_epochs': 5, 'learning_rate': 0.00018305438737784601, 'batch_size': 1, 'weight_decay': 0.04075429887272519}. Best is trial 1 with value: 0.235685153665821.

================================================================================
OPTUNA TRIAL 2 (qwen_inven)
================================================================================
  â€¢ num_epochs:    12
  â€¢ learning_rate: 1.16e-05
  â€¢ batch_size:    1
  â€¢ weight_decay:  0.0802
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.084 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7624, 'grad_norm': 19.265953063964844, 'learning_rate': 1.3868233750156767e-06, 'epoch': 0.19}
{'loss': 3.7049, 'grad_norm': 15.985673904418945, 'learning_rate': 3.235921208369913e-06, 'epoch': 0.38}
{'loss': 3.0815, 'grad_norm': 19.135791778564453, 'learning_rate': 5.316156270893428e-06, 'epoch': 0.56}
{'loss': 2.4546, 'grad_norm': 11.38050651550293, 'learning_rate': 7.6275285625862225e-06, 'epoch': 0.75}
{'loss': 2.0143, 'grad_norm': 4.772460460662842, 'learning_rate': 9.938900854279017e-06, 'epoch': 0.94}
{'eval_loss': 1.5144230127334595, 'eval_runtime': 26.8503, 'eval_samples_per_second': 1.75, 'eval_steps_per_second': 1.75, 'epoch': 1.0}
{'loss': 1.4381, 'grad_norm': 11.224884033203125, 'learning_rate': 1.155611411958385e-05, 'epoch': 1.11}
{'loss': 1.1586, 'grad_norm': 6.764093399047852, 'learning_rate': 1.1542833472051869e-05, 'epoch': 1.3}
{'loss': 1.0305, 'grad_norm': 6.969120979309082, 'learning_rate': 1.1512989221651786e-05, 'epoch': 1.49}
{'loss': 1.0362, 'grad_norm': 4.422817707061768, 'learning_rate': 1.1466667123722638e-05, 'epoch': 1.68}
{'loss': 1.1396, 'grad_norm': 3.8620355129241943, 'learning_rate': 1.1404000281530853e-05, 'epoch': 1.86}
{'eval_loss': 1.037601113319397, 'eval_runtime': 26.2771, 'eval_samples_per_second': 1.789, 'eval_steps_per_second': 1.789, 'epoch': 2.0}
{'loss': 0.8899, 'grad_norm': 5.022408485412598, 'learning_rate': 1.1325168763807468e-05, 'epoch': 2.04}
{'loss': 0.9109, 'grad_norm': 4.044623851776123, 'learning_rate': 1.123039908733337e-05, 'epoch': 2.23}
{'loss': 0.9283, 'grad_norm': 7.028414726257324, 'learning_rate': 1.111996356605939e-05, 'epoch': 2.41}
{'loss': 0.9489, 'grad_norm': 4.122922897338867, 'learning_rate': 1.0994179528631446e-05, 'epoch': 2.6}
{'loss': 0.7742, 'grad_norm': 5.8494439125061035, 'learning_rate': 1.0853408406569168e-05, 'epoch': 2.79}
{'loss': 1.0058, 'grad_norm': 9.068699836730957, 'learning_rate': 1.0698054695718036e-05, 'epoch': 2.98}
{'eval_loss': 0.9932129383087158, 'eval_runtime': 26.5766, 'eval_samples_per_second': 1.768, 'eval_steps_per_second': 1.768, 'epoch': 3.0}
{'loss': 0.7929, 'grad_norm': 2.900075912475586, 'learning_rate': 1.052856479395922e-05, 'epoch': 3.15}
{'loss': 0.8358, 'grad_norm': 4.259698867797852, 'learning_rate': 1.0345425718516912e-05, 'epoch': 3.34}
{'loss': 0.6994, 'grad_norm': 3.5912506580352783, 'learning_rate': 1.014916370654881e-05, 'epoch': 3.53}
{'loss': 0.8665, 'grad_norm': 4.374929904937744, 'learning_rate': 9.940342703040957e-06, 'epoch': 3.71}
{'loss': 0.7733, 'grad_norm': 5.770328044891357, 'learning_rate': 9.719562740351769e-06, 'epoch': 3.9}
{'eval_loss': 0.9834411144256592, 'eval_runtime': 27.4805, 'eval_samples_per_second': 1.71, 'eval_steps_per_second': 1.71, 'epoch': 4.0}
{'loss': 0.7323, 'grad_norm': 3.436023473739624, 'learning_rate': 9.487458214061564e-06, 'epoch': 4.08}
{'loss': 0.7154, 'grad_norm': 5.133464336395264, 'learning_rate': 9.244696060081808e-06, 'epoch': 4.26}
{'loss': 0.6748, 'grad_norm': 5.4685378074646, 'learning_rate': 8.991973838262039e-06, 'epoch': 4.45}
{'loss': 0.6968, 'grad_norm': 4.260049819946289, 'learning_rate': 8.730017728001043e-06, 'epoch': 4.64}
{'loss': 0.7382, 'grad_norm': 5.273393154144287, 'learning_rate': 8.459580441621818e-06, 'epoch': 4.83}
{'loss': 0.5585, 'grad_norm': 2.799384117126465, 'learning_rate': 8.181439061506036e-06, 'epoch': 5.0}
{'eval_loss': 1.0058865547180176, 'eval_runtime': 26.9346, 'eval_samples_per_second': 1.745, 'eval_steps_per_second': 1.745, 'epoch': 5.0}
{'loss': 0.5521, 'grad_norm': 5.061845302581787, 'learning_rate': 7.89639280720288e-06, 'epoch': 5.19}
{'loss': 0.5858, 'grad_norm': 4.5858025550842285, 'learning_rate': 7.605260738928295e-06, 'epoch': 5.38}
{'loss': 0.5874, 'grad_norm': 4.491359233856201, 'learning_rate': 7.308879404053482e-06, 'epoch': 5.56}
{'loss': 0.5599, 'grad_norm': 4.288106441497803, 'learning_rate': 7.008100433345309e-06, 'epoch': 5.75}
{'loss': 0.5399, 'grad_norm': 12.300568580627441, 'learning_rate': 6.703788093865627e-06, 'epoch': 5.94}
{'eval_loss': 1.046096682548523, 'eval_runtime': 25.477, 'eval_samples_per_second': 1.845, 'eval_steps_per_second': 1.845, 'epoch': 6.0}
{'loss': 0.4354, 'grad_norm': 5.851154327392578, 'learning_rate': 6.396816805561116e-06, 'epoch': 6.11}
{'loss': 0.4794, 'grad_norm': 5.861605167388916, 'learning_rate': 6.088068628679514e-06, 'epoch': 6.3}
{'loss': 0.4776, 'grad_norm': 4.943459510803223, 'learning_rate': 5.809409208019796e-06, 'epoch': 6.49}
{'loss': 0.4652, 'grad_norm': 5.548851013183594, 'learning_rate': 5.499731251008109e-06, 'epoch': 6.68}
{'loss': 0.4733, 'grad_norm': 49.334815979003906, 'learning_rate': 5.190854117200976e-06, 'epoch': 6.86}
{'eval_loss': 1.0941848754882812, 'eval_runtime': 25.5995, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 1.836, 'epoch': 7.0}
{'train_runtime': 2792.7245, 'train_samples_per_second': 0.915, 'train_steps_per_second': 0.228, 'train_loss': 1.0537982863723916, 'epoch': 7.0}
2792.7245 seconds used for training.
46.55 minutes used for training.
Peak reserved memory = 13.084 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.232 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 12...
Validation CER at epoch 12: 0.2471
  âœ… Trial 2 validation CER = 0.2471
[I 2025-11-02 20:59:07,382] Trial 2 finished with value: 0.24711776229165996 and parameters: {'num_epochs': 12, 'learning_rate': 1.1556861458463973e-05, 'batch_size': 1, 'weight_decay': 0.08015800889786145}. Best is trial 1 with value: 0.235685153665821.

================================================================================
OPTUNA TRIAL 3 (qwen_inven)
================================================================================
  â€¢ num_epochs:    8
  â€¢ learning_rate: 2.86e-04
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0673
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.084 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.2823, 'grad_norm': 14.37016773223877, 'learning_rate': 3.435091117894892e-05, 'epoch': 0.37}
{'loss': 1.7137, 'grad_norm': 4.540680885314941, 'learning_rate': 9.160242981053045e-05, 'epoch': 0.75}
{'eval_loss': 1.1173648834228516, 'eval_runtime': 20.2272, 'eval_samples_per_second': 2.324, 'eval_steps_per_second': 1.187, 'epoch': 0.97}
{'loss': 1.1617, 'grad_norm': 3.129627227783203, 'learning_rate': 0.000148853948442112, 'epoch': 1.15}
{'loss': 0.902, 'grad_norm': 2.0408742427825928, 'learning_rate': 0.00020610546707369352, 'epoch': 1.52}
{'loss': 1.0059, 'grad_norm': 2.9403111934661865, 'learning_rate': 0.00026335698570527507, 'epoch': 1.9}
{'eval_loss': 1.156418800354004, 'eval_runtime': 20.5179, 'eval_samples_per_second': 2.291, 'eval_steps_per_second': 1.17, 'epoch': 1.97}
{'loss': 0.7472, 'grad_norm': 2.9381628036499023, 'learning_rate': 0.0002852402442206992, 'epoch': 2.3}
{'loss': 0.8246, 'grad_norm': 4.739725112915039, 'learning_rate': 0.00027907540899961477, 'epoch': 2.67}
{'eval_loss': 1.1054009199142456, 'eval_runtime': 19.9312, 'eval_samples_per_second': 2.358, 'eval_steps_per_second': 1.204, 'epoch': 2.97}
{'loss': 0.7302, 'grad_norm': 2.4869654178619385, 'learning_rate': 0.00026755356130117716, 'epoch': 3.07}
{'loss': 0.4012, 'grad_norm': 8.335983276367188, 'learning_rate': 0.00025112872260325673, 'epoch': 3.45}
{'loss': 0.4532, 'grad_norm': 3.1956193447113037, 'learning_rate': 0.00023044811805016972, 'epoch': 3.82}
{'eval_loss': 1.1771470308303833, 'eval_runtime': 20.8346, 'eval_samples_per_second': 2.256, 'eval_steps_per_second': 1.152, 'epoch': 3.97}
{'loss': 0.3747, 'grad_norm': 2.009279251098633, 'learning_rate': 0.00020632667237345192, 'epoch': 4.22}
{'loss': 0.2353, 'grad_norm': 3.239370107650757, 'learning_rate': 0.00017971489756572374, 'epoch': 4.6}
{'loss': 0.2237, 'grad_norm': 5.228077411651611, 'learning_rate': 0.00015166143770241385, 'epoch': 4.97}
{'eval_loss': 1.2712630033493042, 'eval_runtime': 19.8642, 'eval_samples_per_second': 2.366, 'eval_steps_per_second': 1.208, 'epoch': 4.97}
{'loss': 0.0954, 'grad_norm': 1.6462445259094238, 'learning_rate': 0.00012327174684261755, 'epoch': 5.37}
{'loss': 0.1056, 'grad_norm': 9.79202938079834, 'learning_rate': 9.56645283174155e-05, 'epoch': 5.75}
{'eval_loss': 1.3685202598571777, 'eval_runtime': 20.7625, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 1.156, 'epoch': 5.97}
{'train_runtime': 1760.6601, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.118, 'train_loss': 0.7887863910351044, 'epoch': 5.97}
1760.6601 seconds used for training.
29.34 minutes used for training.
Peak reserved memory = 13.084 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.232 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 8...
Validation CER at epoch 8: 0.2439
  âœ… Trial 3 validation CER = 0.2439
[I 2025-11-02 21:51:18,745] Trial 3 finished with value: 0.24387424130511576 and parameters: {'num_epochs': 8, 'learning_rate': 0.00028625759315790766, 'batch_size': 2, 'weight_decay': 0.06729804754742079}. Best is trial 1 with value: 0.235685153665821.

================================================================================
OPTUNA TRIAL 4 (qwen_inven)
================================================================================
  â€¢ num_epochs:    5
  â€¢ learning_rate: 2.49e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0405
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.718, 'grad_norm': 11.245396614074707, 'learning_rate': 2.991698450640289e-06, 'epoch': 0.37}
{'loss': 3.1497, 'grad_norm': 7.740236282348633, 'learning_rate': 7.479246126600723e-06, 'epoch': 0.75}
{'eval_loss': 2.1654481887817383, 'eval_runtime': 20.0418, 'eval_samples_per_second': 2.345, 'eval_steps_per_second': 1.197, 'epoch': 0.97}
{'loss': 2.4199, 'grad_norm': 26.86973762512207, 'learning_rate': 1.2465410211001204e-05, 'epoch': 1.15}
{'loss': 1.6689, 'grad_norm': 4.190278053283691, 'learning_rate': 1.7451574295401686e-05, 'epoch': 1.52}
{'loss': 1.3911, 'grad_norm': 15.816817283630371, 'learning_rate': 2.2437738379802168e-05, 'epoch': 1.9}
{'eval_loss': 1.1675907373428345, 'eval_runtime': 20.1679, 'eval_samples_per_second': 2.33, 'eval_steps_per_second': 1.19, 'epoch': 1.97}
{'loss': 1.115, 'grad_norm': 21.866390228271484, 'learning_rate': 2.4691301060139313e-05, 'epoch': 2.3}
{'loss': 1.022, 'grad_norm': 2.247439384460449, 'learning_rate': 2.2830020006334564e-05, 'epoch': 2.67}
{'eval_loss': 1.058341383934021, 'eval_runtime': 20.3652, 'eval_samples_per_second': 2.308, 'eval_steps_per_second': 1.178, 'epoch': 2.97}
{'loss': 1.0915, 'grad_norm': 5.859577655792236, 'learning_rate': 1.9390821066612072e-05, 'epoch': 3.07}
{'loss': 0.9005, 'grad_norm': 5.360953330993652, 'learning_rate': 1.4897291103128564e-05, 'epoch': 3.45}
{'loss': 0.9111, 'grad_norm': 2.143725872039795, 'learning_rate': 1.0033529318873846e-05, 'epoch': 3.82}
{'eval_loss': 1.0390437841415405, 'eval_runtime': 20.9989, 'eval_samples_per_second': 2.238, 'eval_steps_per_second': 1.143, 'epoch': 3.97}
{'loss': 0.932, 'grad_norm': 3.5265300273895264, 'learning_rate': 5.53999935539034e-06, 'epoch': 4.22}
{'loss': 0.8522, 'grad_norm': 3.7425715923309326, 'learning_rate': 2.100800415667843e-06, 'epoch': 4.6}
{'loss': 0.8338, 'grad_norm': 3.5402979850769043, 'learning_rate': 2.395193618630963e-07, 'epoch': 4.97}
{'eval_loss': 1.0383150577545166, 'eval_runtime': 19.9599, 'eval_samples_per_second': 2.355, 'eval_steps_per_second': 1.202, 'epoch': 4.97}
{'train_runtime': 1464.6512, 'train_samples_per_second': 0.727, 'train_steps_per_second': 0.089, 'train_loss': 1.5388988494873046, 'epoch': 4.97}
1464.6512 seconds used for training.
24.41 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 5...
Validation CER at epoch 5: 0.2235
  âœ… Trial 4 validation CER = 0.2235
[I 2025-11-02 22:37:49,623] Trial 4 finished with value: 0.2235139214489868 and parameters: {'num_epochs': 5, 'learning_rate': 2.493082042200241e-05, 'batch_size': 2, 'weight_decay': 0.04051604051757162}. Best is trial 4 with value: 0.2235139214489868.

================================================================================
OPTUNA TRIAL 5 (qwen_inven)
================================================================================
  â€¢ num_epochs:    14
  â€¢ learning_rate: 1.97e-05
  â€¢ batch_size:    1
  â€¢ weight_decay:  0.0444
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7556, 'grad_norm': 20.14804458618164, 'learning_rate': 1.9715514258389334e-06, 'epoch': 0.19}
{'loss': 3.4689, 'grad_norm': 9.808362007141113, 'learning_rate': 5.914654277516799e-06, 'epoch': 0.38}
{'loss': 2.5318, 'grad_norm': 19.983165740966797, 'learning_rate': 9.857757129194666e-06, 'epoch': 0.56}
{'loss': 2.0064, 'grad_norm': 13.275196075439453, 'learning_rate': 1.3800859980872532e-05, 'epoch': 0.75}
{'loss': 1.5314, 'grad_norm': 7.213050365447998, 'learning_rate': 1.77439628325504e-05, 'epoch': 0.94}
{'eval_loss': 1.1664493083953857, 'eval_runtime': 25.9618, 'eval_samples_per_second': 1.81, 'eval_steps_per_second': 1.81, 'epoch': 1.0}
{'loss': 1.0758, 'grad_norm': 5.734807968139648, 'learning_rate': 1.9713888920263616e-05, 'epoch': 1.11}
{'loss': 1.001, 'grad_norm': 3.39509916305542, 'learning_rate': 1.9695610021055266e-05, 'epoch': 1.3}
{'loss': 0.9308, 'grad_norm': 3.544344663619995, 'learning_rate': 1.9661823848341727e-05, 'epoch': 1.49}
{'loss': 0.9615, 'grad_norm': 9.158991813659668, 'learning_rate': 1.9605093405130787e-05, 'epoch': 1.68}
{'loss': 1.0772, 'grad_norm': 7.2254438400268555, 'learning_rate': 1.9528276709659296e-05, 'epoch': 1.86}
{'eval_loss': 0.9947372674942017, 'eval_runtime': 25.3727, 'eval_samples_per_second': 1.852, 'eval_steps_per_second': 1.852, 'epoch': 2.0}
{'loss': 0.8411, 'grad_norm': 6.802219390869141, 'learning_rate': 1.9431532057432747e-05, 'epoch': 2.04}
{'loss': 0.8141, 'grad_norm': 5.122447490692139, 'learning_rate': 1.9315058809326208e-05, 'epoch': 2.23}
{'loss': 0.8294, 'grad_norm': 3.0746634006500244, 'learning_rate': 1.9179096980763047e-05, 'epoch': 2.41}
{'loss': 0.8773, 'grad_norm': 3.504901885986328, 'learning_rate': 1.9023926747117243e-05, 'epoch': 2.6}
{'loss': 0.6872, 'grad_norm': 3.286214590072632, 'learning_rate': 1.8849867866358367e-05, 'epoch': 2.79}
{'loss': 0.9115, 'grad_norm': 4.6667938232421875, 'learning_rate': 1.865727902012909e-05, 'epoch': 2.98}
{'eval_loss': 0.969334065914154, 'eval_runtime': 26.323, 'eval_samples_per_second': 1.786, 'eval_steps_per_second': 1.786, 'epoch': 3.0}
{'loss': 0.6767, 'grad_norm': 2.7974557876586914, 'learning_rate': 1.8446557074612987e-05, 'epoch': 3.15}
{'loss': 0.6953, 'grad_norm': 4.979987144470215, 'learning_rate': 1.8218136262715823e-05, 'epoch': 3.34}
{'loss': 0.5849, 'grad_norm': 4.466043949127197, 'learning_rate': 1.7972487289245494e-05, 'epoch': 3.53}
{'loss': 0.7271, 'grad_norm': 7.249000072479248, 'learning_rate': 1.7710116360934697e-05, 'epoch': 3.71}
{'loss': 0.6413, 'grad_norm': 4.054844856262207, 'learning_rate': 1.743156414330503e-05, 'epoch': 3.9}
{'eval_loss': 0.9900546073913574, 'eval_runtime': 26.7624, 'eval_samples_per_second': 1.756, 'eval_steps_per_second': 1.756, 'epoch': 4.0}
{'loss': 0.6043, 'grad_norm': 3.656135082244873, 'learning_rate': 1.713740464652221e-05, 'epoch': 4.08}
{'loss': 0.5098, 'grad_norm': 4.407050609588623, 'learning_rate': 1.6828244042538245e-05, 'epoch': 4.26}
{'loss': 0.516, 'grad_norm': 3.60085129737854, 'learning_rate': 1.6504719415958084e-05, 'epoch': 4.45}
{'loss': 0.5294, 'grad_norm': 6.433830738067627, 'learning_rate': 1.616749745120483e-05, 'epoch': 4.64}
{'loss': 0.5179, 'grad_norm': 5.621450424194336, 'learning_rate': 1.5817273058688888e-05, 'epoch': 4.83}
{'loss': 0.4109, 'grad_norm': 2.838491439819336, 'learning_rate': 1.5454767942812027e-05, 'epoch': 5.0}
{'eval_loss': 1.0705610513687134, 'eval_runtime': 27.0856, 'eval_samples_per_second': 1.735, 'eval_steps_per_second': 1.735, 'epoch': 5.0}
{'loss': 0.3297, 'grad_norm': 5.356112957000732, 'learning_rate': 1.508072911475732e-05, 'epoch': 5.19}
{'loss': 0.3784, 'grad_norm': 23.298175811767578, 'learning_rate': 1.4695927353129589e-05, 'epoch': 5.38}
{'loss': 0.3306, 'grad_norm': 5.99856424331665, 'learning_rate': 1.4301155615618562e-05, 'epoch': 5.56}
{'loss': 0.3412, 'grad_norm': 5.660808086395264, 'learning_rate': 1.3897227404957758e-05, 'epoch': 5.75}
{'loss': 0.3096, 'grad_norm': 4.3684234619140625, 'learning_rate': 1.3484975092546375e-05, 'epoch': 5.94}
{'eval_loss': 1.1788495779037476, 'eval_runtime': 27.1873, 'eval_samples_per_second': 1.729, 'eval_steps_per_second': 1.729, 'epoch': 6.0}
{'train_runtime': 2375.3777, 'train_samples_per_second': 1.255, 'train_steps_per_second': 0.312, 'train_loss': 0.9735031896903191, 'epoch': 6.0}
2375.3777 seconds used for training.
39.59 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 14...
Validation CER at epoch 14: 0.2540
  âœ… Trial 5 validation CER = 0.2540
[I 2025-11-02 23:40:12,263] Trial 5 finished with value: 0.25399017309483285 and parameters: {'num_epochs': 14, 'learning_rate': 1.971551425838933e-05, 'batch_size': 1, 'weight_decay': 0.044364620298950344}. Best is trial 4 with value: 0.2235139214489868.

================================================================================
OPTUNA TRIAL 6 (qwen_inven)
================================================================================
  â€¢ num_epochs:    4
  â€¢ learning_rate: 3.79e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0264
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.6882, 'grad_norm': 12.834002494812012, 'learning_rate': 4.553694065804359e-06, 'epoch': 0.37}
{'loss': 2.8567, 'grad_norm': 7.066934585571289, 'learning_rate': 1.214318417547829e-05, 'epoch': 0.75}
{'eval_loss': 1.8539856672286987, 'eval_runtime': 20.4798, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 1.172, 'epoch': 0.97}
{'loss': 2.0776, 'grad_norm': 15.34866714477539, 'learning_rate': 1.9732674285152223e-05, 'epoch': 1.15}
{'loss': 1.329, 'grad_norm': 5.367756366729736, 'learning_rate': 2.7322164394826152e-05, 'epoch': 1.52}
{'loss': 1.2159, 'grad_norm': 14.676011085510254, 'learning_rate': 3.491165450450009e-05, 'epoch': 1.9}
{'eval_loss': 1.0909104347229004, 'eval_runtime': 20.5396, 'eval_samples_per_second': 2.288, 'eval_steps_per_second': 1.168, 'epoch': 1.97}
{'loss': 1.0235, 'grad_norm': 3.396169662475586, 'learning_rate': 3.6803194903155396e-05, 'epoch': 2.3}
{'loss': 0.9437, 'grad_norm': 12.45722770690918, 'learning_rate': 3.03040483382726e-05, 'epoch': 2.67}
{'eval_loss': 1.027713418006897, 'eval_runtime': 19.8456, 'eval_samples_per_second': 2.368, 'eval_steps_per_second': 1.209, 'epoch': 2.97}
{'loss': 1.0092, 'grad_norm': 3.7722015380859375, 'learning_rate': 2.0076949284046676e-05, 'epoch': 3.07}
{'loss': 0.7965, 'grad_norm': 2.4963889122009277, 'learning_rate': 9.486862637092418e-06, 'epoch': 3.45}
{'loss': 0.8161, 'grad_norm': 2.901566982269287, 'learning_rate': 2.5419971823139025e-06, 'epoch': 3.82}
{'eval_loss': 1.028019905090332, 'eval_runtime': 20.4162, 'eval_samples_per_second': 2.302, 'eval_steps_per_second': 1.176, 'epoch': 3.97}
{'train_runtime': 1186.08, 'train_samples_per_second': 0.718, 'train_steps_per_second': 0.088, 'train_loss': 1.5478229751953712, 'epoch': 3.97}
1186.08 seconds used for training.
19.77 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 4...
Validation CER at epoch 4: 0.2224
  âœ… Trial 6 validation CER = 0.2224
[I 2025-11-03 00:22:46,236] Trial 6 finished with value: 0.2223578149587334 and parameters: {'num_epochs': 4, 'learning_rate': 3.794745054836966e-05, 'batch_size': 2, 'weight_decay': 0.026431049671015117}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 7 (qwen_inven)
================================================================================
  â€¢ num_epochs:    11
  â€¢ learning_rate: 6.07e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0780
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.6389, 'grad_norm': 14.676529884338379, 'learning_rate': 7.2797035340603175e-06, 'epoch': 0.37}
{'loss': 2.5922, 'grad_norm': 6.07681941986084, 'learning_rate': 1.941254275749418e-05, 'epoch': 0.75}
{'eval_loss': 1.5631564855575562, 'eval_runtime': 20.6633, 'eval_samples_per_second': 2.275, 'eval_steps_per_second': 1.161, 'epoch': 0.97}
{'loss': 1.7867, 'grad_norm': 7.5892415046691895, 'learning_rate': 3.0332098058584656e-05, 'epoch': 1.15}
{'loss': 1.1372, 'grad_norm': 3.5044844150543213, 'learning_rate': 4.246493728201852e-05, 'epoch': 1.52}
{'loss': 1.1488, 'grad_norm': 2.6908257007598877, 'learning_rate': 5.4597776505452385e-05, 'epoch': 1.9}
{'eval_loss': 1.0540210008621216, 'eval_runtime': 20.6328, 'eval_samples_per_second': 2.278, 'eval_steps_per_second': 1.163, 'epoch': 1.97}
{'loss': 0.9513, 'grad_norm': 2.7906076908111572, 'learning_rate': 6.059703346054328e-05, 'epoch': 2.3}
{'loss': 0.8871, 'grad_norm': 4.353263854980469, 'learning_rate': 6.006151546423541e-05, 'epoch': 2.67}
{'eval_loss': 1.013393759727478, 'eval_runtime': 20.7037, 'eval_samples_per_second': 2.27, 'eval_steps_per_second': 1.159, 'epoch': 2.97}
{'loss': 0.92, 'grad_norm': 2.148735523223877, 'learning_rate': 5.899995510804713e-05, 'epoch': 3.07}
{'loss': 0.6468, 'grad_norm': 15.027920722961426, 'learning_rate': 5.743113599971974e-05, 'epoch': 3.45}
{'loss': 0.6632, 'grad_norm': 10.113182067871094, 'learning_rate': 5.538281735480426e-05, 'epoch': 3.82}
{'eval_loss': 1.0793795585632324, 'eval_runtime': 21.1056, 'eval_samples_per_second': 2.227, 'eval_steps_per_second': 1.137, 'epoch': 3.97}
{'loss': 0.6257, 'grad_norm': 2.765068292617798, 'learning_rate': 5.289124281571911e-05, 'epoch': 4.22}
{'loss': 0.4654, 'grad_norm': 23.110519409179688, 'learning_rate': 5.000049914449109e-05, 'epoch': 4.6}
{'loss': 0.4451, 'grad_norm': 5.200618267059326, 'learning_rate': 4.676173613668711e-05, 'epoch': 4.97}
{'eval_loss': 1.1496644020080566, 'eval_runtime': 20.7597, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 1.156, 'epoch': 4.97}
{'loss': 0.3026, 'grad_norm': 5.473536968231201, 'learning_rate': 4.323226155964319e-05, 'epoch': 5.37}
{'loss': 0.2618, 'grad_norm': 3.7039763927459717, 'learning_rate': 3.9474527129459645e-05, 'epoch': 5.75}
{'eval_loss': 1.27959406375885, 'eval_runtime': 20.6647, 'eval_samples_per_second': 2.274, 'eval_steps_per_second': 1.161, 'epoch': 5.97}
{'train_runtime': 1790.3343, 'train_samples_per_second': 1.309, 'train_steps_per_second': 0.16, 'train_loss': 1.065613722954041, 'epoch': 5.97}
1790.3343 seconds used for training.
29.84 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 11...
Validation CER at epoch 11: 0.2394
  âœ… Trial 7 validation CER = 0.2394
[I 2025-11-03 01:15:49,566] Trial 7 finished with value: 0.23941038568997078 and parameters: {'num_epochs': 11, 'learning_rate': 6.066419611716931e-05, 'batch_size': 2, 'weight_decay': 0.07795583394302537}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 8 (qwen_inven)
================================================================================
  â€¢ num_epochs:    5
  â€¢ learning_rate: 2.49e-05
  â€¢ batch_size:    1
  â€¢ weight_decay:  0.0555
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7409, 'grad_norm': 14.30766773223877, 'learning_rate': 2.990028830787858e-06, 'epoch': 0.19}
{'loss': 3.2247, 'grad_norm': 14.096238136291504, 'learning_rate': 7.973410215434288e-06, 'epoch': 0.38}
{'loss': 2.3401, 'grad_norm': 5.5936102867126465, 'learning_rate': 1.295679160008072e-05, 'epoch': 0.56}
{'loss': 1.7845, 'grad_norm': 6.380919456481934, 'learning_rate': 1.794017298472715e-05, 'epoch': 0.75}
{'loss': 1.3412, 'grad_norm': 8.579176902770996, 'learning_rate': 2.292355436937358e-05, 'epoch': 0.94}
{'eval_loss': 1.1033384799957275, 'eval_runtime': 24.8178, 'eval_samples_per_second': 1.894, 'eval_steps_per_second': 1.894, 'epoch': 1.0}
{'loss': 1.0135, 'grad_norm': 8.780475616455078, 'learning_rate': 2.4869056997782516e-05, 'epoch': 1.11}
{'loss': 0.953, 'grad_norm': 3.885754346847534, 'learning_rate': 2.4577970808023725e-05, 'epoch': 1.3}
{'loss': 0.9045, 'grad_norm': 3.495849370956421, 'learning_rate': 2.4028578147328976e-05, 'epoch': 1.49}
{'loss': 0.937, 'grad_norm': 2.9733307361602783, 'learning_rate': 2.323258836643277e-05, 'epoch': 1.68}
{'loss': 1.0419, 'grad_norm': 3.7466888427734375, 'learning_rate': 2.220696660555709e-05, 'epoch': 1.86}
{'eval_loss': 0.9804494976997375, 'eval_runtime': 26.8785, 'eval_samples_per_second': 1.749, 'eval_steps_per_second': 1.749, 'epoch': 2.0}
{'loss': 0.8081, 'grad_norm': 3.0187132358551025, 'learning_rate': 2.1105544521680975e-05, 'epoch': 2.04}
{'loss': 0.7718, 'grad_norm': 3.207517385482788, 'learning_rate': 1.9707515682321233e-05, 'epoch': 2.23}
{'loss': 0.797, 'grad_norm': 14.94027042388916, 'learning_rate': 1.8316270872600353e-05, 'epoch': 2.41}
{'loss': 0.8091, 'grad_norm': 3.580156087875366, 'learning_rate': 1.6652901754250653e-05, 'epoch': 2.6}
{'loss': 0.6472, 'grad_norm': 2.616443395614624, 'learning_rate': 1.4900135252735772e-05, 'epoch': 2.79}
{'loss': 0.8527, 'grad_norm': 20.271167755126953, 'learning_rate': 1.3095328542969556e-05, 'epoch': 2.98}
{'eval_loss': 0.9680051803588867, 'eval_runtime': 25.8841, 'eval_samples_per_second': 1.816, 'eval_steps_per_second': 1.816, 'epoch': 3.0}
{'loss': 0.65, 'grad_norm': 3.021141529083252, 'learning_rate': 1.1276947946555104e-05, 'epoch': 3.15}
{'loss': 0.6481, 'grad_norm': 21.28277587890625, 'learning_rate': 9.483749088923114e-06, 'epoch': 3.34}
{'loss': 0.5456, 'grad_norm': 5.852257251739502, 'learning_rate': 7.753950890461423e-06, 'epoch': 3.53}
{'loss': 0.6842, 'grad_norm': 3.6339304447174072, 'learning_rate': 6.1244209965807195e-06, 'epoch': 3.71}
{'loss': 0.6125, 'grad_norm': 5.307224273681641, 'learning_rate': 4.629890007860385e-06, 'epoch': 3.9}
{'eval_loss': 0.9943222403526306, 'eval_runtime': 27.2759, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 4.0}
{'loss': 0.5939, 'grad_norm': 6.119564056396484, 'learning_rate': 3.3022112575946844e-06, 'epoch': 4.08}
{'loss': 0.5505, 'grad_norm': 2.109429359436035, 'learning_rate': 2.1696819132957823e-06, 'epoch': 4.26}
{'loss': 0.5423, 'grad_norm': 2.964115858078003, 'learning_rate': 1.2564398716963353e-06, 'epoch': 4.45}
{'loss': 0.5668, 'grad_norm': 5.909419536590576, 'learning_rate': 5.819493013876653e-07, 'epoch': 4.64}
{'loss': 0.5978, 'grad_norm': 4.04035758972168, 'learning_rate': 1.6058579785894555e-07, 'epoch': 4.83}
{'eval_loss': 1.0197422504425049, 'eval_runtime': 27.2857, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 4.92}
{'train_runtime': 1970.9218, 'train_samples_per_second': 0.54, 'train_steps_per_second': 0.134, 'train_loss': 1.0638528365009237, 'epoch': 4.92}
1970.9218 seconds used for training.
32.85 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 5...
Validation CER at epoch 5: 0.2249
  âœ… Trial 8 validation CER = 0.2249
[I 2025-11-03 02:12:04,463] Trial 8 finished with value: 0.2248627123542824 and parameters: {'num_epochs': 5, 'learning_rate': 2.491690692323215e-05, 'batch_size': 1, 'weight_decay': 0.055547868637760595}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 9 (qwen_inven)
================================================================================
  â€¢ num_epochs:    15
  â€¢ learning_rate: 9.28e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0993
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.193 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.6498, 'grad_norm': 15.2919282913208, 'learning_rate': 9.281895340700561e-06, 'epoch': 0.37}
{'loss': 2.4827, 'grad_norm': 16.680801391601562, 'learning_rate': 2.7845686022101677e-05, 'epoch': 0.75}
{'eval_loss': 1.3857131004333496, 'eval_runtime': 20.9221, 'eval_samples_per_second': 2.246, 'eval_steps_per_second': 1.147, 'epoch': 0.97}
{'loss': 1.5923, 'grad_norm': 29.630569458007812, 'learning_rate': 4.64094767035028e-05, 'epoch': 1.15}
{'loss': 1.0313, 'grad_norm': 3.5340492725372314, 'learning_rate': 6.497326738490391e-05, 'epoch': 1.52}
{'loss': 1.1019, 'grad_norm': 3.561061382293701, 'learning_rate': 8.353705806630504e-05, 'epoch': 1.9}
{'eval_loss': 1.0303456783294678, 'eval_runtime': 20.4246, 'eval_samples_per_second': 2.301, 'eval_steps_per_second': 1.175, 'epoch': 1.97}
{'loss': 0.8895, 'grad_norm': 2.7726385593414307, 'learning_rate': 9.276943332595425e-05, 'epoch': 2.3}
{'loss': 0.7828, 'grad_norm': 2.636516809463501, 'learning_rate': 9.23739065221118e-05, 'epoch': 2.67}
{'eval_loss': 1.024503231048584, 'eval_runtime': 20.8023, 'eval_samples_per_second': 2.259, 'eval_steps_per_second': 1.154, 'epoch': 2.97}
{'loss': 0.8556, 'grad_norm': 6.219890594482422, 'learning_rate': 9.158622740965846e-05, 'epoch': 3.07}
{'loss': 0.524, 'grad_norm': 8.540888786315918, 'learning_rate': 9.041311618905385e-05, 'epoch': 3.45}
{'loss': 0.563, 'grad_norm': 3.2642831802368164, 'learning_rate': 8.886458143160482e-05, 'epoch': 3.82}
{'eval_loss': 1.1031440496444702, 'eval_runtime': 21.0766, 'eval_samples_per_second': 2.23, 'eval_steps_per_second': 1.139, 'epoch': 3.97}
{'loss': 0.4485, 'grad_norm': 7.613814830780029, 'learning_rate': 8.69538346898639e-05, 'epoch': 4.22}
{'loss': 0.3284, 'grad_norm': 6.392362117767334, 'learning_rate': 8.469717778132134e-05, 'epoch': 4.6}
{'loss': 0.3498, 'grad_norm': 6.895486831665039, 'learning_rate': 8.211386370704602e-05, 'epoch': 4.97}
{'eval_loss': 1.221535563468933, 'eval_runtime': 20.5935, 'eval_samples_per_second': 2.282, 'eval_steps_per_second': 1.165, 'epoch': 4.97}
{'loss': 0.1836, 'grad_norm': 3.2863454818725586, 'learning_rate': 7.922593239186872e-05, 'epoch': 5.37}
{'loss': 0.1846, 'grad_norm': 4.1002888679504395, 'learning_rate': 7.605802264751466e-05, 'epoch': 5.75}
{'eval_loss': 1.3290475606918335, 'eval_runtime': 21.0552, 'eval_samples_per_second': 2.232, 'eval_steps_per_second': 1.14, 'epoch': 5.97}
{'train_runtime': 1794.7357, 'train_samples_per_second': 1.78, 'train_steps_per_second': 0.217, 'train_loss': 0.9658652406472427, 'epoch': 5.97}
1794.7357 seconds used for training.
29.91 minutes used for training.
Peak reserved memory = 13.193 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.575 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 15...
Validation CER at epoch 15: 0.2427
  âœ… Trial 9 validation CER = 0.2427
[I 2025-11-03 03:05:02,320] Trial 9 finished with value: 0.24265390667651499 and parameters: {'num_epochs': 15, 'learning_rate': 9.28189534070056e-05, 'batch_size': 2, 'weight_decay': 0.09933481532961277}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 10 (qwen_inven)
================================================================================
  â€¢ num_epochs:    5
  â€¢ learning_rate: 8.71e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0055
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.4934, 'grad_norm': 10.758070945739746, 'learning_rate': 1.2191853700506324e-05, 'epoch': 0.37}
{'loss': 2.4635, 'grad_norm': 22.086503982543945, 'learning_rate': 2.612540078679926e-05, 'epoch': 0.75}
{'eval_loss': 1.4046882390975952, 'eval_runtime': 21.092, 'eval_samples_per_second': 2.228, 'eval_steps_per_second': 1.138, 'epoch': 0.97}
{'loss': 1.6103, 'grad_norm': 7.741714000701904, 'learning_rate': 4.354233464466544e-05, 'epoch': 1.15}
{'loss': 1.0455, 'grad_norm': 4.238752841949463, 'learning_rate': 5.9217575116745e-05, 'epoch': 1.52}
{'loss': 1.1102, 'grad_norm': 4.495358943939209, 'learning_rate': 7.663450897461117e-05, 'epoch': 1.9}
{'eval_loss': 1.0358383655548096, 'eval_runtime': 21.3838, 'eval_samples_per_second': 2.198, 'eval_steps_per_second': 1.122, 'epoch': 1.97}
{'loss': 0.9025, 'grad_norm': 2.156381130218506, 'learning_rate': 8.654859089549322e-05, 'epoch': 2.3}
{'loss': 0.8023, 'grad_norm': 2.3136096000671387, 'learning_rate': 8.066827801245386e-05, 'epoch': 2.67}
{'eval_loss': 1.0222792625427246, 'eval_runtime': 21.2713, 'eval_samples_per_second': 2.21, 'eval_steps_per_second': 1.128, 'epoch': 2.97}
{'loss': 0.8618, 'grad_norm': 3.6080710887908936, 'learning_rate': 6.913587679918341e-05, 'epoch': 3.07}
{'loss': 0.5725, 'grad_norm': 2.4325385093688965, 'learning_rate': 5.3707090798924964e-05, 'epoch': 3.45}
{'loss': 0.5552, 'grad_norm': 2.8253839015960693, 'learning_rate': 3.673081281792448e-05, 'epoch': 3.82}
{'eval_loss': 1.0942579507827759, 'eval_runtime': 19.8324, 'eval_samples_per_second': 2.37, 'eval_steps_per_second': 1.21, 'epoch': 3.97}
{'loss': 0.4875, 'grad_norm': 2.2595884799957275, 'learning_rate': 2.0791527288446214e-05, 'epoch': 4.22}
{'loss': 0.3732, 'grad_norm': 2.5554094314575195, 'learning_rate': 8.315845942370064e-06, 'epoch': 4.6}
{'loss': 0.3475, 'grad_norm': 2.793651580810547, 'learning_rate': 1.2030781723031118e-06, 'epoch': 4.97}
{'eval_loss': 1.1874923706054688, 'eval_runtime': 20.4742, 'eval_samples_per_second': 2.296, 'eval_steps_per_second': 1.172, 'epoch': 4.97}
{'train_runtime': 1503.3815, 'train_samples_per_second': 0.708, 'train_steps_per_second': 0.086, 'train_loss': 1.1250329311077412, 'epoch': 4.97}
1503.3815 seconds used for training.
25.06 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 5...
Validation CER at epoch 5: 0.2311
  âœ… Trial 10 validation CER = 0.2311
[I 2025-11-03 03:53:31,534] Trial 10 finished with value: 0.23112495584315487 and parameters: {'num_epochs': 5, 'learning_rate': 8.708466928933087e-05, 'batch_size': 2, 'weight_decay': 0.0054946306384785265}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 11 (qwen_inven)
================================================================================
  â€¢ num_epochs:    8
  â€¢ learning_rate: 4.56e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0139
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.695, 'grad_norm': 15.731405258178711, 'learning_rate': 4.556018933032725e-06, 'epoch': 0.37}
{'loss': 2.9061, 'grad_norm': 8.84914493560791, 'learning_rate': 1.3668056799098176e-05, 'epoch': 0.75}
{'eval_loss': 1.8187119960784912, 'eval_runtime': 21.6131, 'eval_samples_per_second': 2.175, 'eval_steps_per_second': 1.11, 'epoch': 0.97}
{'loss': 2.047, 'grad_norm': 37.346351623535156, 'learning_rate': 2.2780094665163626e-05, 'epoch': 1.15}
{'loss': 1.2663, 'grad_norm': 20.28715705871582, 'learning_rate': 3.189213253122907e-05, 'epoch': 1.52}
{'loss': 1.202, 'grad_norm': 6.349315166473389, 'learning_rate': 4.100417039729453e-05, 'epoch': 1.9}
{'eval_loss': 1.0930746793746948, 'eval_runtime': 21.3764, 'eval_samples_per_second': 2.199, 'eval_steps_per_second': 1.123, 'epoch': 1.97}
{'loss': 1.019, 'grad_norm': 3.283907651901245, 'learning_rate': 4.5447704651785815e-05, 'epoch': 2.3}
{'loss': 0.9419, 'grad_norm': 5.467321395874023, 'learning_rate': 4.455448144182185e-05, 'epoch': 2.67}
{'eval_loss': 1.0248041152954102, 'eval_runtime': 21.0604, 'eval_samples_per_second': 2.232, 'eval_steps_per_second': 1.14, 'epoch': 2.97}
{'loss': 0.9911, 'grad_norm': 2.9857330322265625, 'learning_rate': 4.280323272042893e-05, 'epoch': 3.07}
{'loss': 0.7704, 'grad_norm': 4.321444988250732, 'learning_rate': 4.0262966909989956e-05, 'epoch': 3.45}
{'loss': 0.7503, 'grad_norm': 3.2362022399902344, 'learning_rate': 3.703378386146855e-05, 'epoch': 3.82}
{'eval_loss': 1.0462515354156494, 'eval_runtime': 21.6367, 'eval_samples_per_second': 2.172, 'eval_steps_per_second': 1.109, 'epoch': 3.97}
{'loss': 0.7141, 'grad_norm': 3.4583487510681152, 'learning_rate': 3.32429303931161e-05, 'epoch': 4.22}
{'loss': 0.5829, 'grad_norm': 3.664661169052124, 'learning_rate': 2.903978609570126e-05, 'epoch': 4.6}
{'loss': 0.5488, 'grad_norm': 3.2073423862457275, 'learning_rate': 2.4589976989942338e-05, 'epoch': 4.97}
{'eval_loss': 1.0962347984313965, 'eval_runtime': 21.1528, 'eval_samples_per_second': 2.222, 'eval_steps_per_second': 1.135, 'epoch': 4.97}
{'loss': 0.395, 'grad_norm': 12.601524353027344, 'learning_rate': 2.0519083730160874e-05, 'epoch': 5.37}
{'loss': 0.3923, 'grad_norm': 3.159945249557495, 'learning_rate': 1.608615759836149e-05, 'epoch': 5.75}
{'eval_loss': 1.1861900091171265, 'eval_runtime': 20.9119, 'eval_samples_per_second': 2.248, 'eval_steps_per_second': 1.148, 'epoch': 5.97}
{'train_runtime': 1821.7139, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.114, 'train_loss': 1.1822913334919856, 'epoch': 5.97}
1821.7139 seconds used for training.
30.36 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 8...
Validation CER at epoch 8: 0.2325
  âœ… Trial 11 validation CER = 0.2325
[I 2025-11-03 04:47:57,469] Trial 11 finished with value: 0.2324737467484505 and parameters: {'num_epochs': 8, 'learning_rate': 4.556018933032725e-05, 'batch_size': 2, 'weight_decay': 0.013869087922971266}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 12 (qwen_inven)
================================================================================
  â€¢ num_epochs:    4
  â€¢ learning_rate: 3.07e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0249
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.6995, 'grad_norm': 11.173851013183594, 'learning_rate': 3.6871837281124816e-06, 'epoch': 0.37}
{'loss': 3.0443, 'grad_norm': 12.152586936950684, 'learning_rate': 9.217959320281204e-06, 'epoch': 0.75}
{'eval_loss': 2.071793556213379, 'eval_runtime': 20.4537, 'eval_samples_per_second': 2.298, 'eval_steps_per_second': 1.173, 'epoch': 0.97}
{'loss': 2.3139, 'grad_norm': 8.07767391204834, 'learning_rate': 1.5363265533802007e-05, 'epoch': 1.15}
{'loss': 1.5005, 'grad_norm': 33.24623107910156, 'learning_rate': 2.1508571747322807e-05, 'epoch': 1.52}
{'loss': 1.2879, 'grad_norm': 5.902516841888428, 'learning_rate': 2.765387796084361e-05, 'epoch': 1.9}
{'eval_loss': 1.1235827207565308, 'eval_runtime': 19.9497, 'eval_samples_per_second': 2.356, 'eval_steps_per_second': 1.203, 'epoch': 1.97}
{'loss': 1.0681, 'grad_norm': 2.770746946334839, 'learning_rate': 3.008111279010235e-05, 'epoch': 2.3}
{'loss': 0.9851, 'grad_norm': 2.7066407203674316, 'learning_rate': 2.523858226325419e-05, 'epoch': 2.67}
{'eval_loss': 1.0379871129989624, 'eval_runtime': 20.9206, 'eval_samples_per_second': 2.247, 'eval_steps_per_second': 1.147, 'epoch': 2.97}
{'loss': 1.0591, 'grad_norm': 2.244086980819702, 'learning_rate': 1.7146831800100794e-05, 'epoch': 3.07}
{'loss': 0.8542, 'grad_norm': 3.5636935234069824, 'learning_rate': 8.46824455702965e-06, 'epoch': 3.45}
{'loss': 0.8708, 'grad_norm': 3.6772043704986572, 'learning_rate': 2.0582872964435737e-06, 'epoch': 3.82}
{'eval_loss': 1.030684232711792, 'eval_runtime': 20.8049, 'eval_samples_per_second': 2.259, 'eval_steps_per_second': 1.154, 'epoch': 3.97}
{'train_runtime': 1201.0761, 'train_samples_per_second': 0.709, 'train_steps_per_second': 0.087, 'train_loss': 1.6399818314955785, 'epoch': 3.97}
1201.0761 seconds used for training.
20.02 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 4...
Validation CER at epoch 4: 0.2287
  âœ… Trial 12 validation CER = 0.2287
[I 2025-11-03 05:31:44,499] Trial 12 finished with value: 0.2286521725167796 and parameters: {'num_epochs': 4, 'learning_rate': 3.072653106760401e-05, 'batch_size': 2, 'weight_decay': 0.02488713745768861}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 13 (qwen_inven)
================================================================================
  â€¢ num_epochs:    7
  â€¢ learning_rate: 1.17e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0265
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7331, 'grad_norm': 15.721529960632324, 'learning_rate': 1.6319403098888016e-06, 'epoch': 0.37}
{'loss': 3.5244, 'grad_norm': 13.298324584960938, 'learning_rate': 3.4970149497617173e-06, 'epoch': 0.75}
{'eval_loss': 2.647582530975342, 'eval_runtime': 20.5499, 'eval_samples_per_second': 2.287, 'eval_steps_per_second': 1.168, 'epoch': 0.97}
{'loss': 2.9346, 'grad_norm': 8.159581184387207, 'learning_rate': 5.595223919618748e-06, 'epoch': 1.15}
{'loss': 2.1676, 'grad_norm': 9.141560554504395, 'learning_rate': 7.926567219459893e-06, 'epoch': 1.52}
{'loss': 1.8838, 'grad_norm': 11.103058815002441, 'learning_rate': 1.0257910519301037e-05, 'epoch': 1.9}
{'eval_loss': 1.5578042268753052, 'eval_runtime': 21.1718, 'eval_samples_per_second': 2.22, 'eval_steps_per_second': 1.134, 'epoch': 1.97}
{'loss': 1.5043, 'grad_norm': 5.996906757354736, 'learning_rate': 1.1630325241779722e-05, 'epoch': 2.3}
{'loss': 1.2134, 'grad_norm': 2.717392921447754, 'learning_rate': 1.1336161567239959e-05, 'epoch': 2.67}
{'eval_loss': 1.1375654935836792, 'eval_runtime': 20.0521, 'eval_samples_per_second': 2.344, 'eval_steps_per_second': 1.197, 'epoch': 2.97}
{'loss': 1.2268, 'grad_norm': 3.274507522583008, 'learning_rate': 1.073148521768703e-05, 'epoch': 3.07}
{'loss': 1.038, 'grad_norm': 10.410576820373535, 'learning_rate': 9.850385949052418e-06, 'epoch': 3.45}
{'loss': 1.0451, 'grad_norm': 4.924017906188965, 'learning_rate': 8.742537374404295e-06, 'epoch': 3.82}
{'eval_loss': 1.0812649726867676, 'eval_runtime': 20.6929, 'eval_samples_per_second': 2.271, 'eval_steps_per_second': 1.16, 'epoch': 3.97}
{'loss': 1.0684, 'grad_norm': 13.056384086608887, 'learning_rate': 7.470396521451317e-06, 'epoch': 4.22}
{'loss': 0.9846, 'grad_norm': 4.095040798187256, 'learning_rate': 6.105682701226081e-06, 'epoch': 4.6}
{'loss': 0.9659, 'grad_norm': 3.5071256160736084, 'learning_rate': 4.72533419828726e-06, 'epoch': 4.97}
{'eval_loss': 1.0587743520736694, 'eval_runtime': 21.1943, 'eval_samples_per_second': 2.218, 'eval_steps_per_second': 1.132, 'epoch': 4.97}
{'loss': 0.9648, 'grad_norm': 2.3226826190948486, 'learning_rate': 3.407170731564438e-06, 'epoch': 5.37}
{'loss': 0.9174, 'grad_norm': 2.2842979431152344, 'learning_rate': 2.2255062226978557e-06, 'epoch': 5.75}
{'eval_loss': 1.0539664030075073, 'eval_runtime': 20.4117, 'eval_samples_per_second': 2.303, 'eval_steps_per_second': 1.176, 'epoch': 5.97}
{'loss': 0.8936, 'grad_norm': 19.606700897216797, 'learning_rate': 1.2469592102328779e-06, 'epoch': 6.15}
{'loss': 0.9814, 'grad_norm': 3.1229641437530518, 'learning_rate': 5.266971053756422e-07, 'epoch': 6.52}
{'loss': 0.9106, 'grad_norm': 1.9204856157302856, 'learning_rate': 1.0532602638997416e-07, 'epoch': 6.9}
{'eval_loss': 1.05274498462677, 'eval_runtime': 20.0366, 'eval_samples_per_second': 2.346, 'eval_steps_per_second': 1.198, 'epoch': 6.97}
{'train_runtime': 2088.6252, 'train_samples_per_second': 0.714, 'train_steps_per_second': 0.087, 'train_loss': 1.5460741919475598, 'epoch': 6.97}
2088.6252 seconds used for training.
34.81 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 7...
Validation CER at epoch 7: 0.2427
  âœ… Trial 13 validation CER = 0.2427
[I 2025-11-03 06:30:06,020] Trial 13 finished with value: 0.24265390667651499 and parameters: {'num_epochs': 7, 'learning_rate': 1.1656716499205725e-05, 'batch_size': 2, 'weight_decay': 0.026466949931423858}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 14 (qwen_inven)
================================================================================
  â€¢ num_epochs:    7
  â€¢ learning_rate: 3.21e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0315
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7225, 'grad_norm': 13.514754295349121, 'learning_rate': 3.210879008213613e-06, 'epoch': 0.37}
{'loss': 3.1329, 'grad_norm': 14.096156120300293, 'learning_rate': 8.990461222998116e-06, 'epoch': 0.75}
{'eval_loss': 2.092319965362549, 'eval_runtime': 20.4191, 'eval_samples_per_second': 2.302, 'eval_steps_per_second': 1.175, 'epoch': 0.97}
{'loss': 2.3366, 'grad_norm': 5.11915922164917, 'learning_rate': 1.541221923942534e-05, 'epoch': 1.15}
{'loss': 1.5264, 'grad_norm': 3.658787488937378, 'learning_rate': 2.1833977255852568e-05, 'epoch': 1.52}
{'loss': 1.2863, 'grad_norm': 4.274209022521973, 'learning_rate': 2.825573527227979e-05, 'epoch': 1.9}
{'eval_loss': 1.1162683963775635, 'eval_runtime': 20.8389, 'eval_samples_per_second': 2.255, 'eval_steps_per_second': 1.152, 'epoch': 1.97}
{'loss': 1.0609, 'grad_norm': 2.5165882110595703, 'learning_rate': 3.203609453834788e-05, 'epoch': 2.3}
{'loss': 0.9854, 'grad_norm': 4.70377779006958, 'learning_rate': 3.1225811498846094e-05, 'epoch': 2.67}
{'eval_loss': 1.036512851715088, 'eval_runtime': 20.4972, 'eval_samples_per_second': 2.293, 'eval_steps_per_second': 1.171, 'epoch': 2.97}
{'loss': 1.047, 'grad_norm': 2.5913281440734863, 'learning_rate': 2.956021158683397e-05, 'epoch': 3.07}
{'loss': 0.8412, 'grad_norm': 3.366469383239746, 'learning_rate': 2.7133196100951635e-05, 'epoch': 3.45}
{'loss': 0.8574, 'grad_norm': 2.6740663051605225, 'learning_rate': 2.4081592561602097e-05, 'epoch': 3.82}
{'eval_loss': 1.0280553102493286, 'eval_runtime': 20.7639, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 1.156, 'epoch': 3.97}
{'loss': 0.8393, 'grad_norm': 2.4203062057495117, 'learning_rate': 2.0577440804530538e-05, 'epoch': 4.22}
{'loss': 0.7418, 'grad_norm': 2.816608428955078, 'learning_rate': 1.6818293914513272e-05, 'epoch': 4.6}
{'loss': 0.7014, 'grad_norm': 6.944802284240723, 'learning_rate': 1.301608080209234e-05, 'epoch': 4.97}
{'eval_loss': 1.0428593158721924, 'eval_runtime': 20.5334, 'eval_samples_per_second': 2.289, 'eval_steps_per_second': 1.169, 'epoch': 4.97}
{'loss': 0.6407, 'grad_norm': 3.511960506439209, 'learning_rate': 9.385158316345355e-06, 'epoch': 5.37}
{'loss': 0.5928, 'grad_norm': 3.334172487258911, 'learning_rate': 6.130226478096317e-06, 'epoch': 5.75}
{'eval_loss': 1.0745757818222046, 'eval_runtime': 20.2822, 'eval_samples_per_second': 2.317, 'eval_steps_per_second': 1.183, 'epoch': 5.97}
{'loss': 0.5561, 'grad_norm': 2.377941131591797, 'learning_rate': 3.4347881348132546e-06, 'epoch': 6.15}
{'loss': 0.5599, 'grad_norm': 3.8333640098571777, 'learning_rate': 1.4508036456516349e-06, 'epoch': 6.52}
{'loss': 0.5421, 'grad_norm': 3.5789058208465576, 'learning_rate': 2.901238330512412e-07, 'epoch': 6.9}
{'eval_loss': 1.106449007987976, 'eval_runtime': 19.7143, 'eval_samples_per_second': 2.384, 'eval_steps_per_second': 1.217, 'epoch': 6.97}
{'train_runtime': 2084.1578, 'train_samples_per_second': 0.715, 'train_steps_per_second': 0.087, 'train_loss': 1.2129468944046524, 'epoch': 6.97}
2084.1578 seconds used for training.
34.74 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 7...
Validation CER at epoch 7: 0.2295
  âœ… Trial 14 validation CER = 0.2295
[I 2025-11-03 07:28:56,796] Trial 14 finished with value: 0.22951925238446963 and parameters: {'num_epochs': 7, 'learning_rate': 3.210879008213613e-05, 'batch_size': 2, 'weight_decay': 0.03150282882504756}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 15 (qwen_inven)
================================================================================
  â€¢ num_epochs:    4
  â€¢ learning_rate: 1.33e-04
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0564
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.5017, 'grad_norm': 12.26186466217041, 'learning_rate': 1.5942045245305702e-05, 'epoch': 0.37}
{'loss': 2.1484, 'grad_norm': 6.205793380737305, 'learning_rate': 4.251212065414854e-05, 'epoch': 0.75}
{'eval_loss': 1.1668120622634888, 'eval_runtime': 20.5596, 'eval_samples_per_second': 2.286, 'eval_steps_per_second': 1.167, 'epoch': 0.97}
{'loss': 1.3224, 'grad_norm': 4.91416597366333, 'learning_rate': 6.908219606299138e-05, 'epoch': 1.15}
{'loss': 0.964, 'grad_norm': 2.3581104278564453, 'learning_rate': 9.565227147183421e-05, 'epoch': 1.52}
{'loss': 1.0552, 'grad_norm': 2.324561357498169, 'learning_rate': 0.00012222234688067705, 'epoch': 1.9}
{'eval_loss': 1.02528977394104, 'eval_runtime': 20.0973, 'eval_samples_per_second': 2.339, 'eval_steps_per_second': 1.194, 'epoch': 1.97}
{'loss': 0.7927, 'grad_norm': 2.5948944091796875, 'learning_rate': 0.0001300598224798649, 'epoch': 2.3}
{'loss': 0.7144, 'grad_norm': 2.6661040782928467, 'learning_rate': 0.00010912247667521005, 'epoch': 2.67}
{'eval_loss': 1.0294359922409058, 'eval_runtime': 21.3154, 'eval_samples_per_second': 2.205, 'eval_steps_per_second': 1.126, 'epoch': 2.97}
{'loss': 0.7481, 'grad_norm': 2.60156512260437, 'learning_rate': 7.413668222895631e-05, 'epoch': 3.07}
{'loss': 0.4552, 'grad_norm': 2.7264654636383057, 'learning_rate': 3.661361836872428e-05, 'epoch': 3.45}
{'loss': 0.432, 'grad_norm': 2.183743953704834, 'learning_rate': 8.899287810791834e-06, 'epoch': 3.82}
{'eval_loss': 1.110245704650879, 'eval_runtime': 20.7704, 'eval_samples_per_second': 2.263, 'eval_steps_per_second': 1.155, 'epoch': 3.97}
{'train_runtime': 1196.117, 'train_samples_per_second': 0.712, 'train_steps_per_second': 0.087, 'train_loss': 1.18537161556574, 'epoch': 3.97}
1196.117 seconds used for training.
19.94 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 4...
Validation CER at epoch 4: 0.2251
  âœ… Trial 15 validation CER = 0.2251
[I 2025-11-03 08:13:01,783] Trial 15 finished with value: 0.22511962490767204 and parameters: {'num_epochs': 4, 'learning_rate': 0.00013285037704421419, 'batch_size': 2, 'weight_decay': 0.05638331237693295}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 16 (qwen_inven)
================================================================================
  â€¢ num_epochs:    6
  â€¢ learning_rate: 1.88e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0153
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7408, 'grad_norm': 14.595524787902832, 'learning_rate': 1.883074768974999e-06, 'epoch': 0.37}
{'loss': 3.3415, 'grad_norm': 8.487013816833496, 'learning_rate': 5.649224306924996e-06, 'epoch': 0.75}
{'eval_loss': 2.2994890213012695, 'eval_runtime': 19.7991, 'eval_samples_per_second': 2.374, 'eval_steps_per_second': 1.212, 'epoch': 0.97}
{'loss': 2.5716, 'grad_norm': 14.677124977111816, 'learning_rate': 9.415373844874994e-06, 'epoch': 1.15}
{'loss': 1.8241, 'grad_norm': 12.274791717529297, 'learning_rate': 1.3181523382824991e-05, 'epoch': 1.52}
{'loss': 1.5225, 'grad_norm': 6.225560665130615, 'learning_rate': 1.694767292077499e-05, 'epoch': 1.9}
{'eval_loss': 1.2330567836761475, 'eval_runtime': 20.8638, 'eval_samples_per_second': 2.253, 'eval_steps_per_second': 1.15, 'epoch': 1.97}
{'loss': 1.1834, 'grad_norm': 3.263624429702759, 'learning_rate': 1.8727556956118363e-05, 'epoch': 2.3}
{'loss': 1.066, 'grad_norm': 4.447798252105713, 'learning_rate': 1.7915552920344357e-05, 'epoch': 2.67}
{'eval_loss': 1.0778429508209229, 'eval_runtime': 20.3776, 'eval_samples_per_second': 2.306, 'eval_steps_per_second': 1.178, 'epoch': 2.97}
{'loss': 1.1327, 'grad_norm': 5.156844615936279, 'learning_rate': 1.6362350002242826e-05, 'epoch': 3.07}
{'loss': 0.9435, 'grad_norm': 6.1024556159973145, 'learning_rate': 1.4203384438709888e-05, 'epoch': 3.45}
{'loss': 0.9587, 'grad_norm': 6.714383602142334, 'learning_rate': 1.162691377684738e-05, 'epoch': 3.82}
{'eval_loss': 1.0443869829177856, 'eval_runtime': 20.3703, 'eval_samples_per_second': 2.307, 'eval_steps_per_second': 1.178, 'epoch': 3.97}
{'loss': 0.9709, 'grad_norm': 3.790912628173828, 'learning_rate': 8.857601186450545e-06, 'epoch': 4.22}
{'loss': 0.8922, 'grad_norm': 4.549700736999512, 'learning_rate': 6.13692527386326e-06, 'epoch': 4.6}
{'loss': 0.8619, 'grad_norm': 2.550403118133545, 'learning_rate': 3.7021236228682804e-06, 'epoch': 4.97}
{'eval_loss': 1.0379319190979004, 'eval_runtime': 20.7564, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 1.156, 'epoch': 4.97}
{'loss': 0.8605, 'grad_norm': 3.122318744659424, 'learning_rate': 1.7655061443961623e-06, 'epoch': 5.37}
{'loss': 0.8265, 'grad_norm': 2.5019757747650146, 'learning_rate': 4.959420702354469e-07, 'epoch': 5.75}
{'eval_loss': 1.037911057472229, 'eval_runtime': 20.4086, 'eval_samples_per_second': 2.303, 'eval_steps_per_second': 1.176, 'epoch': 5.97}
{'train_runtime': 1827.0137, 'train_samples_per_second': 0.7, 'train_steps_per_second': 0.085, 'train_loss': 1.4867679675420125, 'epoch': 5.97}
1827.0137 seconds used for training.
30.45 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 6...
Validation CER at epoch 6: 0.2234
  âœ… Trial 16 validation CER = 0.2234
[I 2025-11-03 09:07:23,675] Trial 16 finished with value: 0.22335335110311827 and parameters: {'num_epochs': 6, 'learning_rate': 1.883074768974999e-05, 'batch_size': 2, 'weight_decay': 0.015280810153384027}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 17 (qwen_inven)
================================================================================
  â€¢ num_epochs:    9
  â€¢ learning_rate: 1.47e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0049
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7384, 'grad_norm': 12.85700511932373, 'learning_rate': 1.7648661788042207e-06, 'epoch': 0.37}
{'loss': 3.4934, 'grad_norm': 11.3153076171875, 'learning_rate': 4.118021083876515e-06, 'epoch': 0.75}
{'eval_loss': 2.562130928039551, 'eval_runtime': 19.8485, 'eval_samples_per_second': 2.368, 'eval_steps_per_second': 1.209, 'epoch': 0.97}
{'loss': 2.8723, 'grad_norm': 12.324624061584473, 'learning_rate': 6.765320352082847e-06, 'epoch': 1.15}
{'loss': 2.0857, 'grad_norm': 6.208686828613281, 'learning_rate': 9.706763983423214e-06, 'epoch': 1.52}
{'loss': 1.748, 'grad_norm': 7.505609512329102, 'learning_rate': 1.2648207614763582e-05, 'epoch': 1.9}
{'eval_loss': 1.4148204326629639, 'eval_runtime': 20.351, 'eval_samples_per_second': 2.309, 'eval_steps_per_second': 1.179, 'epoch': 1.97}
{'loss': 1.3432, 'grad_norm': 12.745250701904297, 'learning_rate': 1.4697573601644166e-05, 'epoch': 2.3}
{'loss': 1.1352, 'grad_norm': 4.552121162414551, 'learning_rate': 1.4526817711832333e-05, 'epoch': 2.67}
{'eval_loss': 1.1061618328094482, 'eval_runtime': 21.4457, 'eval_samples_per_second': 2.192, 'eval_steps_per_second': 1.119, 'epoch': 2.97}
{'loss': 1.1842, 'grad_norm': 2.4000916481018066, 'learning_rate': 1.4147457995928522e-05, 'epoch': 3.07}
{'loss': 0.9926, 'grad_norm': 5.437215805053711, 'learning_rate': 1.3570526600388557e-05, 'epoch': 3.45}
{'loss': 1.0095, 'grad_norm': 5.789793491363525, 'learning_rate': 1.2812801246682918e-05, 'epoch': 3.82}
{'eval_loss': 1.054612636566162, 'eval_runtime': 20.9595, 'eval_samples_per_second': 2.242, 'eval_steps_per_second': 1.145, 'epoch': 3.97}
{'loss': 1.0212, 'grad_norm': 5.432221412658691, 'learning_rate': 1.1896317318973182e-05, 'epoch': 4.22}
{'loss': 0.9262, 'grad_norm': 5.440486431121826, 'learning_rate': 1.084772705388602e-05, 'epoch': 4.6}
{'loss': 0.9155, 'grad_norm': 9.362496376037598, 'learning_rate': 9.697524467764276e-06, 'epoch': 4.97}
{'eval_loss': 1.0351693630218506, 'eval_runtime': 20.7202, 'eval_samples_per_second': 2.268, 'eval_steps_per_second': 1.158, 'epoch': 4.97}
{'loss': 0.8925, 'grad_norm': 2.55967116355896, 'learning_rate': 8.479158561258224e-06, 'epoch': 5.37}
{'loss': 0.8398, 'grad_norm': 3.409416437149048, 'learning_rate': 7.228060590122905e-06, 'epoch': 5.75}
{'eval_loss': 1.034218430519104, 'eval_runtime': 20.855, 'eval_samples_per_second': 2.254, 'eval_steps_per_second': 1.151, 'epoch': 5.97}
{'loss': 0.7923, 'grad_norm': 16.628578186035156, 'learning_rate': 5.980613690125012e-06, 'epoch': 6.15}
{'loss': 0.8576, 'grad_norm': 19.14712905883789, 'learning_rate': 4.773094820360859e-06, 'epoch': 6.52}
{'loss': 0.7894, 'grad_norm': 3.3951704502105713, 'learning_rate': 3.640619794294355e-06, 'epoch': 6.9}
{'eval_loss': 1.0430817604064941, 'eval_runtime': 20.8906, 'eval_samples_per_second': 2.25, 'eval_steps_per_second': 1.149, 'epoch': 6.97}
{'loss': 0.7635, 'grad_norm': 3.494680881500244, 'learning_rate': 2.6161220780299757e-06, 'epoch': 7.3}
{'loss': 0.7202, 'grad_norm': 2.387789249420166, 'learning_rate': 1.7293950533519497e-06, 'epoch': 7.67}
{'eval_loss': 1.0518461465835571, 'eval_runtime': 20.8442, 'eval_samples_per_second': 2.255, 'eval_steps_per_second': 1.151, 'epoch': 7.97}
{'train_runtime': 2403.7641, 'train_samples_per_second': 0.797, 'train_steps_per_second': 0.097, 'train_loss': 1.3820750942597022, 'epoch': 7.97}
2403.7641 seconds used for training.
40.06 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 9...
Validation CER at epoch 9: 0.2241
  âœ… Trial 17 validation CER = 0.2241
[I 2025-11-03 10:11:40,717] Trial 17 finished with value: 0.22412408876328718 and parameters: {'num_epochs': 9, 'learning_rate': 1.470721815670184e-05, 'batch_size': 2, 'weight_decay': 0.004898011248940667}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 18 (qwen_inven)
================================================================================
  â€¢ num_epochs:    6
  â€¢ learning_rate: 4.84e-04
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0178
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.1598, 'grad_norm': 7.103558540344238, 'learning_rate': 5.812851701735019e-05, 'epoch': 0.37}
{'loss': 1.4926, 'grad_norm': 4.049184799194336, 'learning_rate': 0.0001453212925433755, 'epoch': 0.75}
{'eval_loss': 1.6904808282852173, 'eval_runtime': 19.7215, 'eval_samples_per_second': 2.383, 'eval_steps_per_second': 1.217, 'epoch': 0.97}
{'loss': 1.2989, 'grad_norm': 2.2309069633483887, 'learning_rate': 0.00023251406806940077, 'epoch': 1.15}
{'loss': 0.8885, 'grad_norm': 3.738182783126831, 'learning_rate': 0.00032939492976498446, 'epoch': 1.52}
{'loss': 1.0268, 'grad_norm': 4.438767433166504, 'learning_rate': 0.0004262757914605681, 'epoch': 1.9}
{'eval_loss': 1.08090078830719, 'eval_runtime': 21.0852, 'eval_samples_per_second': 2.229, 'eval_steps_per_second': 1.138, 'epoch': 1.97}
{'loss': 0.7485, 'grad_norm': 3.157989025115967, 'learning_rate': 0.00048270431481199573, 'epoch': 2.3}
{'loss': 0.6969, 'grad_norm': 3.7756359577178955, 'learning_rate': 0.00046385239417756017, 'epoch': 2.67}
{'eval_loss': 1.1580544710159302, 'eval_runtime': 20.5194, 'eval_samples_per_second': 2.291, 'eval_steps_per_second': 1.17, 'epoch': 2.97}
{'loss': 0.7088, 'grad_norm': 2.564105987548828, 'learning_rate': 0.0004256730089507334, 'epoch': 3.07}
{'loss': 0.466, 'grad_norm': 5.352671146392822, 'learning_rate': 0.00037149532636514965, 'epoch': 3.45}
{'loss': 0.4597, 'grad_norm': 8.510367393493652, 'learning_rate': 0.0003060435336906296, 'epoch': 3.82}
{'eval_loss': 1.2273154258728027, 'eval_runtime': 20.5239, 'eval_samples_per_second': 2.29, 'eval_steps_per_second': 1.169, 'epoch': 3.97}
{'loss': 0.3738, 'grad_norm': 3.1185736656188965, 'learning_rate': 0.00023502489840306588, 'epoch': 4.22}
{'loss': 0.2612, 'grad_norm': 5.577619552612305, 'learning_rate': 0.00016463210569558329, 'epoch': 4.6}
{'loss': 0.2037, 'grad_norm': 3.280202627182007, 'learning_rate': 0.00010100326852032777, 'epoch': 4.97}
{'eval_loss': 1.2872624397277832, 'eval_runtime': 21.0194, 'eval_samples_per_second': 2.236, 'eval_steps_per_second': 1.142, 'epoch': 4.97}
{'train_runtime': 1483.9987, 'train_samples_per_second': 0.861, 'train_steps_per_second': 0.105, 'train_loss': 0.9069203388232451, 'epoch': 4.97}
1483.9987 seconds used for training.
24.73 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 6...
Validation CER at epoch 6: 0.2728
  âœ… Trial 18 validation CER = 0.2728
[I 2025-11-03 11:00:49,259] Trial 18 finished with value: 0.272809017630624 and parameters: {'num_epochs': 6, 'learning_rate': 0.0004844043084779183, 'batch_size': 2, 'weight_decay': 0.017754476008704948}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 19 (qwen_inven)
================================================================================
  â€¢ num_epochs:    10
  â€¢ learning_rate: 4.49e-05
  â€¢ batch_size:    1
  â€¢ weight_decay:  0.0002
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.716, 'grad_norm': 13.59289836883545, 'learning_rate': 4.488708965560159e-06, 'epoch': 0.19}
{'loss': 2.9683, 'grad_norm': 16.058351516723633, 'learning_rate': 1.3466126896680475e-05, 'epoch': 0.38}
{'loss': 2.0317, 'grad_norm': 10.207978248596191, 'learning_rate': 2.2443544827800793e-05, 'epoch': 0.56}
{'loss': 1.4631, 'grad_norm': 20.58579444885254, 'learning_rate': 3.142096275892111e-05, 'epoch': 0.75}
{'loss': 1.1768, 'grad_norm': 5.514745712280273, 'learning_rate': 4.039838069004143e-05, 'epoch': 0.94}
{'eval_loss': 1.0524895191192627, 'eval_runtime': 25.9164, 'eval_samples_per_second': 1.814, 'eval_steps_per_second': 1.814, 'epoch': 1.0}
{'loss': 0.9347, 'grad_norm': 6.113780975341797, 'learning_rate': 4.4875073100626036e-05, 'epoch': 1.11}
{'loss': 0.8965, 'grad_norm': 4.024555683135986, 'learning_rate': 4.4779017852810924e-05, 'epoch': 1.3}
{'loss': 0.8506, 'grad_norm': 3.963223934173584, 'learning_rate': 4.458731868049852e-05, 'epoch': 1.49}
{'loss': 0.8766, 'grad_norm': 2.8152472972869873, 'learning_rate': 4.430079646897485e-05, 'epoch': 1.68}
{'loss': 0.9847, 'grad_norm': 4.252323627471924, 'learning_rate': 4.3920678150337365e-05, 'epoch': 1.86}
{'eval_loss': 0.9663810133934021, 'eval_runtime': 26.0104, 'eval_samples_per_second': 1.807, 'eval_steps_per_second': 1.807, 'epoch': 2.0}
{'loss': 0.7634, 'grad_norm': 11.76417064666748, 'learning_rate': 4.344859144958328e-05, 'epoch': 2.04}
{'loss': 0.6562, 'grad_norm': 29.90469741821289, 'learning_rate': 4.288655791444129e-05, 'epoch': 2.23}
{'loss': 0.6607, 'grad_norm': 3.1310958862304688, 'learning_rate': 4.223698425879385e-05, 'epoch': 2.41}
{'loss': 0.6711, 'grad_norm': 4.442770004272461, 'learning_rate': 4.150265205675879e-05, 'epoch': 2.6}
{'loss': 0.5114, 'grad_norm': 2.5603132247924805, 'learning_rate': 4.068670583156196e-05, 'epoch': 2.79}
{'loss': 0.7467, 'grad_norm': 5.171590805053711, 'learning_rate': 3.9792639590205994e-05, 'epoch': 2.98}
{'eval_loss': 0.981092631816864, 'eval_runtime': 27.2501, 'eval_samples_per_second': 1.725, 'eval_steps_per_second': 1.725, 'epoch': 3.0}
{'loss': 0.4834, 'grad_norm': 3.300809383392334, 'learning_rate': 3.8824281861595925e-05, 'epoch': 3.15}
{'loss': 0.4036, 'grad_norm': 7.0326738357543945, 'learning_rate': 3.778577930219064e-05, 'epoch': 3.34}
{'loss': 0.3711, 'grad_norm': 2.7076406478881836, 'learning_rate': 3.679482273339624e-05, 'epoch': 3.53}
{'loss': 0.4569, 'grad_norm': 4.201755046844482, 'learning_rate': 3.563552948674711e-05, 'epoch': 3.71}
{'loss': 0.3926, 'grad_norm': 15.958465576171875, 'learning_rate': 3.441974613651804e-05, 'epoch': 3.9}
{'eval_loss': 1.0823639631271362, 'eval_runtime': 27.0508, 'eval_samples_per_second': 1.737, 'eval_steps_per_second': 1.737, 'epoch': 4.0}
{'loss': 0.363, 'grad_norm': 3.516648054122925, 'learning_rate': 3.3152678853665166e-05, 'epoch': 4.08}
{'loss': 0.2292, 'grad_norm': 6.095745086669922, 'learning_rate': 3.1839753414817375e-05, 'epoch': 4.26}
{'loss': 0.252, 'grad_norm': 7.263270854949951, 'learning_rate': 3.0486591968267786e-05, 'epoch': 4.45}
{'loss': 0.2576, 'grad_norm': 4.685101509094238, 'learning_rate': 2.9098988959071666e-05, 'epoch': 4.64}
{'loss': 0.238, 'grad_norm': 4.224149703979492, 'learning_rate': 2.7682886316343076e-05, 'epoch': 4.83}
{'loss': 0.1963, 'grad_norm': 7.755485534667969, 'learning_rate': 2.6244348009002195e-05, 'epoch': 5.0}
{'eval_loss': 1.2263306379318237, 'eval_runtime': 27.025, 'eval_samples_per_second': 1.739, 'eval_steps_per_second': 1.739, 'epoch': 5.0}
{'train_runtime': 2034.759, 'train_samples_per_second': 1.047, 'train_steps_per_second': 0.26, 'train_loss': 0.8723056930082815, 'epoch': 5.0}
2034.759 seconds used for training.
33.91 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 10...
Validation CER at epoch 10: 0.2366
  âœ… Trial 19 validation CER = 0.2366
[I 2025-11-03 11:58:41,478] Trial 19 finished with value: 0.23658434760268474 and parameters: {'num_epochs': 10, 'learning_rate': 4.4887089655601586e-05, 'batch_size': 1, 'weight_decay': 0.00021053776457237439}. Best is trial 6 with value: 0.2223578149587334.

================================================================================
OPTUNA TRIAL 20 (qwen_inven)
================================================================================
  â€¢ num_epochs:    7
  â€¢ learning_rate: 1.81e-05
  â€¢ batch_size:    2
  â€¢ weight_decay:  0.0157
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
Preparing training and validation datasets...
Training: 213 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.7324, 'grad_norm': 14.540129661560059, 'learning_rate': 2.167228614536025e-06, 'epoch': 0.37}
{'loss': 3.4127, 'grad_norm': 10.956808090209961, 'learning_rate': 5.056866767250725e-06, 'epoch': 0.75}
{'eval_loss': 2.3765053749084473, 'eval_runtime': 20.7628, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 1.156, 'epoch': 0.97}
{'loss': 2.6735, 'grad_norm': 6.512207984924316, 'learning_rate': 8.6689144581441e-06, 'epoch': 1.15}
{'loss': 1.9131, 'grad_norm': 5.069849967956543, 'learning_rate': 1.2280962149037476e-05, 'epoch': 1.52}
{'loss': 1.6131, 'grad_norm': 11.042884826660156, 'learning_rate': 1.589300983993085e-05, 'epoch': 1.9}
{'eval_loss': 1.3011285066604614, 'eval_runtime': 20.7199, 'eval_samples_per_second': 2.268, 'eval_steps_per_second': 1.158, 'epoch': 1.97}
{'loss': 1.2373, 'grad_norm': 3.3203768730163574, 'learning_rate': 1.8019349375431687e-05, 'epoch': 2.3}
{'loss': 1.0818, 'grad_norm': 5.103861331939697, 'learning_rate': 1.7563589290060106e-05, 'epoch': 2.67}
{'eval_loss': 1.085557460784912, 'eval_runtime': 20.4759, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 1.172, 'epoch': 2.97}
{'loss': 1.1462, 'grad_norm': 8.143409729003906, 'learning_rate': 1.6626738929030343e-05, 'epoch': 3.07}
{'loss': 0.9525, 'grad_norm': 6.861619472503662, 'learning_rate': 1.5261614977128302e-05, 'epoch': 3.45}
{'loss': 0.9741, 'grad_norm': 2.285414457321167, 'learning_rate': 1.3545178840850156e-05, 'epoch': 3.82}
{'eval_loss': 1.0470882654190063, 'eval_runtime': 20.3767, 'eval_samples_per_second': 2.307, 'eval_steps_per_second': 1.178, 'epoch': 3.97}
{'loss': 0.9719, 'grad_norm': 2.243227243423462, 'learning_rate': 1.157419780570487e-05, 'epoch': 4.22}
{'loss': 0.8893, 'grad_norm': 2.544358968734741, 'learning_rate': 9.459789600182017e-06, 'epoch': 4.6}
{'loss': 0.8718, 'grad_norm': 2.8887617588043213, 'learning_rate': 7.321157926756647e-06, 'epoch': 4.97}
{'eval_loss': 1.0313050746917725, 'eval_runtime': 19.6652, 'eval_samples_per_second': 2.39, 'eval_steps_per_second': 1.22, 'epoch': 4.97}
{'loss': 0.838, 'grad_norm': 2.5390536785125732, 'learning_rate': 5.278872131043676e-06, 'epoch': 5.37}
{'loss': 0.8028, 'grad_norm': 2.7664756774902344, 'learning_rate': 3.4480698802756204e-06, 'epoch': 5.75}
{'eval_loss': 1.0365439653396606, 'eval_runtime': 20.0968, 'eval_samples_per_second': 2.339, 'eval_steps_per_second': 1.194, 'epoch': 5.97}
{'loss': 0.7808, 'grad_norm': 6.954805850982666, 'learning_rate': 1.931966062770247e-06, 'epoch': 6.15}
{'loss': 0.833, 'grad_norm': 6.643467426300049, 'learning_rate': 8.160338562758836e-07, 'epoch': 6.52}
{'loss': 0.7881, 'grad_norm': 19.254043579101562, 'learning_rate': 1.6318601830918842e-07, 'epoch': 6.9}
{'eval_loss': 1.0375796556472778, 'eval_runtime': 19.5011, 'eval_samples_per_second': 2.41, 'eval_steps_per_second': 1.231, 'epoch': 6.97}
{'train_runtime': 2064.8423, 'train_samples_per_second': 0.722, 'train_steps_per_second': 0.088, 'train_loss': 1.4106145874484555, 'epoch': 6.97}
2064.8423 seconds used for training.
34.41 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Evaluating on validation set at epoch 7...
Validation CER at epoch 7: 0.2404
  âœ… Trial 20 validation CER = 0.2404
[I 2025-11-03 12:56:14,929] Trial 20 finished with value: 0.2403738077651819 and parameters: {'num_epochs': 7, 'learning_rate': 1.8060238454466876e-05, 'batch_size': 2, 'weight_decay': 0.015737558572856726}. Best is trial 6 with value: 0.2223578149587334.

============================================================
OPTUNA DONE
============================================================
Best trial #: 6
Best validation CER: 0.2224
  num_epochs: 4
  learning_rate: 3.794745054836966e-05
  batch_size: 2
  weight_decay: 0.026431049671015117
Best params saved to /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/best_hyperparameters.json

================================================================================
TRAINING FINAL QWEN MODEL WITH BEST HYPERPARAMETERS
================================================================================
  num_epochs: 4
  learning_rate: 3.794745054836966e-05
  batch_size: 2
  weight_decay: 0.026431049671015117
================================================================================
Loading Qwen2.5-VL model with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
ðŸ“„ Combined dataset saved to /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/combined_train_val.jsonl (260 samples)
Preparing training and validation datasets...
Training: 260 samples, Validation: 47 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
13.195 GB of memory reserved.
Starting training with early stopping...
{'loss': 3.6743, 'grad_norm': 15.045154571533203, 'learning_rate': 4.553694065804359e-06, 'epoch': 0.31}
{'loss': 2.7728, 'grad_norm': 11.560620307922363, 'learning_rate': 1.1384235164510898e-05, 'epoch': 0.62}
{'loss': 2.0962, 'grad_norm': 9.4117431640625, 'learning_rate': 1.897372527418483e-05, 'epoch': 0.92}
{'eval_loss': 1.4860597848892212, 'eval_runtime': 19.5363, 'eval_samples_per_second': 2.406, 'eval_steps_per_second': 1.228, 'epoch': 1.0}
{'loss': 1.3715, 'grad_norm': 5.864502429962158, 'learning_rate': 2.6563215383858758e-05, 'epoch': 1.22}
{'loss': 1.1308, 'grad_norm': 6.908075332641602, 'learning_rate': 3.33937564825653e-05, 'epoch': 1.52}
{'loss': 1.1204, 'grad_norm': 3.0695438385009766, 'learning_rate': 3.7701745789288565e-05, 'epoch': 1.83}
{'eval_loss': 0.930000364780426, 'eval_runtime': 20.1096, 'eval_samples_per_second': 2.337, 'eval_steps_per_second': 1.193, 'epoch': 2.0}
{'loss': 0.9085, 'grad_norm': 6.739477157592773, 'learning_rate': 3.5010129761757565e-05, 'epoch': 2.12}
{'loss': 0.9657, 'grad_norm': 14.223703384399414, 'learning_rate': 2.975202971661116e-05, 'epoch': 2.43}
{'loss': 0.9024, 'grad_norm': 4.77064847946167, 'learning_rate': 2.3514434942185995e-05, 'epoch': 2.74}
{'eval_loss': 0.7922483682632446, 'eval_runtime': 20.8025, 'eval_samples_per_second': 2.259, 'eval_steps_per_second': 1.154, 'epoch': 3.0}
{'loss': 0.8012, 'grad_norm': 1.9842606782913208, 'learning_rate': 1.593012570030958e-05, 'epoch': 3.03}
{'loss': 0.7725, 'grad_norm': 2.241222858428955, 'learning_rate': 8.832917520795695e-06, 'epoch': 3.34}
{'loss': 0.7979, 'grad_norm': 2.4209415912628174, 'learning_rate': 3.3586554976320233e-06, 'epoch': 3.65}
{'eval_loss': 0.7451070547103882, 'eval_runtime': 19.6954, 'eval_samples_per_second': 2.386, 'eval_steps_per_second': 1.219, 'epoch': 3.89}
{'train_runtime': 1438.9382, 'train_samples_per_second': 0.723, 'train_steps_per_second': 0.089, 'train_loss': 1.403551023453474, 'epoch': 3.89}
1438.9382 seconds used for training.
23.98 minutes used for training.
Peak reserved memory = 13.195 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 41.581 %.
Peak reserved memory for training % of max memory = 0.0 %.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/final_model...
Model saved successfully!

============================================================
PHASE 3: TEST EVALUATION
============================================================
Starting evaluation on test.jsonl...
Loaded 48 test samples
Processing test image 1/48: inventarbuch-022.jpg
  Processed successfully. CER: 0.256
Processing test image 2/48: inventarbuch-099.jpg
  Processed successfully. CER: 0.171
Processing test image 3/48: inventarbuch-245.jpg
  Processed successfully. CER: 0.292
Processing test image 4/48: inventarbuch-263.jpg
  Processed successfully. CER: 0.137
Processing test image 5/48: inventarbuch-033.jpg
  Processed successfully. CER: 0.281
Processing test image 6/48: inventarbuch-143.jpg
  Processed successfully. CER: 0.165
Processing test image 7/48: inventarbuch-244.jpg
  Processed successfully. CER: 0.386
Processing test image 8/48: inventarbuch-024.jpg
  Processed successfully. CER: 0.058
Processing test image 9/48: inventarbuch-141.jpg
  Processed successfully. CER: 0.097
Processing test image 10/48: inventarbuch-183.jpg
  Processed successfully. CER: 0.452
Processing test image 11/48: inventarbuch-191.jpg
  Processed successfully. CER: 0.227
Processing test image 12/48: inventarbuch-051.jpg
  Processed successfully. CER: 0.055
Processing test image 13/48: inventarbuch-203.jpg
  Processed successfully. CER: 0.181
Processing test image 14/48: inventarbuch-296.jpg
  Processed successfully. CER: 0.245
Processing test image 15/48: inventarbuch-302.jpg
  Processed successfully. CER: 0.024
Processing test image 16/48: inventarbuch-179.jpg
  Processed successfully. CER: 0.236
Processing test image 17/48: inventarbuch-114.jpg
  Processed successfully. CER: 0.233
Processing test image 18/48: inventarbuch-082.jpg
  Processed successfully. CER: 0.291
Processing test image 19/48: inventarbuch-287.jpg
  Processed successfully. CER: 0.324
Processing test image 20/48: inventarbuch-181.jpg
  Processed successfully. CER: 0.231
Processing test image 21/48: inventarbuch-299.jpg
  Processed successfully. CER: 0.225
Processing test image 22/48: inventarbuch-084.jpg
  Processed successfully. CER: 0.343
Processing test image 23/48: inventarbuch-004.jpg
  Processed successfully. CER: 0.167
Processing test image 24/48: inventarbuch-148.jpg
  Processed successfully. CER: 0.085
Processing test image 25/48: inventarbuch-238.jpg
  Processed successfully. CER: 0.261
Processing test image 26/48: inventarbuch-116.jpg
  Processed successfully. CER: 0.137
Processing test image 27/48: inventarbuch-223.jpg
  Processed successfully. CER: 0.182
Processing test image 28/48: inventarbuch-104.jpg
  Processed successfully. CER: 0.248
Processing test image 29/48: inventarbuch-015.jpg
  Processed successfully. CER: 0.241
Processing test image 30/48: inventarbuch-272.jpg
  Processed successfully. CER: 0.212
Processing test image 31/48: inventarbuch-124.jpg
  Processed successfully. CER: 0.164
Processing test image 32/48: inventarbuch-115.jpg
  Processed successfully. CER: 1.000
Processing test image 33/48: inventarbuch-049.jpg
  Processed successfully. CER: 0.033
Processing test image 34/48: inventarbuch-017.jpg
  Processed successfully. CER: 0.047
Processing test image 35/48: inventarbuch-018.jpg
  Processed successfully. CER: 0.208
Processing test image 36/48: inventarbuch-225.jpg
  Processed successfully. CER: 0.277
Processing test image 37/48: inventarbuch-046.jpg
  Processed successfully. CER: 0.204
Processing test image 38/48: inventarbuch-294.jpg
  Processed successfully. CER: 0.149
Processing test image 39/48: inventarbuch-054.jpg
  Processed successfully. CER: 0.328
Processing test image 40/48: inventarbuch-073.jpg
  Processed successfully. CER: 0.287
Processing test image 41/48: inventarbuch-118.jpg
  Processed successfully. CER: 0.199
Processing test image 42/48: inventarbuch-130.jpg
  Processed successfully. CER: 0.166
Processing test image 43/48: inventarbuch-146.jpg
  Processed successfully. CER: 0.232
Processing test image 44/48: inventarbuch-014.jpg
  Processed successfully. CER: 0.218
Processing test image 45/48: inventarbuch-059.jpg
  Processed successfully. CER: 0.213
Processing test image 46/48: inventarbuch-256.jpg
  Processed successfully. CER: 0.139
Processing test image 47/48: inventarbuch-260.jpg
  Processed successfully. CER: 0.156
Processing test image 48/48: inventarbuch-279.jpg
  Processed successfully. CER: 0.269
CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/cer_evaluation_results.txt

âœ… Final summary saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/inve/final_summary.txt

ðŸŽ‰ Qwen HPO pipeline finished.

=== JOB_STATISTICS ===
=== current date     : Mon Nov  3 01:44:46 PM CET 2025
= Job-ID             : 1295013 on tinygpu
= Job-Name           : inven_hpo
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 18:47:59
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg074
= Subm/Elig/Start/End: 2025-11-02T18:52:37 / 2025-11-02T18:52:37 / 2025-11-02T18:57:10 / 2025-11-03T13:45:09
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc             102.3G   104.9G   209.7G        N/A  30,542      500K   1,000K        N/A    
    /home/woody           184.1G  1000.0G  1500.0G        N/A     841K   5,000K   7,500K        N/A    
    /home/vault           860.8G  1048.6G  2097.2G        N/A   4,944      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 75320, 39 %, 17 %, 13924 MiB, 67666846 ms
