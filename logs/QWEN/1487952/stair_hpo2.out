### Starting TaskPrologue of job 1487952 on tg073 at Tue Jan 13 06:43:13 PM CET 2026
Running on cores 2-3,10-11,18-19,26-27 with governor ondemand
Tue Jan 13 18:43:13 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             25W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!

================================================================================
[HPO] Starting / Resuming Optuna HPO for Qwen Staircase OCR (ENHANCED MODE)
================================================================================
[HPO]   Storage: sqlite:////home/hpc/iwi5/iwi5298h/Uddipan-Thesis/vlmmodels.db
[HPO]   Study name: qwen_stair_enhanced_v2
[HPO]   Target total trials (COMPLETE): 30
[HPO]   Completed trials so far: 0
[HPO]   Remaining trials to run: 30

================================================================================
[HPO] Qwen Staircase OCR HPO TRIAL (ENHANCED MODE)
================================================================================
[HPO] Trial output directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/stair_enhanced/trials/trial_0
[HPO]   â€¢ num_epochs: 5
[HPO]   â€¢ batch_size: 1
[HPO]   â€¢ grad_accum: 4
[HPO]   â€¢ learning_rate: 1.9906996673933362e-05
[HPO]   â€¢ weight_decay: 0.1426071459614874
[HPO]   â€¢ lora_r: 32
[HPO]   â€¢ lora_alpha: 64
[HPO]   â€¢ lora_dropout: 0.04246782213565523
[HPO]   â€¢ warmup_ratio: 0.03636499344142013
[HPO]   â€¢ max_seq_length: 2048
[HPO] Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
[HPO]   LoRA config: r=32, alpha=64, dropout=0.04246782213565523
[HPO]   max_seq_length=2048
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
[HPO] Created temporary dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_cqp_nh8m
[HPO] Model loaded with LoRA for HPO.
[HPO] Preparing training data (no augmentation for HPO)
[HPO] Total training samples: 115 (original: 115, augmented: 0)
[HPO] Training samples: 115, Validation raw records: 25
Unsloth: Model does not have a default image size - using 512
[HPO] Starting training for this trial (NO eval during training)...

Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.8822, 'grad_norm': 1.8129568099975586, 'learning_rate': 1.9515676753861283e-05, 'epoch': 0.7}
{'loss': 0.9426, 'grad_norm': 1.7602370977401733, 'learning_rate': 1.7234794265952624e-05, 'epoch': 1.38}
{'loss': 0.3958, 'grad_norm': 2.0415093898773193, 'learning_rate': 1.3382148558411939e-05, 'epoch': 2.07}
{'loss': 0.1945, 'grad_norm': 0.5876195430755615, 'learning_rate': 8.789383734218155e-06, 'epoch': 2.77}
{'loss': 0.1461, 'grad_norm': 0.5496031641960144, 'learning_rate': 4.447908314257048e-06, 'epoch': 3.45}
{'loss': 0.1238, 'grad_norm': 0.6272866129875183, 'learning_rate': 1.2948867058790695e-06, 'epoch': 4.14}
{'loss': 0.1209, 'grad_norm': 0.5336926579475403, 'learning_rate': 1.0939974769117769e-08, 'epoch': 4.83}
{'train_runtime': 1716.5203, 'train_samples_per_second': 0.335, 'train_steps_per_second': 0.082, 'train_loss': 0.5436811293874468, 'epoch': 4.83}
[HPO] Training finished for this trial.

================================================================================
[HPO] RUNNING CER EVALUATION ON VALIDATION SET (HPO OBJECTIVE)
================================================================================
[HPO]   Using 10 validation samples for CER
[HPO]   Evaluating 1/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (110).jpg[HPO]   Evaluating 2/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (1).jpg[HPO]   Evaluating 3/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (93).jpg[HPO]   Evaluating 4/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (86).jpg[HPO]   Evaluating 5/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (80).jpg[HPO]   Evaluating 6/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (119).jpg[HPO]   Evaluating 7/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (147).jpg[HPO]   Evaluating 8/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (72).jpg[HPO]   Evaluating 9/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (118).jpg[HPO]   Evaluating 10/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (90).jpg
[HPO]   âœ… Validation CER (HPO objective): 0.5684 (56.84%)
================================================================================

[HPO]   Best Validation CER in this trial: 0.5684 (56.84%)
[HPO] Cleaned temp dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_cqp_nh8m

================================================================================
[HPO] Qwen Staircase OCR HPO TRIAL (ENHANCED MODE)
================================================================================
[HPO] Trial output directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/stair_enhanced/trials/trial_1
[HPO]   â€¢ num_epochs: 5
[HPO]   â€¢ batch_size: 1
[HPO]   â€¢ grad_accum: 4
[HPO]   â€¢ learning_rate: 2.4602080610141604e-05
[HPO]   â€¢ weight_decay: 0.043684371029706286
[HPO]   â€¢ lora_r: 32
[HPO]   â€¢ lora_alpha: 128
[HPO]   â€¢ lora_dropout: 0.03410482473745831
[HPO]   â€¢ warmup_ratio: 0.013010318597055905
[HPO]   â€¢ max_seq_length: 1024
[HPO] Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
[HPO]   LoRA config: r=32, alpha=128, dropout=0.03410482473745831
[HPO]   max_seq_length=1024
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
[HPO] Created temporary dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_7qyhhlnf
[HPO] Model loaded with LoRA for HPO.
[HPO] Preparing training data (no augmentation for HPO)
[HPO] Total training samples: 115 (original: 115, augmented: 0)
[HPO] Training samples: 115, Validation raw records: 25
Unsloth: Model does not have a default image size - using 512
[HPO] Starting training for this trial (NO eval during training)...

{'loss': 1.6929, 'grad_norm': 4.767164707183838, 'learning_rate': 2.3891829885947592e-05, 'epoch': 0.7}
{'loss': 0.3997, 'grad_norm': 2.318023920059204, 'learning_rate': 2.1097631103776502e-05, 'epoch': 1.38}
{'loss': 0.1178, 'grad_norm': 1.0738892555236816, 'learning_rate': 1.6420407911732582e-05, 'epoch': 2.07}
{'loss': 0.0784, 'grad_norm': 0.8197601437568665, 'learning_rate': 1.1182446898673613e-05, 'epoch': 2.77}
{'loss': 0.0648, 'grad_norm': 0.7424808740615845, 'learning_rate': 5.909617193817043e-06, 'epoch': 3.45}
{'loss': 0.0553, 'grad_norm': 0.757055938243866, 'learning_rate': 1.9390053656303203e-06, 'epoch': 4.14}
{'loss': 0.0522, 'grad_norm': 3.494532346725464, 'learning_rate': 7.960209020830987e-08, 'epoch': 4.83}
{'train_runtime': 1578.3292, 'train_samples_per_second': 0.364, 'train_steps_per_second': 0.089, 'train_loss': 0.35157505784715926, 'epoch': 4.83}
[HPO] Training finished for this trial.

================================================================================
[HPO] RUNNING CER EVALUATION ON VALIDATION SET (HPO OBJECTIVE)
================================================================================
[HPO]   Using 10 validation samples for CER
[HPO]   Evaluating 1/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (110).jpg[HPO]   Evaluating 2/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (1).jpg[HPO]   Evaluating 3/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (93).jpg[HPO]   Evaluating 4/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (86).jpg[HPO]   Evaluating 5/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (80).jpg[HPO]   Evaluating 6/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (119).jpg[HPO]   Evaluating 7/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (147).jpg[HPO]   Evaluating 8/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (72).jpg[HPO]   Evaluating 9/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (118).jpg[HPO]   Evaluating 10/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (90).jpg
[HPO]   âœ… Validation CER (HPO objective): 0.5603 (56.03%)
================================================================================

[HPO]   Best Validation CER in this trial: 0.5603 (56.03%)
[HPO] Cleaned temp dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_7qyhhlnf

================================================================================
[HPO] Qwen Staircase OCR HPO TRIAL (ENHANCED MODE)
================================================================================
[HPO] Trial output directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/stair_enhanced/trials/trial_2
[HPO]   â€¢ num_epochs: 5
[HPO]   â€¢ batch_size: 1
[HPO]   â€¢ grad_accum: 4
[HPO]   â€¢ learning_rate: 1.53808216661567e-05
[HPO]   â€¢ weight_decay: 0.014650817100957579
[HPO]   â€¢ lora_r: 32
[HPO]   â€¢ lora_alpha: 16
[HPO]   â€¢ lora_dropout: 0.03697089110510541
[HPO]   â€¢ warmup_ratio: 0.19391692555291173
[HPO]   â€¢ max_seq_length: 1024
[HPO] Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
[HPO]   LoRA config: r=32, alpha=16, dropout=0.03697089110510541
[HPO]   max_seq_length=1024
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
[HPO] Created temporary dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_q6ytrexm
[HPO] Model loaded with LoRA for HPO.
[HPO] Preparing training data (no augmentation for HPO)
[HPO] Total training samples: 115 (original: 115, augmented: 0)
[HPO] Training samples: 115, Validation raw records: 25
Unsloth: Model does not have a default image size - using 512
[HPO] Starting training for this trial (NO eval during training)...

{'loss': 3.6056, 'grad_norm': 3.201845645904541, 'learning_rate': 9.887671071100736e-06, 'epoch': 0.7}
{'loss': 2.1859, 'grad_norm': 1.0463279485702515, 'learning_rate': 1.508025981545758e-05, 'epoch': 1.38}
{'loss': 1.5749, 'grad_norm': 0.5593342185020447, 'learning_rate': 1.3128352483258536e-05, 'epoch': 2.07}
{'loss': 1.3473, 'grad_norm': 0.6550197601318359, 'learning_rate': 9.401688234162701e-06, 'epoch': 2.77}
{'loss': 1.1432, 'grad_norm': 0.4453575611114502, 'learning_rate': 5.150429157079379e-06, 'epoch': 3.45}
{'loss': 1.0059, 'grad_norm': 0.7115731835365295, 'learning_rate': 1.6778055306645075e-06, 'epoch': 4.14}
{'loss': 0.9556, 'grad_norm': 0.5136643648147583, 'learning_rate': 4.835568915312995e-08, 'epoch': 4.83}
{'train_runtime': 1608.1601, 'train_samples_per_second': 0.358, 'train_steps_per_second': 0.087, 'train_loss': 1.688342503138951, 'epoch': 4.83}
[HPO] Training finished for this trial.

================================================================================
[HPO] RUNNING CER EVALUATION ON VALIDATION SET (HPO OBJECTIVE)
================================================================================
[HPO]   Using 10 validation samples for CER
[HPO]   Evaluating 1/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (110).jpg[HPO]   Evaluating 2/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (1).jpg[HPO]   Evaluating 3/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (93).jpg[HPO]   Evaluating 4/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (86).jpg[HPO]   Evaluating 5/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (80).jpg[HPO]   Evaluating 6/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (119).jpg[HPO]   Evaluating 7/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (147).jpg[HPO]   Evaluating 8/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (72).jpg[HPO]   Evaluating 9/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (118).jpg[HPO]   Evaluating 10/10: FMIS_FormblaÌˆtterMielke_gefÃ¼llt (90).jpg
[HPO]   âœ… Validation CER (HPO objective): 0.7269 (72.69%)
================================================================================

[HPO]   Best Validation CER in this trial: 0.7269 (72.69%)
[HPO] Cleaned temp dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_q6ytrexm

================================================================================
[HPO] Qwen Staircase OCR HPO TRIAL (ENHANCED MODE)
================================================================================
[HPO] Trial output directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/hpo/stair_enhanced/trials/trial_3
[HPO]   â€¢ num_epochs: 5
[HPO]   â€¢ batch_size: 1
[HPO]   â€¢ grad_accum: 8
[HPO]   â€¢ learning_rate: 4.537761219144585e-05
[HPO]   â€¢ weight_decay: 0.13828113525346752
[HPO]   â€¢ lora_r: 64
[HPO]   â€¢ lora_alpha: 64
[HPO]   â€¢ lora_dropout: 0.16043939615080793
[HPO]   â€¢ warmup_ratio: 0.014910128735954166
[HPO]   â€¢ max_seq_length: 512
[HPO] Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
[HPO]   LoRA config: r=64, alpha=64, dropout=0.16043939615080793
[HPO]   max_seq_length=512
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
[HPO] Created temporary dir: /tmp/1487952.tinygpu/qwen_hpo_tmp_aymhglgg
[HPO] Model loaded with LoRA for HPO.
[HPO] Preparing training data (no augmentation for HPO)
[HPO] Total training samples: 115 (original: 115, augmented: 0)
[HPO] Training samples: 115, Validation raw records: 25
Unsloth: Model does not have a default image size - using 512
[HPO] Starting training for this trial (NO eval during training)...

=== JOB_STATISTICS ===
=== current date     : Tue Jan 13 09:02:36 PM CET 2026
= Job-ID             : 1487952 on tinygpu
= Job-Name           : stair_hpo2
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 02:19:30
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg073
= Subm/Elig/Start/End: 2026-01-13T15:31:00 / 2026-01-13T15:31:00 / 2026-01-13T18:43:06 / 2026-01-13T21:02:36
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              88.4G   104.9G   209.7G        N/A  29,218      500K   1,000K        N/A    
    /home/woody           219.6G  1000.0G  1500.0G        N/A   1,017K   5,000K   7,500K        N/A    
    /home/vault          1047.9G  1048.6G  2097.2G        N/A   8,049      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:3B:00.0, 1658061, 50 %, 25 %, 18926 MiB, 8353058 ms
