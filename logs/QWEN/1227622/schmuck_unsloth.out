### Starting TaskPrologue of job 1227622 on tg072 at Fri Sep 26 04:45:58 PM CEST 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Fri Sep 26 16:45:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   29C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635
Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.3: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
============================================================
STARTING TRAINING FOR SCHMUCK DATASET
============================================================
Preparing training and validation datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
7.854 GB of memory reserved.
Starting training with early stopping...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.9096, 'grad_norm': 28.77302360534668, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}
{'loss': 2.2935, 'grad_norm': 4.324982166290283, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.39}
{'loss': 1.155, 'grad_norm': 2.5701651573181152, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.58}
{'loss': 0.2596, 'grad_norm': 1.089698314666748, 'learning_rate': 3.3e-05, 'epoch': 0.77}
{'loss': 0.0393, 'grad_norm': 0.7700645327568054, 'learning_rate': 4.3e-05, 'epoch': 0.97}
{'eval_loss': 0.032282356172800064, 'eval_runtime': 50.7839, 'eval_samples_per_second': 1.733, 'eval_steps_per_second': 0.866, 'epoch': 0.99}
{'loss': 0.0267, 'grad_norm': 0.36511045694351196, 'learning_rate': 4.999782812965259e-05, 'epoch': 1.17}
{'loss': 0.0234, 'grad_norm': 0.5542979836463928, 'learning_rate': 4.995922759815339e-05, 'epoch': 1.37}
{'loss': 0.0227, 'grad_norm': 0.6091301441192627, 'learning_rate': 4.987244904838563e-05, 'epoch': 1.56}
{'loss': 0.0177, 'grad_norm': 0.2551838159561157, 'learning_rate': 4.973765998627628e-05, 'epoch': 1.75}
{'loss': 0.0159, 'grad_norm': 0.7380942702293396, 'learning_rate': 4.9555120590946336e-05, 'epoch': 1.95}
{'eval_loss': 0.01910676434636116, 'eval_runtime': 48.3727, 'eval_samples_per_second': 1.819, 'eval_steps_per_second': 0.91, 'epoch': 1.99}
{'loss': 0.012, 'grad_norm': 0.29687562584877014, 'learning_rate': 4.9325183212495095e-05, 'epoch': 2.15}
{'loss': 0.0084, 'grad_norm': 0.6083372235298157, 'learning_rate': 4.904829169186982e-05, 'epoch': 2.35}
{'loss': 0.0079, 'grad_norm': 0.340141624212265, 'learning_rate': 4.872498050413334e-05, 'epoch': 2.54}
{'loss': 0.0092, 'grad_norm': 0.25700753927230835, 'learning_rate': 4.835587372678357e-05, 'epoch': 2.73}
{'loss': 0.0098, 'grad_norm': 0.12259677797555923, 'learning_rate': 4.794168383511616e-05, 'epoch': 2.93}
{'eval_loss': 0.017891965806484222, 'eval_runtime': 48.8295, 'eval_samples_per_second': 1.802, 'eval_steps_per_second': 0.901, 'epoch': 2.99}
{'loss': 0.0051, 'grad_norm': 0.18967044353485107, 'learning_rate': 4.7483210326955673e-05, 'epoch': 3.14}
{'loss': 0.0045, 'grad_norm': 0.4393308758735657, 'learning_rate': 4.698133817940986e-05, 'epoch': 3.33}
{'loss': 0.0044, 'grad_norm': 0.146936297416687, 'learning_rate': 4.643703614062601e-05, 'epoch': 3.52}
{'loss': 0.0059, 'grad_norm': 0.276523232460022, 'learning_rate': 4.585135485984656e-05, 'epoch': 3.71}
{'loss': 0.0045, 'grad_norm': 0.1997944563627243, 'learning_rate': 4.522542485937369e-05, 'epoch': 3.91}
{'eval_loss': 0.019787419587373734, 'eval_runtime': 49.0617, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.897, 'epoch': 3.99}
{'loss': 0.004, 'grad_norm': 0.2890356183052063, 'learning_rate': 4.4560454352357394e-05, 'epoch': 4.12}
{'loss': 0.0027, 'grad_norm': 0.08911462873220444, 'learning_rate': 4.3857726910619314e-05, 'epoch': 4.31}
{'loss': 0.0032, 'grad_norm': 0.18335787951946259, 'learning_rate': 4.311859898701413e-05, 'epoch': 4.5}
{'loss': 0.0031, 'grad_norm': 0.22149692475795746, 'learning_rate': 4.234449729711091e-05, 'epoch': 4.7}
{'loss': 0.0041, 'grad_norm': 0.2125939577817917, 'learning_rate': 4.153691606524857e-05, 'epoch': 4.89}
{'eval_loss': 0.02115844003856182, 'eval_runtime': 49.0905, 'eval_samples_per_second': 1.793, 'eval_steps_per_second': 0.896, 'epoch': 4.99}
{'loss': 0.0042, 'grad_norm': 3.7069687843322754, 'learning_rate': 4.069741414028127e-05, 'epoch': 5.1}
{'loss': 0.0025, 'grad_norm': 0.5584484338760376, 'learning_rate': 3.9827611986581105e-05, 'epoch': 5.29}
{'loss': 0.0022, 'grad_norm': 0.18883928656578064, 'learning_rate': 3.8929188556106255e-05, 'epoch': 5.48}
{'loss': 0.0029, 'grad_norm': 0.3022192418575287, 'learning_rate': 3.8003878047572464e-05, 'epoch': 5.68}
{'loss': 0.0019, 'grad_norm': 0.10903304070234299, 'learning_rate': 3.705346655898333e-05, 'epoch': 5.87}
{'eval_loss': 0.023898668587207794, 'eval_runtime': 48.3498, 'eval_samples_per_second': 1.82, 'eval_steps_per_second': 0.91, 'epoch': 5.99}
{'train_runtime': 3896.9934, 'train_samples_per_second': 1.59, 'train_steps_per_second': 0.196, 'train_loss': 0.25709639541695223, 'epoch': 5.99}
3896.9934 seconds used for training.
64.95 minutes used for training.
Peak reserved memory = 10.602 GB.
Peak reserved memory for training = 2.748 GB.
Peak reserved memory % of max memory = 33.41 %.
Peak reserved memory for training % of max memory = 8.66 %.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635...
Model saved successfully!

============================================================
STARTING EVALUATION ON SCHMUCK TEST SET
============================================================
Starting evaluation on test.jsonl...
Dataset: 89 samples, 5 chunks of size 20

Processing test chunk 1/5
Processing 1/20 in chunk 1Processing 2/20 in chunk 1Processing 3/20 in chunk 1Processing 4/20 in chunk 1Processing 5/20 in chunk 1Processing 6/20 in chunk 1Processing 7/20 in chunk 1Processing 8/20 in chunk 1Processing 9/20 in chunk 1Processing 10/20 in chunk 1Processing 11/20 in chunk 1Processing 12/20 in chunk 1Processing 13/20 in chunk 1Processing 14/20 in chunk 1Processing 15/20 in chunk 1Processing 16/20 in chunk 1Processing 17/20 in chunk 1Processing 18/20 in chunk 1Processing 19/20 in chunk 1Processing 20/20 in chunk 1
Processing test chunk 2/5
Processing 1/20 in chunk 2Processing 2/20 in chunk 2Processing 3/20 in chunk 2Processing 4/20 in chunk 2Processing 5/20 in chunk 2Processing 6/20 in chunk 2Processing 7/20 in chunk 2Processing 8/20 in chunk 2Processing 9/20 in chunk 2Processing 10/20 in chunk 2Processing 11/20 in chunk 2Processing 12/20 in chunk 2Processing 13/20 in chunk 2Processing 14/20 in chunk 2Processing 15/20 in chunk 2Processing 16/20 in chunk 2Processing 17/20 in chunk 2Processing 18/20 in chunk 2Processing 19/20 in chunk 2Processing 20/20 in chunk 2
Processing test chunk 3/5
Processing 1/20 in chunk 3Processing 2/20 in chunk 3Processing 3/20 in chunk 3Processing 4/20 in chunk 3Processing 5/20 in chunk 3Processing 6/20 in chunk 3Processing 7/20 in chunk 3Processing 8/20 in chunk 3Processing 9/20 in chunk 3Processing 10/20 in chunk 3Processing 11/20 in chunk 3Processing 12/20 in chunk 3Processing 13/20 in chunk 3Processing 14/20 in chunk 3Processing 15/20 in chunk 3Processing 16/20 in chunk 3Processing 17/20 in chunk 3Processing 18/20 in chunk 3Processing 19/20 in chunk 3Processing 20/20 in chunk 3
Processing test chunk 4/5
Processing 1/20 in chunk 4Processing 2/20 in chunk 4Processing 3/20 in chunk 4Processing 4/20 in chunk 4Processing 5/20 in chunk 4Processing 6/20 in chunk 4Processing 7/20 in chunk 4Processing 8/20 in chunk 4Processing 9/20 in chunk 4Processing 10/20 in chunk 4Processing 11/20 in chunk 4Processing 12/20 in chunk 4Processing 13/20 in chunk 4Processing 14/20 in chunk 4Processing 15/20 in chunk 4Processing 16/20 in chunk 4Processing 17/20 in chunk 4Processing 18/20 in chunk 4Processing 19/20 in chunk 4Processing 20/20 in chunk 4
Processing test chunk 5/5
Processing 1/9 in chunk 5Processing 2/9 in chunk 5Processing 3/9 in chunk 5Processing 4/9 in chunk 5Processing 5/9 in chunk 5Processing 6/9 in chunk 5Processing 7/9 in chunk 5Processing 8/9 in chunk 5Processing 9/9 in chunk 5CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635/cer_evaluation_results.txt

Evaluation completed!
Predictions saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635/test_predictions.jsonl
CER results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635/cer_evaluation_results.txt
All files saved in: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635

============================================================
FINAL RESULTS SUMMARY - SCHMUCK DATASET
============================================================
âœ… Fixed: Model now generates proper JSON output
âœ… Fixed: Batch decode handling resolved
Average CER: 0.0053 (0.53%)
Median CER: 0.0000 (0.00%)
Perfect matches: 48/89 (53.93%)
Total images processed: 89

Schmu ck dataset training and evaluation completed successfully!
All outputs saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250926_164635
=== JOB_STATISTICS ===
=== current date     : Fri Sep 26 06:35:43 PM CEST 2025
= Job-ID             : 1227622 on tinygpu
= Job-Name           : qwen_schmuck
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 10:00:00
= Elapsed runtime    : 01:49:47
= Total RAM usage    : 20.7 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2025-09-26T16:45:37 / 2025-09-26T16:45:37 / 2025-09-26T16:45:38 / 2025-09-26T18:35:25
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody            64.4G  1000.0G  1500.0G        N/A     346K   5,000K   7,500K        N/A    
    /home/hpc              61.3G   104.9G   209.7G        N/A  35,752      500K   1,000K        N/A    
    /home/vault           629.5G  1048.6G  2097.2G        N/A   3,339      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 4188513, 52 %, 24 %, 18792 MiB, 6575030 ms
