### Starting TaskPrologue of job 1206761 on tg072 at Sat Sep 13 07:19:59 PM CEST 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Sat Sep 13 19:19:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   34C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
[2025-09-13 19:20:46,653] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-13 19:20:53,691] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Created run directory: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118
Loading Qwen2.5-VL model from /home/vault/iwi5/iwi5298h/models/qwen7b with Unsloth...
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.7.11: Fast Qwen2 patching. Transformers: 4.51.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to eager!
Unsloth: Making `model.base_model.model.visual` require gradients
Model loaded successfully with optimized LoRA adapters!
============================================================
STARTING TRAINING FOR SCHMUCK DATASET
============================================================
Preparing training and validation datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples
Unsloth: Model does not have a default image size - using 512
GPU = Tesla V100-PCIE-32GB. Max memory = 31.733 GB.
7.854 GB of memory reserved.
Starting training with early stopping...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.8574, 'grad_norm': nan, 'learning_rate': 5e-06, 'epoch': 0.19}
{'loss': 2.3604, 'grad_norm': 3.8996429443359375, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.39}
{'loss': 1.1525, 'grad_norm': 2.5912063121795654, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.58}
{'loss': 0.268, 'grad_norm': 1.1205425262451172, 'learning_rate': 3.3e-05, 'epoch': 0.77}
{'loss': 0.04, 'grad_norm': 0.6324971914291382, 'learning_rate': 4.3e-05, 'epoch': 0.97}
{'eval_loss': 0.03381139412522316, 'eval_runtime': 50.6883, 'eval_samples_per_second': 1.736, 'eval_steps_per_second': 0.868, 'epoch': 0.99}
{'loss': 0.0268, 'grad_norm': 0.28574514389038086, 'learning_rate': 4.999782812965259e-05, 'epoch': 1.17}
{'loss': 0.0241, 'grad_norm': 0.4989619553089142, 'learning_rate': 4.995922759815339e-05, 'epoch': 1.37}
{'loss': 0.0232, 'grad_norm': 0.6951557397842407, 'learning_rate': 4.987244904838563e-05, 'epoch': 1.56}
{'loss': 0.0178, 'grad_norm': 0.5441486239433289, 'learning_rate': 4.973765998627628e-05, 'epoch': 1.75}
{'loss': 0.0164, 'grad_norm': 0.6124634742736816, 'learning_rate': 4.9555120590946336e-05, 'epoch': 1.95}
{'eval_loss': 0.019677581265568733, 'eval_runtime': 48.707, 'eval_samples_per_second': 1.807, 'eval_steps_per_second': 0.903, 'epoch': 1.99}
{'loss': 0.0118, 'grad_norm': 0.20018059015274048, 'learning_rate': 4.9325183212495095e-05, 'epoch': 2.15}
{'loss': 0.0084, 'grad_norm': 0.4177294671535492, 'learning_rate': 4.904829169186982e-05, 'epoch': 2.35}
{'loss': 0.009, 'grad_norm': 0.5865476131439209, 'learning_rate': 4.872498050413334e-05, 'epoch': 2.54}
{'loss': 0.0086, 'grad_norm': 0.16762477159500122, 'learning_rate': 4.835587372678357e-05, 'epoch': 2.73}
{'loss': 0.0098, 'grad_norm': 0.1310601681470871, 'learning_rate': 4.794168383511616e-05, 'epoch': 2.93}
{'eval_loss': 0.01949404738843441, 'eval_runtime': 48.9447, 'eval_samples_per_second': 1.798, 'eval_steps_per_second': 0.899, 'epoch': 2.99}
{'loss': 0.0063, 'grad_norm': 0.27841800451278687, 'learning_rate': 4.7483210326955673e-05, 'epoch': 3.14}
{'loss': 0.0055, 'grad_norm': 0.3787188231945038, 'learning_rate': 4.698133817940986e-05, 'epoch': 3.33}
{'loss': 0.006, 'grad_norm': 1.3178261518478394, 'learning_rate': 4.643703614062601e-05, 'epoch': 3.52}
{'loss': 0.007, 'grad_norm': 0.21097008883953094, 'learning_rate': 4.585135485984656e-05, 'epoch': 3.71}
{'loss': 0.0055, 'grad_norm': 0.40135446190834045, 'learning_rate': 4.522542485937369e-05, 'epoch': 3.91}
{'eval_loss': 0.019307564944028854, 'eval_runtime': 48.6822, 'eval_samples_per_second': 1.808, 'eval_steps_per_second': 0.904, 'epoch': 3.99}
{'loss': 0.0046, 'grad_norm': 0.18285676836967468, 'learning_rate': 4.4560454352357394e-05, 'epoch': 4.12}
{'loss': 0.0034, 'grad_norm': 0.11088497191667557, 'learning_rate': 4.3857726910619314e-05, 'epoch': 4.31}
{'loss': 0.0041, 'grad_norm': 0.25605788826942444, 'learning_rate': 4.311859898701413e-05, 'epoch': 4.5}
{'loss': 0.003, 'grad_norm': 0.10400453954935074, 'learning_rate': 4.234449729711091e-05, 'epoch': 4.7}
{'loss': 0.0038, 'grad_norm': 0.36747950315475464, 'learning_rate': 4.153691606524857e-05, 'epoch': 4.89}
{'eval_loss': 0.021686328575015068, 'eval_runtime': 48.4848, 'eval_samples_per_second': 1.815, 'eval_steps_per_second': 0.908, 'epoch': 4.99}
{'train_runtime': 3254.8145, 'train_samples_per_second': 1.903, 'train_steps_per_second': 0.235, 'train_loss': 0.30929631127738483, 'epoch': 4.99}
3254.8145 seconds used for training.
54.25 minutes used for training.
Peak reserved memory = 10.602 GB.
Peak reserved memory for training = 2.748 GB.
Peak reserved memory % of max memory = 33.41 %.
Peak reserved memory for training % of max memory = 8.66 %.
Saving model to /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118...
Model saved successfully!

============================================================
STARTING EVALUATION ON SCHMUCK TEST SET
============================================================
Starting evaluation on test.jsonl...
Dataset: 89 samples, 5 chunks of size 20

Processing test chunk 1/5
Processing 1/20 in chunk 1Processing 2/20 in chunk 1Processing 3/20 in chunk 1Processing 4/20 in chunk 1Processing 5/20 in chunk 1Processing 6/20 in chunk 1Processing 7/20 in chunk 1Processing 8/20 in chunk 1Processing 9/20 in chunk 1Processing 10/20 in chunk 1Processing 11/20 in chunk 1Processing 12/20 in chunk 1Processing 13/20 in chunk 1Processing 14/20 in chunk 1Processing 15/20 in chunk 1Processing 16/20 in chunk 1Processing 17/20 in chunk 1Processing 18/20 in chunk 1Processing 19/20 in chunk 1Processing 20/20 in chunk 1
Processing test chunk 2/5
Processing 1/20 in chunk 2Processing 2/20 in chunk 2Processing 3/20 in chunk 2Processing 4/20 in chunk 2Processing 5/20 in chunk 2Processing 6/20 in chunk 2Processing 7/20 in chunk 2Processing 8/20 in chunk 2Processing 9/20 in chunk 2Processing 10/20 in chunk 2Processing 11/20 in chunk 2Processing 12/20 in chunk 2Processing 13/20 in chunk 2Processing 14/20 in chunk 2Processing 15/20 in chunk 2Processing 16/20 in chunk 2Processing 17/20 in chunk 2Processing 18/20 in chunk 2Processing 19/20 in chunk 2Processing 20/20 in chunk 2
Processing test chunk 3/5
Processing 1/20 in chunk 3Processing 2/20 in chunk 3Processing 3/20 in chunk 3Processing 4/20 in chunk 3Processing 5/20 in chunk 3Processing 6/20 in chunk 3Processing 7/20 in chunk 3Processing 8/20 in chunk 3Processing 9/20 in chunk 3Processing 10/20 in chunk 3Processing 11/20 in chunk 3Processing 12/20 in chunk 3Processing 13/20 in chunk 3Processing 14/20 in chunk 3Processing 15/20 in chunk 3Processing 16/20 in chunk 3Processing 17/20 in chunk 3Processing 18/20 in chunk 3Processing 19/20 in chunk 3Processing 20/20 in chunk 3
Processing test chunk 4/5
Processing 1/20 in chunk 4Processing 2/20 in chunk 4Processing 3/20 in chunk 4Processing 4/20 in chunk 4Processing 5/20 in chunk 4Processing 6/20 in chunk 4Processing 7/20 in chunk 4Processing 8/20 in chunk 4Processing 9/20 in chunk 4Processing 10/20 in chunk 4Processing 11/20 in chunk 4Processing 12/20 in chunk 4Processing 13/20 in chunk 4Processing 14/20 in chunk 4Processing 15/20 in chunk 4Processing 16/20 in chunk 4Processing 17/20 in chunk 4Processing 18/20 in chunk 4Processing 19/20 in chunk 4Processing 20/20 in chunk 4
Processing test chunk 5/5
Processing 1/9 in chunk 5Processing 2/9 in chunk 5Processing 3/9 in chunk 5Processing 4/9 in chunk 5Processing 5/9 in chunk 5Processing 6/9 in chunk 5Processing 7/9 in chunk 5Processing 8/9 in chunk 5Processing 9/9 in chunk 5CER evaluation results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118/cer_evaluation_results.txt

Evaluation completed!
Predictions saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118/test_predictions.jsonl
CER results saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118/cer_evaluation_results.txt
All files saved in: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118

============================================================
FINAL RESULTS SUMMARY - SCHMUCK DATASET
============================================================
âœ… Fixed: Model now generates proper JSON output
âœ… Fixed: Batch decode handling resolved
Average CER: 0.0056 (0.56%)
Median CER: 0.0029 (0.29%)
Perfect matches: 28/89 (31.46%)
Total images processed: 89

Schmu ck dataset training and evaluation completed successfully!
All outputs saved to: /home/vault/iwi5/iwi5298h/models_image_text/qwen/schmuck_dataset/run_20250913_192118
=== JOB_STATISTICS ===
=== current date     : Sat Sep 13 08:59:05 PM CEST 2025
= Job-ID             : 1206761 on tinygpu
= Job-Name           : qwen_schmuck
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_qwen.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 10:00:00
= Elapsed runtime    : 01:39:07
= Total RAM usage    : 15.1 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2025-09-13T14:04:45 / 2025-09-13T14:04:45 / 2025-09-13T19:19:47 / 2025-09-13T20:58:54
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              26.1G   104.9G   209.7G        N/A  32,509      500K   1,000K        N/A    
    /home/woody            46.8G  1000.0G  1500.0G        N/A     239K   5,000K   7,500K        N/A    
    /home/vault           872.5G  1048.6G  2097.2G        N/A   2,926      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 4193723, 51 %, 24 %, 18766 MiB, 5932328 ms
