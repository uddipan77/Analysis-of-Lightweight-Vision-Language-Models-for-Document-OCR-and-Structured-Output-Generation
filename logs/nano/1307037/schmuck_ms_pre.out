### Starting TaskPrologue of job 1307037 on tg074 at Wed Nov  5 08:58:05 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Wed Nov  5 20:58:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:AF:00.0 Off |                    0 |
| N/A   34C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!

============================================================
NANONETS-OCR-S FINETUNING WITH UNSLOTH (MULTI-STAGE, STABLE)
============================================================
Model: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
Output: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_205850_unsloth_multistage
============================================================

============================================================
LOADING NANONETS-OCR-S WITH UNSLOTH
============================================================
Model path: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
Created temporary directory for preprocessed images: /tmp/1307037.tinygpu/schmuck_preproc_bfocv885
âœ“ Model directory verified
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.10.7: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to fast eager.

âœ… Model loaded successfully!
Applying LoRA adaptors...
Unsloth: Making `model.base_model.model.model` require gradients
âœ… LoRA applied successfully!
============================================================


============================================================
PHASE 1+2: MULTI-STAGE TRAINING
============================================================

Preparing datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples


===== STAGE 1: Warm-up training (no eval, teacher forcing) =====

Unsloth: Model does not have a default image size - using 512
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.4151, 'grad_norm': 1.193328857421875, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.19}
{'loss': 0.4081, 'grad_norm': 1.1515463590621948, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.39}
{'loss': 0.3734, 'grad_norm': 0.966184675693512, 'learning_rate': 2.9e-06, 'epoch': 0.58}
{'loss': 0.3312, 'grad_norm': 0.6094619631767273, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.77}
{'loss': 0.284, 'grad_norm': 0.4035198390483856, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.97}
{'loss': 0.2644, 'grad_norm': 0.27240896224975586, 'learning_rate': 4.9115889547708975e-06, 'epoch': 1.15}
{'loss': 0.2229, 'grad_norm': 0.2887845039367676, 'learning_rate': 4.613990008754565e-06, 'epoch': 1.35}
{'loss': 0.2028, 'grad_norm': 0.28212660551071167, 'learning_rate': 4.132055295476304e-06, 'epoch': 1.54}
{'loss': 0.1781, 'grad_norm': 0.2576785981655121, 'learning_rate': 3.5078085732199314e-06, 'epoch': 1.74}
{'loss': 0.1589, 'grad_norm': 0.24300776422023773, 'learning_rate': 2.7956829273034146e-06, 'epoch': 1.93}
{'loss': 0.1529, 'grad_norm': 0.2710086703300476, 'learning_rate': 2.057774312193568e-06, 'epoch': 2.12}
{'loss': 0.1326, 'grad_norm': 0.2821893095970154, 'learning_rate': 1.3584269065157175e-06, 'epoch': 2.31}
{'loss': 0.1215, 'grad_norm': 0.3175787627696991, 'learning_rate': 7.586224273425083e-07, 'epoch': 2.5}
{'loss': 0.1223, 'grad_norm': 0.30120477080345154, 'learning_rate': 3.1066264451090816e-07, 'epoch': 2.7}
{'loss': 0.1169, 'grad_norm': 0.3193844258785248, 'learning_rate': 5.360876925123992e-08, 'epoch': 2.89}
{'train_runtime': 1500.594, 'train_samples_per_second': 0.826, 'train_steps_per_second': 0.104, 'train_loss': 0.228369537454385, 'epoch': 3.0}

===== STAGE 2: Main training (eval each epoch, loss-based best model) =====

Unsloth: Model does not have a default image size - using 512

============================================================
GPU INFORMATION
============================================================
GPU: Tesla V100-PCIE-32GB
Max memory: 31.733 GB
Reserved: 5.693 GB

âœ… UNSLOTH OPTIMIZATIONS:
   â€¢ 4-bit quantization
   â€¢ LoRA rank 16 (RSLoRA)
   â€¢ Gradient checkpointing
   â€¢ Stage 2 Batch size: 1, Grad accum: 8
   â€¢ Max seq length: 1536
============================================================

Starting Stage 2 training with eval loss-based best model...
{'loss': 0.1134, 'grad_norm': 0.294955313205719, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.19}
{'loss': 0.1052, 'grad_norm': 0.2715219557285309, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.39}
{'loss': 0.0877, 'grad_norm': 0.2176002562046051, 'learning_rate': 5.8e-06, 'epoch': 0.58}
{'loss': 0.0758, 'grad_norm': 0.20338408648967743, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.77}
{'loss': 0.0666, 'grad_norm': 0.13232216238975525, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.97}
{'eval_loss': 0.42588016390800476, 'eval_runtime': 45.6464, 'eval_samples_per_second': 1.928, 'eval_steps_per_second': 1.928, 'epoch': 1.0}
{'loss': 0.0578, 'grad_norm': 0.08629433065652847, 'learning_rate': 9.9939352462415e-06, 'epoch': 1.15}
{'loss': 0.0506, 'grad_norm': 0.07680065184831619, 'learning_rate': 9.972989553272501e-06, 'epoch': 1.35}
{'loss': 0.0543, 'grad_norm': 0.08681277930736542, 'learning_rate': 9.93715076075656e-06, 'epoch': 1.54}
{'loss': 0.0491, 'grad_norm': 0.10166986286640167, 'learning_rate': 9.88652619864055e-06, 'epoch': 1.74}
{'loss': 0.0442, 'grad_norm': 0.08354996144771576, 'learning_rate': 9.821267477265705e-06, 'epoch': 1.93}
{'eval_loss': 0.31910860538482666, 'eval_runtime': 46.0786, 'eval_samples_per_second': 1.91, 'eval_steps_per_second': 1.91, 'epoch': 2.0}
{'loss': 0.0453, 'grad_norm': 0.08376079797744751, 'learning_rate': 9.741570033325254e-06, 'epoch': 2.12}
{'loss': 0.0413, 'grad_norm': 0.08659744262695312, 'learning_rate': 9.647672544570981e-06, 'epoch': 2.31}
{'loss': 0.0378, 'grad_norm': 0.07392235100269318, 'learning_rate': 9.539856215021568e-06, 'epoch': 2.5}
{'loss': 0.0412, 'grad_norm': 0.08697035908699036, 'learning_rate': 9.418443932813328e-06, 'epoch': 2.7}
{'loss': 0.0384, 'grad_norm': 0.07848239690065384, 'learning_rate': 9.283799303215442e-06, 'epoch': 2.89}
{'eval_loss': 0.2678433060646057, 'eval_runtime': 45.8531, 'eval_samples_per_second': 1.919, 'eval_steps_per_second': 1.919, 'epoch': 3.0}
{'loss': 0.0402, 'grad_norm': 0.08340933918952942, 'learning_rate': 9.136325559705593e-06, 'epoch': 3.08}
{'loss': 0.0343, 'grad_norm': 0.07392504066228867, 'learning_rate': 8.976464356367133e-06, 'epoch': 3.27}
{'loss': 0.0347, 'grad_norm': 0.09799209982156754, 'learning_rate': 8.804694445224274e-06, 'epoch': 3.46}
{'loss': 0.0331, 'grad_norm': 0.07239530980587006, 'learning_rate': 8.621530242476446e-06, 'epoch': 3.66}
{'loss': 0.0323, 'grad_norm': 0.09044761210680008, 'learning_rate': 8.427520287925669e-06, 'epoch': 3.85}
{'eval_loss': 0.23033881187438965, 'eval_runtime': 45.6495, 'eval_samples_per_second': 1.928, 'eval_steps_per_second': 1.928, 'epoch': 4.0}
{'loss': 0.0335, 'grad_norm': 0.08042997121810913, 'learning_rate': 8.22324560221058e-06, 'epoch': 4.04}
{'loss': 0.0334, 'grad_norm': 0.08522440493106842, 'learning_rate': 8.009317946766975e-06, 'epoch': 4.23}
{'loss': 0.027, 'grad_norm': 0.10507892072200775, 'learning_rate': 7.786377991725813e-06, 'epoch': 4.43}
{'loss': 0.0267, 'grad_norm': 0.12281005084514618, 'learning_rate': 7.555093397235553e-06, 'epoch': 4.62}
{'loss': 0.026, 'grad_norm': 0.11056715995073318, 'learning_rate': 7.316156813954821e-06, 'epoch': 4.81}
{'loss': 0.0322, 'grad_norm': 0.18035179376602173, 'learning_rate': 7.070283808703553e-06, 'epoch': 5.0}
{'eval_loss': 0.20874647796154022, 'eval_runtime': 45.7816, 'eval_samples_per_second': 1.922, 'eval_steps_per_second': 1.922, 'epoch': 5.0}
{'loss': 0.029, 'grad_norm': 0.09279187768697739, 'learning_rate': 6.818210721484859e-06, 'epoch': 5.19}
{'loss': 0.0273, 'grad_norm': 0.0899137333035469, 'learning_rate': 6.5606924602953925e-06, 'epoch': 5.39}
{'loss': 0.0242, 'grad_norm': 0.09097300469875336, 'learning_rate': 6.298500240328342e-06, 'epoch': 5.58}
{'loss': 0.0255, 'grad_norm': 0.09193161129951477, 'learning_rate': 6.032419274339654e-06, 'epoch': 5.77}
{'loss': 0.0227, 'grad_norm': 0.0923597663640976, 'learning_rate': 5.763246421094373e-06, 'epoch': 5.97}
{'eval_loss': 0.1957593411207199, 'eval_runtime': 45.4703, 'eval_samples_per_second': 1.935, 'eval_steps_per_second': 1.935, 'epoch': 6.0}
{'loss': 0.0239, 'grad_norm': 0.10331779718399048, 'learning_rate': 5.491787798935557e-06, 'epoch': 6.15}
{'loss': 0.0226, 'grad_norm': 0.1041790321469307, 'learning_rate': 5.218856371622605e-06, 'epoch': 6.35}
{'loss': 0.0237, 'grad_norm': 0.10506964474916458, 'learning_rate': 4.945269513668962e-06, 'epoch': 6.54}
{'loss': 0.0223, 'grad_norm': 0.08165571838617325, 'learning_rate': 4.671846562470489e-06, 'epoch': 6.74}
{'loss': 0.0266, 'grad_norm': 0.10247154533863068, 'learning_rate': 4.3994063645554185e-06, 'epoch': 6.93}
{'eval_loss': 0.18721729516983032, 'eval_runtime': 45.4477, 'eval_samples_per_second': 1.936, 'eval_steps_per_second': 1.936, 'epoch': 7.0}
{'loss': 0.0227, 'grad_norm': 0.10476264357566833, 'learning_rate': 4.1287648233043366e-06, 'epoch': 7.12}
{'loss': 0.0223, 'grad_norm': 0.1168799176812172, 'learning_rate': 3.860732455484314e-06, 'epoch': 7.31}
{'loss': 0.0224, 'grad_norm': 0.09729446470737457, 'learning_rate': 3.5961119639148443e-06, 'epoch': 7.5}
{'loss': 0.0231, 'grad_norm': 0.11999327689409256, 'learning_rate': 3.335695833535001e-06, 'epoch': 7.7}
{'loss': 0.0205, 'grad_norm': 0.10992354899644852, 'learning_rate': 3.0802639580710465e-06, 'epoch': 7.89}
{'eval_loss': 0.18260492384433746, 'eval_runtime': 45.9842, 'eval_samples_per_second': 1.914, 'eval_steps_per_second': 1.914, 'epoch': 8.0}
{'loss': 0.0217, 'grad_norm': 0.09882791340351105, 'learning_rate': 2.83058130441221e-06, 'epoch': 8.08}
{'loss': 0.02, 'grad_norm': 0.12918800115585327, 'learning_rate': 2.587395621689325e-06, 'epoch': 8.27}
{'loss': 0.0184, 'grad_norm': 0.0854264423251152, 'learning_rate': 2.351435201917159e-06, 'epoch': 8.46}
{'loss': 0.0224, 'grad_norm': 0.10503040999174118, 'learning_rate': 2.1234066989068972e-06, 'epoch': 8.66}
{'loss': 0.0235, 'grad_norm': 0.1384073793888092, 'learning_rate': 1.9039930119806698e-06, 'epoch': 8.85}
{'eval_loss': 0.18041060864925385, 'eval_runtime': 46.047, 'eval_samples_per_second': 1.911, 'eval_steps_per_second': 1.911, 'epoch': 9.0}
{'loss': 0.02, 'grad_norm': 0.0968828797340393, 'learning_rate': 1.6938512408259655e-06, 'epoch': 9.04}
{'loss': 0.0203, 'grad_norm': 0.114517942070961, 'learning_rate': 1.4936107176147606e-06, 'epoch': 9.23}
{'loss': 0.0201, 'grad_norm': 0.09063895046710968, 'learning_rate': 1.303871122280742e-06, 'epoch': 9.43}
{'loss': 0.0231, 'grad_norm': 0.11387566477060318, 'learning_rate': 1.1252006865989868e-06, 'epoch': 9.62}
{'loss': 0.018, 'grad_norm': 0.10559308528900146, 'learning_rate': 9.58134492446543e-07, 'epoch': 9.81}
{'loss': 0.0201, 'grad_norm': 0.22487005591392517, 'learning_rate': 8.031728693402502e-07, 'epoch': 10.0}
{'eval_loss': 0.1791946440935135, 'eval_runtime': 45.9256, 'eval_samples_per_second': 1.916, 'eval_steps_per_second': 1.916, 'epoch': 10.0}
{'loss': 0.0204, 'grad_norm': 0.11563724279403687, 'learning_rate': 6.607798960508693e-07, 'epoch': 10.19}
{'loss': 0.0185, 'grad_norm': 0.08415350317955017, 'learning_rate': 5.313820107808665e-07, 'epoch': 10.39}
{'loss': 0.0217, 'grad_norm': 0.10239719599485397, 'learning_rate': 4.153667340681067e-07, 'epoch': 10.58}
{'loss': 0.0189, 'grad_norm': 0.09562382102012634, 'learning_rate': 3.1308150824009785e-07, 'epoch': 10.77}
{'loss': 0.0192, 'grad_norm': 0.10276822745800018, 'learning_rate': 2.2483265689436929e-07, 'epoch': 10.97}
{'eval_loss': 0.17896966636180878, 'eval_runtime': 45.8234, 'eval_samples_per_second': 1.92, 'eval_steps_per_second': 1.92, 'epoch': 11.0}
{'loss': 0.0202, 'grad_norm': 0.10419514775276184, 'learning_rate': 1.5088446752115403e-07, 'epoch': 11.15}
{'loss': 0.0183, 'grad_norm': 0.10198044031858444, 'learning_rate': 9.145840001572537e-08, 'epoch': 11.35}
{'loss': 0.0176, 'grad_norm': 0.10751647502183914, 'learning_rate': 4.67324234507216e-08, 'epoch': 11.54}
{'loss': 0.0238, 'grad_norm': 0.1337641328573227, 'learning_rate': 1.6840483094713867e-08, 'epoch': 11.74}
{'loss': 0.0184, 'grad_norm': 0.09114158898591995, 'learning_rate': 1.8720992731741104e-09, 'epoch': 11.93}
{'eval_loss': 0.17897750437259674, 'eval_runtime': 45.8265, 'eval_samples_per_second': 1.92, 'eval_steps_per_second': 1.92, 'epoch': 12.0}
{'train_runtime': 6576.8131, 'train_samples_per_second': 0.754, 'train_steps_per_second': 0.095, 'train_loss': 0.0329095808048852, 'epoch': 12.0}

============================================================
TRAINING COMPLETED (Stage 2)
============================================================
Runtime: 109.61 min
Peak memory: 5.693 GB
Training memory: 0.0 GB
============================================================


Saving model to /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_205850_unsloth_multistage...
âœ… Model saved!

============================================================
PHASE 3: EVALUATION (CHUNKED)
============================================================

Starting evaluation with chunked loading...
Dataset: 89 samples, 5 chunks of size 20

Processing chunk 1/5
Processing 1/20 in chunk 1Processing 2/20 in chunk 1Processing 3/20 in chunk 1Processing 4/20 in chunk 1Processing 5/20 in chunk 1Processing 6/20 in chunk 1Processing 7/20 in chunk 1Processing 8/20 in chunk 1Processing 9/20 in chunk 1Processing 10/20 in chunk 1Processing 11/20 in chunk 1Processing 12/20 in chunk 1Processing 13/20 in chunk 1Processing 14/20 in chunk 1Processing 15/20 in chunk 1Processing 16/20 in chunk 1Processing 17/20 in chunk 1Processing 18/20 in chunk 1Processing 19/20 in chunk 1Processing 20/20 in chunk 1
Processing chunk 2/5
Processing 1/20 in chunk 2Processing 2/20 in chunk 2Processing 3/20 in chunk 2Processing 4/20 in chunk 2Processing 5/20 in chunk 2Processing 6/20 in chunk 2Processing 7/20 in chunk 2Processing 8/20 in chunk 2Processing 9/20 in chunk 2Processing 10/20 in chunk 2Processing 11/20 in chunk 2Processing 12/20 in chunk 2Processing 13/20 in chunk 2Processing 14/20 in chunk 2Processing 15/20 in chunk 2Processing 16/20 in chunk 2Processing 17/20 in chunk 2Processing 18/20 in chunk 2Processing 19/20 in chunk 2Processing 20/20 in chunk 2
Processing chunk 3/5
Processing 1/20 in chunk 3Processing 2/20 in chunk 3Processing 3/20 in chunk 3Processing 4/20 in chunk 3Processing 5/20 in chunk 3Processing 6/20 in chunk 3Processing 7/20 in chunk 3Processing 8/20 in chunk 3Processing 9/20 in chunk 3Processing 10/20 in chunk 3Processing 11/20 in chunk 3Processing 12/20 in chunk 3Processing 13/20 in chunk 3Processing 14/20 in chunk 3Processing 15/20 in chunk 3Processing 16/20 in chunk 3Processing 17/20 in chunk 3Processing 18/20 in chunk 3Processing 19/20 in chunk 3Processing 20/20 in chunk 3
Processing chunk 4/5
Processing 1/20 in chunk 4Processing 2/20 in chunk 4Processing 3/20 in chunk 4Processing 4/20 in chunk 4Processing 5/20 in chunk 4Processing 6/20 in chunk 4Processing 7/20 in chunk 4Processing 8/20 in chunk 4Processing 9/20 in chunk 4Processing 10/20 in chunk 4Processing 11/20 in chunk 4Processing 12/20 in chunk 4Processing 13/20 in chunk 4Processing 14/20 in chunk 4Processing 15/20 in chunk 4Processing 16/20 in chunk 4Processing 17/20 in chunk 4Processing 18/20 in chunk 4Processing 19/20 in chunk 4Processing 20/20 in chunk 4
Processing chunk 5/5
Processing 1/9 in chunk 5Processing 2/9 in chunk 5Processing 3/9 in chunk 5Processing 4/9 in chunk 5Processing 5/9 in chunk 5Processing 6/9 in chunk 5Processing 7/9 in chunk 5Processing 8/9 in chunk 5Processing 9/9 in chunk 5
Results saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_205850_unsloth_multistage/cer_evaluation_results.txt

============================================================
FINAL RESULTS
============================================================
Model: Nanonets-OCR-s (finetuned, best by eval loss)
Dataset: Schmuck Jewelry Catalog

Metrics:
  Avg CER: 0.0470 (4.70%)
  Weighted CER: 0.0480 (4.80%)
  Median: 0.0320 | Std: 0.0519

Accuracy:
  Perfect: 5/89 (5.62%)
  Parsing: 0/89 (0.00%)

Saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_205850_unsloth_multistage
============================================================

Cleaned up temporary directory: /tmp/1307037.tinygpu/schmuck_preproc_bfocv885
Nano training completed at: Thu Nov  6 12:06:12 AM CET 2025
=== JOB_STATISTICS ===
=== current date     : Thu Nov  6 12:06:12 AM CET 2025
= Job-ID             : 1307037 on tinygpu
= Job-Name           : schmuck_ms_pre
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_nano.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 03:08:08
= Total RAM usage    : 9.5 GiB of requested  GiB (%)   
= Node list          : tg074
= Subm/Elig/Start/End: 2025-11-05T20:27:24 / 2025-11-05T20:27:24 / 2025-11-05T20:58:27 / 2025-11-06T00:06:35
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc             102.2G   104.9G   209.7G        N/A  30,706      500K   1,000K        N/A    
    /home/woody           184.1G  1000.0G  1500.0G        N/A     841K   5,000K   7,500K        N/A    
    /home/vault           904.7G  1048.6G  2097.2G        N/A   5,889      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 2359284, 33 %, 15 %, 6238 MiB, 11276988 ms
