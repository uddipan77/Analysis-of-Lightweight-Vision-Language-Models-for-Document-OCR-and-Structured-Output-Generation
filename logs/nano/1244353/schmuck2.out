### Starting TaskPrologue of job 1244353 on tg080 at Mon Oct 20 11:59:37 PM CEST 2025
Running on cores 4-5,20-21,36-37,52-53 with governor powersave
Mon Oct 20 23:59:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:3D:00.0 Off |                  N/A |
| 30%   33C    P8             20W /  300W |       1MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!

============================================================
NANONETS-OCR-S FINETUNING WITH UNSLOTH
============================================================
Model: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
Output: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/schmuck/run_20251021_000001_unsloth
============================================================

============================================================
LOADING NANONETS-OCR-S WITH UNSLOTH
============================================================
Model path: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
âœ“ Model directory verified
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.10.7: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 3080. Num GPUs = 1. Max memory: 9.644 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to fast eager.

âœ… Model loaded successfully!
Applying LoRA adaptors...
Unsloth: Making `model.base_model.model.model` require gradients
âœ… LoRA applied successfully!
============================================================


============================================================
PHASE 1: TRAINING
============================================================

Preparing datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples

Unsloth: Model does not have a default image size - using 512

============================================================
GPU INFORMATION
============================================================
GPU: NVIDIA GeForce RTX 3080
Max memory: 9.644 GB
Reserved: 3.594 GB

âœ… UNSLOTH OPTIMIZATIONS:
   â€¢ 4-bit quantization
   â€¢ LoRA rank 16 (RSLoRA)
   â€¢ Gradient checkpointing
   â€¢ Batch size: 2, Grad accum: 4
============================================================

Starting training...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.6899, 'grad_norm': 2.2306408882141113, 'learning_rate': 9e-06, 'epoch': 0.19}
{'loss': 0.5205, 'grad_norm': 0.5923997163772583, 'learning_rate': 1.9e-05, 'epoch': 0.39}
{'loss': 0.328, 'grad_norm': 0.5905028581619263, 'learning_rate': 2.9e-05, 'epoch': 0.58}
{'loss': 0.1699, 'grad_norm': 0.5029515624046326, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.77}
{'loss': 0.0348, 'grad_norm': 0.0632178783416748, 'learning_rate': 4.9e-05, 'epoch': 0.97}
{'eval_loss': 0.07455699890851974, 'eval_runtime': 45.9163, 'eval_samples_per_second': 1.917, 'eval_steps_per_second': 0.958, 'epoch': 1.0}
{'loss': 0.0178, 'grad_norm': 0.05073593184351921, 'learning_rate': 4.9981250280992086e-05, 'epoch': 1.15}
{'loss': 0.0155, 'grad_norm': 0.051714036613702774, 'learning_rate': 4.9916472543854254e-05, 'epoch': 1.35}
{'loss': 0.0168, 'grad_norm': 0.059142209589481354, 'learning_rate': 4.980555523544831e-05, 'epoch': 1.54}
{'loss': 0.0136, 'grad_norm': 0.057537876069545746, 'learning_rate': 4.964870374909576e-05, 'epoch': 1.73}
{'loss': 0.0125, 'grad_norm': 0.04733605682849884, 'learning_rate': 4.944620853764143e-05, 'epoch': 1.93}
{'eval_loss': 0.047209955751895905, 'eval_runtime': 42.4442, 'eval_samples_per_second': 2.073, 'eval_steps_per_second': 1.037, 'epoch': 2.0}
{'loss': 0.0112, 'grad_norm': 0.05110706388950348, 'learning_rate': 4.919844457560161e-05, 'epoch': 2.12}
{'loss': 0.0106, 'grad_norm': 0.06223692372441292, 'learning_rate': 4.8905870664797644e-05, 'epoch': 2.31}
{'loss': 0.0092, 'grad_norm': 0.04499354958534241, 'learning_rate': 4.8569028584760524e-05, 'epoch': 2.5}
{'loss': 0.0104, 'grad_norm': 0.05472031608223915, 'learning_rate': 4.818854208947991e-05, 'epoch': 2.7}
{'loss': 0.0082, 'grad_norm': 0.05818236619234085, 'learning_rate': 4.776511575235541e-05, 'epoch': 2.89}
{'eval_loss': 0.03744664043188095, 'eval_runtime': 43.7666, 'eval_samples_per_second': 2.011, 'eval_steps_per_second': 1.005, 'epoch': 3.0}
{'loss': 0.0083, 'grad_norm': 0.039988841861486435, 'learning_rate': 4.7299533661488714e-05, 'epoch': 3.08}
{'loss': 0.0067, 'grad_norm': 0.04577566683292389, 'learning_rate': 4.679265796773305e-05, 'epoch': 3.27}
{'loss': 0.0074, 'grad_norm': 0.0776434913277626, 'learning_rate': 4.6245427288188205e-05, 'epoch': 3.46}
{'loss': 0.0074, 'grad_norm': 0.05479898676276207, 'learning_rate': 4.565885496809774e-05, 'epoch': 3.66}
{'loss': 0.0056, 'grad_norm': 0.05265765264630318, 'learning_rate': 4.5034027204366916e-05, 'epoch': 3.85}
{'eval_loss': 0.03199879080057144, 'eval_runtime': 43.5942, 'eval_samples_per_second': 2.019, 'eval_steps_per_second': 1.009, 'epoch': 4.0}
{'loss': 0.0066, 'grad_norm': 0.04347189515829086, 'learning_rate': 4.4372101034176015e-05, 'epoch': 4.04}
{'loss': 0.0058, 'grad_norm': 0.0501786433160305, 'learning_rate': 4.367430219241388e-05, 'epoch': 4.23}
{'loss': 0.0038, 'grad_norm': 0.04283589869737625, 'learning_rate': 4.294192284189905e-05, 'epoch': 4.43}
{'loss': 0.005, 'grad_norm': 0.04473351687192917, 'learning_rate': 4.2176319180591676e-05, 'epoch': 4.62}
{'loss': 0.0037, 'grad_norm': 0.050772424787282944, 'learning_rate': 4.1378908930227134e-05, 'epoch': 4.81}
{'loss': 0.0059, 'grad_norm': 0.0838245153427124, 'learning_rate': 4.0551168711021694e-05, 'epoch': 5.0}
{'eval_loss': 0.03160615637898445, 'eval_runtime': 44.7554, 'eval_samples_per_second': 1.966, 'eval_steps_per_second': 0.983, 'epoch': 5.0}
{'loss': 0.0041, 'grad_norm': 0.04667719826102257, 'learning_rate': 3.969463130731183e-05, 'epoch': 5.19}
{'loss': 0.0036, 'grad_norm': 0.050168052315711975, 'learning_rate': 3.88108828291905e-05, 'epoch': 5.39}
{'loss': 0.0033, 'grad_norm': 0.055804815143346786, 'learning_rate': 3.790155977539632e-05, 'epoch': 5.58}
{'loss': 0.0031, 'grad_norm': 0.03652273491024971, 'learning_rate': 3.6968346002894714e-05, 'epoch': 5.77}
{'loss': 0.0026, 'grad_norm': 0.043532729148864746, 'learning_rate': 3.601296960876237e-05, 'epoch': 5.97}
{'eval_loss': 0.03277791291475296, 'eval_runtime': 44.7623, 'eval_samples_per_second': 1.966, 'eval_steps_per_second': 0.983, 'epoch': 6.0}
{'loss': 0.0026, 'grad_norm': 0.031872764229774475, 'learning_rate': 3.503719973014933e-05, 'epoch': 6.15}
{'loss': 0.0022, 'grad_norm': 0.03291277959942818, 'learning_rate': 3.404284326824419e-05, 'epoch': 6.35}
{'loss': 0.0023, 'grad_norm': 0.04476465284824371, 'learning_rate': 3.3031741542309114e-05, 'epoch': 6.54}
{'loss': 0.0023, 'grad_norm': 0.08455467224121094, 'learning_rate': 3.2005766879980406e-05, 'epoch': 6.73}
{'loss': 0.0026, 'grad_norm': 0.03372346609830856, 'learning_rate': 3.0966819150148755e-05, 'epoch': 6.93}
{'eval_loss': 0.035276707261800766, 'eval_runtime': 43.2738, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 1.017, 'epoch': 7.0}
{'train_runtime': 3273.1359, 'train_samples_per_second': 1.893, 'train_steps_per_second': 0.238, 'train_loss': 0.053956355463334736, 'epoch': 7.0}

============================================================
TRAINING COMPLETED
============================================================
Runtime: 54.55 min
Peak memory: 6.848 GB
Training memory: 3.254 GB
============================================================


Saving model to /home/vault/iwi5/iwi5298h/models_image_text/nanonets/schmuck/run_20251021_000001_unsloth...
âœ… Model saved!

============================================================
PHASE 2: EVALUATION
============================================================

Starting evaluation with chunked loading...
Dataset: 89 samples, 5 chunks of size 20

Processing chunk 1/5
Processing 1/20 in chunk 1Processing 2/20 in chunk 1Processing 3/20 in chunk 1Processing 4/20 in chunk 1Processing 5/20 in chunk 1Processing 6/20 in chunk 1Processing 7/20 in chunk 1Processing 8/20 in chunk 1Processing 9/20 in chunk 1Processing 10/20 in chunk 1Processing 11/20 in chunk 1Processing 12/20 in chunk 1Processing 13/20 in chunk 1Processing 14/20 in chunk 1Processing 15/20 in chunk 1Processing 16/20 in chunk 1Processing 17/20 in chunk 1Processing 18/20 in chunk 1Processing 19/20 in chunk 1Processing 20/20 in chunk 1
Processing chunk 2/5
Processing 1/20 in chunk 2Processing 2/20 in chunk 2Processing 3/20 in chunk 2Processing 4/20 in chunk 2Processing 5/20 in chunk 2Processing 6/20 in chunk 2Processing 7/20 in chunk 2Processing 8/20 in chunk 2Processing 9/20 in chunk 2Processing 10/20 in chunk 2Processing 11/20 in chunk 2Processing 12/20 in chunk 2Processing 13/20 in chunk 2Processing 14/20 in chunk 2Processing 15/20 in chunk 2Processing 16/20 in chunk 2Processing 17/20 in chunk 2Processing 18/20 in chunk 2Processing 19/20 in chunk 2Processing 20/20 in chunk 2
Processing chunk 3/5
Processing 1/20 in chunk 3Processing 2/20 in chunk 3Processing 3/20 in chunk 3Processing 4/20 in chunk 3Processing 5/20 in chunk 3Processing 6/20 in chunk 3Processing 7/20 in chunk 3Processing 8/20 in chunk 3Processing 9/20 in chunk 3Processing 10/20 in chunk 3Processing 11/20 in chunk 3Processing 12/20 in chunk 3Processing 13/20 in chunk 3Processing 14/20 in chunk 3Processing 15/20 in chunk 3Processing 16/20 in chunk 3Processing 17/20 in chunk 3Processing 18/20 in chunk 3Processing 19/20 in chunk 3Processing 20/20 in chunk 3
Processing chunk 4/5
Processing 1/20 in chunk 4Processing 2/20 in chunk 4Processing 3/20 in chunk 4Processing 4/20 in chunk 4Processing 5/20 in chunk 4Processing 6/20 in chunk 4Processing 7/20 in chunk 4Processing 8/20 in chunk 4Processing 9/20 in chunk 4Processing 10/20 in chunk 4Processing 11/20 in chunk 4Processing 12/20 in chunk 4Processing 13/20 in chunk 4Processing 14/20 in chunk 4Processing 15/20 in chunk 4Processing 16/20 in chunk 4Processing 17/20 in chunk 4Processing 18/20 in chunk 4Processing 19/20 in chunk 4Processing 20/20 in chunk 4
Processing chunk 5/5
Processing 1/9 in chunk 5Processing 2/9 in chunk 5Processing 3/9 in chunk 5Processing 4/9 in chunk 5Processing 5/9 in chunk 5Processing 6/9 in chunk 5Processing 7/9 in chunk 5Processing 8/9 in chunk 5Processing 9/9 in chunk 5
Results saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/schmuck/run_20251021_000001_unsloth/cer_evaluation_results.txt

============================================================
FINAL RESULTS
============================================================
Model: Nanonets-OCR-s (finetuned)
Dataset: Schmuck Jewelry Catalog

Metrics:
  Avg CER: 0.0051 (0.51%)
  Weighted CER: 0.0052 (0.52%)
  Median: 0.0026 | Std: 0.0128

Accuracy:
  Perfect: 39/89 (43.82%)
  Parsing: 0/89 (0.00%)

Saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/schmuck/run_20251021_000001_unsloth
============================================================

Nano training completed at: Tue Oct 21 01:38:07 AM CEST 2025
=== JOB_STATISTICS ===
=== current date     : Tue Oct 21 01:38:07 AM CEST 2025
= Job-ID             : 1244353 on tinygpu
= Job-Name           : shcmuck_finetune
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_nano.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : rtx3080
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 10:00:00
= Elapsed runtime    : 01:38:32
= Total RAM usage    : 10.7 GiB of requested  GiB (%)   
= Node list          : tg080
= Subm/Elig/Start/End: 2025-10-20T23:59:34 / 2025-10-20T23:59:34 / 2025-10-20T23:59:35 / 2025-10-21T01:38:07
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           176.0G  1000.0G  1500.0G        N/A     784K   5,000K   7,500K        N/A    
    /home/hpc              97.9G   104.9G   209.7G        N/A  32,258      500K   1,000K        N/A    
    /home/vault           835.4G  1048.6G  2097.2G        N/A   4,134      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:3D:00.0, 1591326, 33 %, 15 %, 9864 MiB, 5904361 ms
