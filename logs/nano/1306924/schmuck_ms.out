### Starting TaskPrologue of job 1306924 on tg074 at Wed Nov  5 04:40:48 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Wed Nov  5 16:40:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0             25W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!

============================================================
NANONETS-OCR-S FINETUNING WITH UNSLOTH (MULTI-STAGE, STABLE)
============================================================
Model: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
Output: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_164125_unsloth_multistage
============================================================

============================================================
LOADING NANONETS-OCR-S WITH UNSLOTH
============================================================
Model path: /home/vault/iwi5/iwi5298h/models/Nanonets-OCR-s
âœ“ Model directory verified
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.10.7: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2_5_Vl does not support SDPA - switching to fast eager.

âœ… Model loaded successfully!
Applying LoRA adaptors...
Unsloth: Making `model.base_model.model.model` require gradients
âœ… LoRA applied successfully!
============================================================


============================================================
PHASE 1+2: MULTI-STAGE TRAINING
============================================================

Preparing datasets...
Found 413 valid samples out of 413 total
Found 88 valid samples out of 88 total
Training: 413 samples, Validation: 88 samples


===== STAGE 1: Warm-up training (no eval, teacher forcing) =====

Unsloth: Model does not have a default image size - using 512
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.352, 'grad_norm': 1.2247450351715088, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.19}
{'loss': 0.3433, 'grad_norm': 1.1405185461044312, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.39}
{'loss': 0.3095, 'grad_norm': 0.9753361344337463, 'learning_rate': 2.9e-06, 'epoch': 0.58}
{'loss': 0.2869, 'grad_norm': 0.643670916557312, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.77}
{'loss': 0.2208, 'grad_norm': 0.3846110999584198, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.97}
{'loss': 0.2018, 'grad_norm': 0.2503569424152374, 'learning_rate': 4.930057285201028e-06, 'epoch': 1.15}
{'loss': 0.1643, 'grad_norm': 0.2717222273349762, 'learning_rate': 4.652609029418389e-06, 'epoch': 1.35}
{'loss': 0.1527, 'grad_norm': 0.26863062381744385, 'learning_rate': 4.241377572657493e-06, 'epoch': 1.54}
{'loss': 0.12, 'grad_norm': 0.24044100940227509, 'learning_rate': 3.641573093484283e-06, 'epoch': 1.74}
{'loss': 0.1036, 'grad_norm': 0.21995626389980316, 'learning_rate': 2.9422256878064326e-06, 'epoch': 1.93}
{'loss': 0.0949, 'grad_norm': 0.2574421167373657, 'learning_rate': 2.204317072696586e-06, 'epoch': 2.12}
{'loss': 0.079, 'grad_norm': 0.2602991759777069, 'learning_rate': 1.49219142678007e-06, 'epoch': 2.31}
{'loss': 0.0686, 'grad_norm': 0.2992241382598877, 'learning_rate': 8.679447045236964e-07, 'epoch': 2.5}
{'loss': 0.0633, 'grad_norm': 0.2818811237812042, 'learning_rate': 3.860099912454346e-07, 'epoch': 2.7}
{'loss': 0.0608, 'grad_norm': 0.29738837480545044, 'learning_rate': 8.841104522910343e-08, 'epoch': 2.89}
{'train_runtime': 1545.9968, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.101, 'train_loss': 0.17080256686760828, 'epoch': 3.0}

===== STAGE 2: Main training (eval each epoch, loss-based best model) =====

Unsloth: Model does not have a default image size - using 512

============================================================
GPU INFORMATION
============================================================
GPU: Tesla V100-PCIE-32GB
Max memory: 31.733 GB
Reserved: 5.439 GB

âœ… UNSLOTH OPTIMIZATIONS:
   â€¢ 4-bit quantization
   â€¢ LoRA rank 16 (RSLoRA)
   â€¢ Gradient checkpointing
   â€¢ Stage 2 Batch size: 1, Grad accum: 8
   â€¢ Max seq length: 1536
============================================================

Starting Stage 2 training with eval loss-based best model...
{'loss': 0.0581, 'grad_norm': 0.2749629020690918, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.19}
{'loss': 0.0489, 'grad_norm': 0.24347932636737823, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.39}
{'loss': 0.0343, 'grad_norm': 0.20231877267360687, 'learning_rate': 5.8e-06, 'epoch': 0.58}
{'loss': 0.0254, 'grad_norm': 0.1758091002702713, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.77}
{'loss': 0.0155, 'grad_norm': 0.09284008294343948, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.97}
{'eval_loss': 0.08979267627000809, 'eval_runtime': 54.1186, 'eval_samples_per_second': 1.626, 'eval_steps_per_second': 1.626, 'epoch': 1.0}
{'loss': 0.0111, 'grad_norm': 0.03699775040149689, 'learning_rate': 9.995207892430525e-06, 'epoch': 1.15}
{'loss': 0.0095, 'grad_norm': 0.031207023188471794, 'learning_rate': 9.975755697461254e-06, 'epoch': 1.35}
{'loss': 0.0114, 'grad_norm': 0.03307424113154411, 'learning_rate': 9.945505455453983e-06, 'epoch': 1.54}
{'loss': 0.009, 'grad_norm': 0.027022775262594223, 'learning_rate': 9.897827329701834e-06, 'epoch': 1.74}
{'loss': 0.0084, 'grad_norm': 0.027240000665187836, 'learning_rate': 9.83548120008529e-06, 'epoch': 1.93}
{'eval_loss': 0.06544509530067444, 'eval_runtime': 51.5112, 'eval_samples_per_second': 1.708, 'eval_steps_per_second': 1.708, 'epoch': 2.0}
{'loss': 0.0084, 'grad_norm': 0.028035970404744148, 'learning_rate': 9.758653780673381e-06, 'epoch': 2.12}
{'loss': 0.0082, 'grad_norm': 0.031169405207037926, 'learning_rate': 9.667575154073962e-06, 'epoch': 2.31}
{'loss': 0.0071, 'grad_norm': 0.02254633605480194, 'learning_rate': 9.562518082382751e-06, 'epoch': 2.5}
{'loss': 0.0079, 'grad_norm': 0.02674933150410652, 'learning_rate': 9.443797190315938e-06, 'epoch': 2.7}
{'loss': 0.0072, 'grad_norm': 0.04462077468633652, 'learning_rate': 9.311768022972682e-06, 'epoch': 2.89}
{'eval_loss': 0.055153489112854004, 'eval_runtime': 51.7012, 'eval_samples_per_second': 1.702, 'eval_steps_per_second': 1.702, 'epoch': 3.0}
{'loss': 0.0077, 'grad_norm': 0.025809861719608307, 'learning_rate': 9.181889190900702e-06, 'epoch': 3.08}
{'loss': 0.0065, 'grad_norm': 0.02678021788597107, 'learning_rate': 9.025695617750848e-06, 'epoch': 3.27}
{'loss': 0.0068, 'grad_norm': 0.03495444729924202, 'learning_rate': 8.857445899109716e-06, 'epoch': 3.46}
{'loss': 0.007, 'grad_norm': 0.02515740506350994, 'learning_rate': 8.696128877719258e-06, 'epoch': 3.66}
{'loss': 0.0054, 'grad_norm': 0.024428727105259895, 'learning_rate': 8.506389282385242e-06, 'epoch': 3.85}
{'eval_loss': 0.04821266978979111, 'eval_runtime': 51.8494, 'eval_samples_per_second': 1.697, 'eval_steps_per_second': 1.697, 'epoch': 4.0}
{'loss': 0.0063, 'grad_norm': 0.032787978649139404, 'learning_rate': 8.306148759174036e-06, 'epoch': 4.04}
{'loss': 0.0061, 'grad_norm': 0.030757209286093712, 'learning_rate': 8.096006988019331e-06, 'epoch': 4.23}
{'loss': 0.0045, 'grad_norm': 0.02411358430981636, 'learning_rate': 7.876593301093104e-06, 'epoch': 4.43}
{'loss': 0.0058, 'grad_norm': 0.029683712869882584, 'learning_rate': 7.648564798082842e-06, 'epoch': 4.62}
{'loss': 0.0045, 'grad_norm': 0.028826238587498665, 'learning_rate': 7.412604378310677e-06, 'epoch': 4.81}
{'loss': 0.0068, 'grad_norm': 0.06392420083284378, 'learning_rate': 7.194041804142556e-06, 'epoch': 5.0}
{'eval_loss': 0.04351021349430084, 'eval_runtime': 51.375, 'eval_samples_per_second': 1.713, 'eval_steps_per_second': 1.713, 'epoch': 5.0}
{'loss': 0.0056, 'grad_norm': 0.02656261809170246, 'learning_rate': 6.9449755025391355e-06, 'epoch': 5.19}
{'loss': 0.0056, 'grad_norm': 0.03173409029841423, 'learning_rate': 6.715813990494793e-06, 'epoch': 5.39}
{'loss': 0.0046, 'grad_norm': 0.02859387919306755, 'learning_rate': 6.456332763888544e-06, 'epoch': 5.58}
{'loss': 0.0042, 'grad_norm': 0.023298749700188637, 'learning_rate': 6.192490114722741e-06, 'epoch': 5.77}
{'loss': 0.0039, 'grad_norm': 0.028122419491410255, 'learning_rate': 5.925076198455517e-06, 'epoch': 5.97}
{'eval_loss': 0.04046516865491867, 'eval_runtime': 51.6165, 'eval_samples_per_second': 1.705, 'eval_steps_per_second': 1.705, 'epoch': 6.0}
{'loss': 0.0044, 'grad_norm': 0.026539232581853867, 'learning_rate': 5.65489186576885e-06, 'epoch': 6.15}
{'loss': 0.0043, 'grad_norm': 0.024904781952500343, 'learning_rate': 5.38274626418248e-06, 'epoch': 6.35}
{'loss': 0.0042, 'grad_norm': 0.03107524663209915, 'learning_rate': 5.109454414820475e-06, 'epoch': 6.54}
{'loss': 0.0043, 'grad_norm': 0.02542644739151001, 'learning_rate': 4.835834771587537e-06, 'epoch': 6.74}
{'loss': 0.0052, 'grad_norm': 0.022511152550578117, 'learning_rate': 4.562706770064824e-06, 'epoch': 6.93}
{'eval_loss': 0.03867575153708458, 'eval_runtime': 51.585, 'eval_samples_per_second': 1.706, 'eval_steps_per_second': 1.706, 'epoch': 7.0}
{'loss': 0.0039, 'grad_norm': 0.027696775272488594, 'learning_rate': 4.31798803888375e-06, 'epoch': 7.12}
{'loss': 0.0037, 'grad_norm': 0.024699309840798378, 'learning_rate': 4.048044454176658e-06, 'epoch': 7.31}
{'loss': 0.0042, 'grad_norm': 0.025276198983192444, 'learning_rate': 3.7809517841067976e-06, 'epoch': 7.5}
{'loss': 0.0047, 'grad_norm': 0.026765016838908195, 'learning_rate': 3.543667236111458e-06, 'epoch': 7.7}
{'loss': 0.0036, 'grad_norm': 0.026570606976747513, 'learning_rate': 3.2841860095052096e-06, 'epoch': 7.89}
{'eval_loss': 0.03767186775803566, 'eval_runtime': 51.8125, 'eval_samples_per_second': 1.698, 'eval_steps_per_second': 1.698, 'epoch': 8.0}
{'loss': 0.0039, 'grad_norm': 0.028435546904802322, 'learning_rate': 3.0298432993367577e-06, 'epoch': 8.08}
{'loss': 0.0032, 'grad_norm': 0.02602497674524784, 'learning_rate': 2.8059581958574434e-06, 'epoch': 8.27}
{'loss': 0.0038, 'grad_norm': 0.023628614842891693, 'learning_rate': 2.563462606428101e-06, 'epoch': 8.46}
{'loss': 0.0038, 'grad_norm': 0.025726590305566788, 'learning_rate': 2.32826395449754e-06, 'epoch': 8.66}
{'loss': 0.0043, 'grad_norm': 0.03459003567695618, 'learning_rate': 2.1010666125360767e-06, 'epoch': 8.85}
{'eval_loss': 0.03697795048356056, 'eval_runtime': 52.0705, 'eval_samples_per_second': 1.69, 'eval_steps_per_second': 1.69, 'epoch': 9.0}
{'loss': 0.0035, 'grad_norm': 0.021884040907025337, 'learning_rate': 1.8825509907063328e-06, 'epoch': 9.04}
{'loss': 0.0036, 'grad_norm': 0.027031227946281433, 'learning_rate': 1.6733714991721738e-06, 'epoch': 9.23}
{'loss': 0.0041, 'grad_norm': 0.02167183719575405, 'learning_rate': 1.4936107176147606e-06, 'epoch': 9.43}
{'loss': 0.0042, 'grad_norm': 0.024366891011595726, 'learning_rate': 1.303871122280742e-06, 'epoch': 9.62}
{'loss': 0.0033, 'grad_norm': 0.025680681690573692, 'learning_rate': 1.1252006865989868e-06, 'epoch': 9.81}
{'loss': 0.0034, 'grad_norm': 0.06214265152812004, 'learning_rate': 9.58134492446543e-07, 'epoch': 10.0}
{'eval_loss': 0.03653134033083916, 'eval_runtime': 51.7648, 'eval_samples_per_second': 1.7, 'eval_steps_per_second': 1.7, 'epoch': 10.0}
{'loss': 0.004, 'grad_norm': 0.022522838786244392, 'learning_rate': 8.181108090993001e-07, 'epoch': 10.19}
{'loss': 0.0032, 'grad_norm': 0.018690019845962524, 'learning_rate': 6.744411496531045e-07, 'epoch': 10.39}
{'loss': 0.0044, 'grad_norm': 0.041856780648231506, 'learning_rate': 5.437256655302814e-07, 'epoch': 10.58}
{'loss': 0.0029, 'grad_norm': 0.020364265888929367, 'learning_rate': 4.263558232112064e-07, 'epoch': 10.77}
{'loss': 0.0032, 'grad_norm': 0.024156853556632996, 'learning_rate': 3.2268312167385687e-07, 'epoch': 10.97}
{'eval_loss': 0.03639607131481171, 'eval_runtime': 51.9558, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 1.694, 'epoch': 11.0}
{'train_runtime': 6197.8062, 'train_samples_per_second': 0.8, 'train_steps_per_second': 0.101, 'train_loss': 0.008109663936335568, 'epoch': 11.0}

============================================================
TRAINING COMPLETED (Stage 2)
============================================================
Runtime: 103.3 min
Peak memory: 5.439 GB
Training memory: 0.0 GB
============================================================


Saving model to /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_164125_unsloth_multistage...
âœ… Model saved!

============================================================
PHASE 3: EVALUATION (CHUNKED)
============================================================

Starting evaluation with chunked loading...
Dataset: 89 samples, 5 chunks of size 20

Processing chunk 1/5
Processing 1/20 in chunk 1Processing 2/20 in chunk 1Processing 3/20 in chunk 1Processing 4/20 in chunk 1Processing 5/20 in chunk 1Processing 6/20 in chunk 1Processing 7/20 in chunk 1Processing 8/20 in chunk 1Processing 9/20 in chunk 1Processing 10/20 in chunk 1Processing 11/20 in chunk 1Processing 12/20 in chunk 1Processing 13/20 in chunk 1Processing 14/20 in chunk 1Processing 15/20 in chunk 1Processing 16/20 in chunk 1Processing 17/20 in chunk 1Processing 18/20 in chunk 1Processing 19/20 in chunk 1Processing 20/20 in chunk 1
Processing chunk 2/5
Processing 1/20 in chunk 2Processing 2/20 in chunk 2Processing 3/20 in chunk 2Processing 4/20 in chunk 2Processing 5/20 in chunk 2Processing 6/20 in chunk 2Processing 7/20 in chunk 2Processing 8/20 in chunk 2Processing 9/20 in chunk 2Processing 10/20 in chunk 2Processing 11/20 in chunk 2Processing 12/20 in chunk 2Processing 13/20 in chunk 2Processing 14/20 in chunk 2Processing 15/20 in chunk 2Processing 16/20 in chunk 2Processing 17/20 in chunk 2Processing 18/20 in chunk 2Processing 19/20 in chunk 2Processing 20/20 in chunk 2
Processing chunk 3/5
Processing 1/20 in chunk 3Processing 2/20 in chunk 3Processing 3/20 in chunk 3Processing 4/20 in chunk 3Processing 5/20 in chunk 3Processing 6/20 in chunk 3Processing 7/20 in chunk 3Processing 8/20 in chunk 3Processing 9/20 in chunk 3Processing 10/20 in chunk 3Processing 11/20 in chunk 3Processing 12/20 in chunk 3Processing 13/20 in chunk 3Processing 14/20 in chunk 3Processing 15/20 in chunk 3Processing 16/20 in chunk 3Processing 17/20 in chunk 3Processing 18/20 in chunk 3Processing 19/20 in chunk 3Processing 20/20 in chunk 3
Processing chunk 4/5
Processing 1/20 in chunk 4Processing 2/20 in chunk 4Processing 3/20 in chunk 4Processing 4/20 in chunk 4Processing 5/20 in chunk 4Processing 6/20 in chunk 4Processing 7/20 in chunk 4Processing 8/20 in chunk 4Processing 9/20 in chunk 4Processing 10/20 in chunk 4Processing 11/20 in chunk 4Processing 12/20 in chunk 4Processing 13/20 in chunk 4Processing 14/20 in chunk 4Processing 15/20 in chunk 4Processing 16/20 in chunk 4Processing 17/20 in chunk 4Processing 18/20 in chunk 4Processing 19/20 in chunk 4Processing 20/20 in chunk 4
Processing chunk 5/5
Processing 1/9 in chunk 5Processing 2/9 in chunk 5Processing 3/9 in chunk 5Processing 4/9 in chunk 5Processing 5/9 in chunk 5Processing 6/9 in chunk 5Processing 7/9 in chunk 5Processing 8/9 in chunk 5Processing 9/9 in chunk 5
Results saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_164125_unsloth_multistage/cer_evaluation_results.txt

============================================================
FINAL RESULTS
============================================================
Model: Nanonets-OCR-s (finetuned, best by eval loss)
Dataset: Schmuck Jewelry Catalog

Metrics:
  Avg CER: 0.1061 (10.61%)
  Weighted CER: 0.1044 (10.44%)
  Median: 0.0051 | Std: 0.5303

Accuracy:
  Perfect: 16/89 (17.98%)
  Parsing: 0/89 (0.00%)

Saved to: /home/vault/iwi5/iwi5298h/models_image_text/nanonets/multistage/schmuck/run_20251105_164125_unsloth_multistage
============================================================

Nano training completed at: Wed Nov  5 07:38:13 PM CET 2025
=== JOB_STATISTICS ===
=== current date     : Wed Nov  5 07:38:13 PM CET 2025
= Job-ID             : 1306924 on tinygpu
= Job-Name           : schmuck_ms
= Job-Command        : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles/job_nano.sh
= Initial workdir    : /home/hpc/iwi5/iwi5298h/Uddipan-Thesis/jobfiles
= Queue/Partition    : v100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 02:57:29
= Total RAM usage    : 11.2 GiB of requested  GiB (%)   
= Node list          : tg074
= Subm/Elig/Start/End: 2025-11-05T16:41:06 / 2025-11-05T16:41:06 / 2025-11-05T16:41:07 / 2025-11-05T19:38:36
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc             102.1G   104.9G   209.7G        N/A  30,695      500K   1,000K        N/A    
    /home/woody           184.1G  1000.0G  1500.0G        N/A     841K   5,000K   7,500K        N/A    
    /home/vault           902.3G  1048.6G  2097.2G        N/A   5,769      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 2306111, 33 %, 14 %, 32488 MiB, 10634486 ms
