#!/usr/bin/env python3
# florence2_staircase_zeroshot.py
# Zero-shot Florence-2-large OCR on the Staircase dataset
# Uses the SAME CER / JSON logic as florence2_schmuck_zeroshot_FIXED.py

import sys
import os

# ‚úÖ Force unbuffered output
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
import json
from typing import List, Dict
import jiwer
from datetime import datetime

# ---------------------------------------------------------------------
# CONFIG ‚Äì CHANGE THESE TWO PATHS FOR YOUR STAIRCASE DATA
# ---------------------------------------------------------------------
TEST_JSONL_PATH = "/home/woody/iwi5/iwi5298h/json_staircase/test.jsonl"  # staircase test.jsonl
IMAGES_DIR = "/home/woody/iwi5/iwi5298h/staircase_images"               # staircase image folder

OUTPUT_ROOT = "/home/vault/iwi5/iwi5298h/models_image_text/florence2/shots_stair"
MODEL_PATH = "/home/vault/iwi5/iwi5298h/models/florence2_large"
DATASET_NAME = "Staircase Dataset"
APPROACH_NAME = "zero_shot_ocr"

# Florence task prompt ‚Äì as requested
TASK_PROMPT = "<OCR>"


def extract_text_values_from_json(json_data: Dict) -> str:
    """
    Extract all text values from JSON data for CER calculation.
    (Currently not used, but kept for parity with Schmuck logic.)
    """
    values = []

    for key, value in json_data.items():
        # üîÅ For staircase we skip image_name, not file_name
        if key == "image_name":
            continue
        if value:
            values.append(str(value))

    return " ".join(values).strip()


def json_to_string(json_obj: Dict) -> str:
    """Convert JSON object to compact string (without image_name)."""
    clean_obj = {k: v for k, v in json_obj.items() if k != "image_name"}
    return json.dumps(clean_obj, ensure_ascii=False, separators=(",", ":"))


class Florence2StaircaseOCRProcessor:
    def __init__(self, model_name: str = MODEL_PATH):
        """Initialize the OCR processor with Florence-2-large model."""
        print("=" * 60)
        print("Loading Florence-2-large model from local cache...")
        print("=" * 60)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        print(f"Device: {self.device}")
        print(f"Dtype: {self.torch_dtype}")

        print("‚è≥ Loading model (this may take a bit)...")

        # You may see a warning: torch_dtype is deprecated, use dtype instead.
        # If you want to silence it, replace torch_dtype= with dtype=.
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=self.torch_dtype,
            trust_remote_code=True,
            attn_implementation="eager",
        ).to(self.device)

        print("‚è≥ Loading processor...")

        self.processor = AutoProcessor.from_pretrained(
            model_name,
            trust_remote_code=True,
        )

        model_params = sum(p.numel() for p in self.model.parameters()) / 1e9
        print(f"‚úÖ Model loaded successfully!")
        print(f"Model parameters: {model_params:.2f}B")
        print("=" * 60 + "\n")

    def extract_ocr_text(self, florence_output) -> str:
        """Extract clean OCR text from Florence-2's structured output."""
        if isinstance(florence_output, dict):
            if "<OCR>" in florence_output:
                return florence_output["<OCR>"].strip()
            elif "text" in florence_output:
                return florence_output["text"].strip()
            else:
                return str(florence_output).strip()
        elif isinstance(florence_output, str):
            return florence_output.strip()
        else:
            return str(florence_output).strip()

    def process_image_zero_shot(self, image_path: str) -> str:
        """Process a single image with zero-shot OCR using Florence-2."""
        try:
            # Load the image
            image = Image.open(image_path).convert("RGB")

            # Florence-2 uses specific task prompts for OCR
            task_prompt = TASK_PROMPT  # "<OCR>"

            # Proper processor usage
            inputs = self.processor(
                text=task_prompt,
                images=image,
                return_tensors="pt",
            )

            # Move inputs to device properly
            input_ids = inputs["input_ids"].to(self.device)
            pixel_values = inputs["pixel_values"].to(self.device, self.torch_dtype)

            # Check if pixel_values is valid
            if pixel_values is None or pixel_values.numel() == 0:
                return "Error: Failed to process image - empty pixel values"

            # ‚úÖ use_cache=False to avoid KV cache errors
            with torch.no_grad():
                generated_ids = self.model.generate(
                    input_ids=input_ids,
                    pixel_values=pixel_values,
                    max_new_tokens=1024,
                    num_beams=3,
                    do_sample=False,
                    use_cache=False,  # critical fix
                    early_stopping=False,
                )

            # Decode the generated text
            generated_text = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=False,
            )[0]

            # Post-process the generation
            parsed_answer = self.processor.post_process_generation(
                generated_text,
                task=task_prompt,
                image_size=(image.width, image.height),
            )

            # Extract clean text
            clean_text = self.extract_ocr_text(parsed_answer)

            return clean_text

        except Exception as e:
            import traceback

            error_msg = f"Error: {str(e)}"
            print(f"  ‚ùå Error in Florence-2 processing: {e}")
            traceback.print_exc()
            return error_msg


def load_jsonl(file_path: str) -> List[Dict]:
    """Load data from a JSONL file."""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line.strip()))
    return data


def save_jsonl(data: List[Dict], file_path: str):
    """Save data to a JSONL file."""
    with open(file_path, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


def main():
    """Main function to process Staircase images with Florence-2 zero-shot OCR."""
    print("=" * 60)
    print("FLORENCE-2 LARGE ZERO-SHOT OCR EVALUATION")
    print(DATASET_NAME)
    print("=" * 60 + "\n")

    # Paths
    test_jsonl_path = TEST_JSONL_PATH
    images_dir = IMAGES_DIR

    # Create output directory with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(
        OUTPUT_ROOT,
        f"run_zeroshot_{timestamp}",
    )
    os.makedirs(output_dir, exist_ok=True)
    print(f"üìÅ Output directory: {output_dir}\n")

    predictions_output_path = os.path.join(output_dir, "test_predictions_zeroshot.jsonl")

    # Load test data
    print("Loading test data...")
    try:
        test_data = load_jsonl(test_jsonl_path)
        print(f"‚úÖ Loaded {len(test_data)} test samples\n")
    except Exception as e:
        print(f"‚ùå Error loading test data: {e}")
        return

    # Initialize OCR processor
    ocr_processor = Florence2StaircaseOCRProcessor()

    # Process test images
    predictions = []
    all_cer_scores = []
    json_parse_failures = 0

    print(f"{'=' * 60}")
    print(f"Processing {len(test_data)} test images with Florence-2 zero-shot OCR...")
    print(f"{'=' * 60}\n")

    for i, test_item in enumerate(test_data):
        # üîÅ Use image_name instead of file_name
        image_name = test_item["image_name"]

        print(f"\n{'‚îÄ' * 60}")
        print(f"[{i+1}/{len(test_data)}] {image_name}")
        print(f"{'‚îÄ' * 60}")

        # Construct image path
        image_path = os.path.join(images_dir, image_name)

        if not os.path.exists(image_path):
            print(f"  ‚ö†Ô∏è  Image not found: {image_path}")
            continue

        try:
            # Process image with zero-shot OCR
            print(f"  üîÑ Processing with Florence-2...")
            predicted_text = ocr_processor.process_image_zero_shot(image_path)

            # Try to parse as JSON
            predicted_json = None
            json_parse_success = False

            try:
                predicted_json = json.loads(predicted_text.strip())
                json_parse_success = True
                print(f"  ‚úÖ JSON parsed successfully")
            except json.JSONDecodeError:
                print(f"  ‚ö†Ô∏è  JSON parse failed - using raw output for CER")
                json_parse_failures += 1

            # Get ground truth as JSON string (without image_name)
            gt_json_string = json_to_string(test_item)

            # Calculate CER based on what we have
            if json_parse_success and predicted_json:
                # If JSON parsed successfully, convert to string for comparison
                pred_json_string = json_to_string({"dummy": "x", **predicted_json})
                cer_score = jiwer.cer(gt_json_string, pred_json_string)
            else:
                # If JSON parse failed, compare raw output with GT JSON string
                cer_score = jiwer.cer(gt_json_string, predicted_text.strip())

            # Store prediction
            prediction_entry = {
                "image_name": image_name,
                "predicted_text": predicted_text.strip(),
                "predicted_json": predicted_json if json_parse_success else None,
                "json_parse_success": json_parse_success,
                "ground_truth_json_string": gt_json_string,
                "ground_truth_fields": {
                    k: v for k, v in test_item.items() if k != "image_name"
                },
                "cer_score": cer_score,
            }
            predictions.append(prediction_entry)
            all_cer_scores.append(cer_score)

            print(f"  ‚úÖ Processed successfully!")
            print(f"  üìä CER: {cer_score:.4f} ({cer_score*100:.2f}%)")
            print(
                f"  üìè Lengths: GT={len(gt_json_string)}, Pred={len(predicted_text.strip())}"
            )

            # Show preview
            preview = (
                predicted_text.strip()[:100] + "..."
                if len(predicted_text.strip()) > 100
                else predicted_text.strip()
            )
            print(f"  üìù Preview: {preview}")

            # Running average
            running_avg = sum(all_cer_scores) / len(all_cer_scores)
            print(
                f"  üìà Running Avg CER: {running_avg:.4f} ({running_avg*100:.2f}%)"
            )

        except Exception as e:
            print(f"  ‚ùå Error: {str(e)}")
            gt_json_string = json_to_string(test_item)
            prediction_entry = {
                "image_name": image_name,
                "predicted_text": f"Error: {str(e)}",
                "predicted_json": None,
                "json_parse_success": False,
                "ground_truth_json_string": gt_json_string,
                "ground_truth_fields": {
                    k: v for k, v in test_item.items() if k != "image_name"
                },
                "cer_score": 1.0,
            }
            predictions.append(prediction_entry)
            all_cer_scores.append(1.0)
            continue

    print(f"\n{'=' * 60}")
    print(f"Processing complete!")
    print(f"{'=' * 60}\n")

    # Save predictions
    print(f"üíæ Saving predictions to {predictions_output_path}...")
    try:
        save_jsonl(predictions, predictions_output_path)
        print(f"‚úÖ Predictions saved successfully!")
    except Exception as e:
        print(f"‚ùå Error saving predictions: {e}")
        return

    # Calculate and print CER statistics
    if all_cer_scores:
        print("\n" + "=" * 60)
        print("FLORENCE-2 ZERO-SHOT EVALUATION RESULTS")
        print(DATASET_NAME)
        print("=" * 60)

        avg_cer = sum(all_cer_scores) / len(all_cer_scores)
        min_cer = min(all_cer_scores)
        max_cer = max(all_cer_scores)

        print(f"\nCER Statistics across {len(all_cer_scores)} images:")
        print("-" * 50)
        print(f"Average CER:  {avg_cer:.4f} ({avg_cer*100:.2f}%)")
        print(f"Minimum CER:  {min_cer:.4f} ({min_cer*100:.2f}%)")
        print(f"Maximum CER:  {max_cer:.4f} ({max_cer*100:.2f}%)")

        # Calculate weighted CER
        total_chars = 0
        total_errors = 0

        for pred in predictions:
            gt_string = pred["ground_truth_json_string"]
            char_count = len(gt_string)
            cer_score = pred["cer_score"]

            total_chars += char_count
            total_errors += int(cer_score * char_count)

        weighted_cer = total_errors / total_chars if total_chars > 0 else 0
        print(f"\nWeighted CER:  {weighted_cer:.4f} ({weighted_cer*100:.2f}%)")
        print(f"Total characters: {total_chars}")
        print(f"Total errors:     {total_errors}")

        perfect_matches = sum(1 for cer in all_cer_scores if cer == 0.0)
        json_success_rate = (
            (len(predictions) - json_parse_failures) / len(predictions) * 100
            if len(predictions) > 0
            else 0
        )

        print(
            f"\nPerfect matches: {perfect_matches}/{len(all_cer_scores)} "
            f"({perfect_matches/len(all_cer_scores)*100:.1f}%)"
        )
        print(
            f"JSON parse success: {len(predictions) - json_parse_failures}/"
            f"{len(predictions)} ({json_success_rate:.1f}%)"
        )
        print(f"JSON parse failures: {json_parse_failures}")

        # Save summary
        summary_file = os.path.join(output_dir, "evaluation_summary_zeroshot.json")
        summary_data = {
            "timestamp": timestamp,
            "model_name": "Florence-2-large",
            "model_path": MODEL_PATH,
            "dataset": DATASET_NAME,
            "approach": APPROACH_NAME,
            "total_images": len(predictions),
            "average_cer": float(avg_cer),
            "minimum_cer": float(min_cer),
            "maximum_cer": float(max_cer),
            "weighted_cer": float(weighted_cer),
            "perfect_matches": int(perfect_matches),
            "json_parse_success_count": len(predictions) - json_parse_failures,
            "json_parse_failures": json_parse_failures,
            "json_success_rate": float(json_success_rate),
            "total_characters": int(total_chars),
            "total_errors": int(total_errors),
            "note": "When JSON parse fails, CER is calculated between raw output and ground truth JSON string",
        }

        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ Summary saved to: {summary_file}")
        print(f"‚úÖ Predictions saved to: {predictions_output_path}")

        print("\n" + "=" * 60)
        print("EVALUATION COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print(f"Model:   Florence-2-large")
        print(f"Dataset: {DATASET_NAME}")
        print(f"Method:  Zero-Shot OCR")
        print(f"Images:  {len(predictions)} processed")
        print(f"Avg CER: {avg_cer:.4f} ({avg_cer*100:.2f}%)")
        print(f"JSON success rate: {json_success_rate:.1f}%")
        print(f"\nAll outputs saved to: {output_dir}")
        print("=" * 60 + "\n")

    else:
        print("\n‚ùå No images were successfully processed.")


if __name__ == "__main__":
    main()
