<p align="center">
  <h1 align="center">Comparative Analysis of Vision Language Models<br>for Structured OCR on Historical Documents</h1>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/release/python-3110/"><img src="https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white" alt="Python 3.11"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?logo=pytorch&logoColor=white" alt="PyTorch 2.0+"></a>
  <a href="https://huggingface.co/docs/transformers"><img src="https://img.shields.io/badge/ü§ó_Transformers-4.47+-orange" alt="Transformers"></a>
  <a href="https://github.com/unslothai/unsloth"><img src="https://img.shields.io/badge/Unsloth-QLoRA-green" alt="Unsloth"></a>
  <a href="https://optuna.org/"><img src="https://img.shields.io/badge/Optuna-HPO-blueviolet?logo=optuna" alt="Optuna"></a>
  <a href="https://www.fau.eu/"><img src="https://img.shields.io/badge/FAU-HPC-lightgrey" alt="FAU HPC"></a>
</p>

<p align="center">
  A systematic benchmarking framework comparing <strong>eight Vision Language Models (VLMs)</strong> on their ability to extract structured JSON output from scanned historical German documents. Experiments span zero-shot, few-shot, fine-tuning, hyperparameter optimization, multi-stage training, multi-dataset training, and image preprocessing studies ‚Äî all evaluated with CER, ANLS*, and mAP-F1 metrics.
</p>

---

## üìë Table of Contents

- [Introduction](#-introduction)
- [Models](#-models)
- [Datasets](#-datasets)
- [Experiment Pipeline](#-experiment-pipeline)
- [Project Structure](#-project-structure)
- [Setup](#-setup)
- [Data Label Creation](#-data-label-creation)
- [Running Experiments](#-running-experiments)
- [Evaluation Metrics](#-evaluation-metrics)
- [Interactive UI](#-interactive-ui)
- [HPC Job Scheduling](#-hpc-job-scheduling)
- [Built With](#-built-with)

---

## üìå Introduction

This project is a **Master's thesis** conducted at [Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg (FAU)](https://www.fau.eu/) investigating how well modern Vision Language Models can perform **OCR on historical German documents** and return the extracted information as **structured JSON**.

Three document collections are used:

| Dataset | Description | Language |
|---|---|---|
| **Inventory** | Historical museum inventory book entries | German (printed + handwritten) |
| **Schmuck** | Jewellery catalogue cards | German (printed + handwritten) |
| **Staircase** | Architectural staircase survey forms | German (printed + handwritten, checkboxes) |

Each model is prompted (or fine-tuned) to read a scanned document image and produce a single JSON object whose keys and nesting match a predefined schema ‚Äî no free-form text, no markdown, just valid JSON.

---

## ü§ñ Models

Eight VLMs are benchmarked across multiple experimental settings:

| # | Model | Type | Quantization | Training Framework |
|---|---|---|---|---|
| 1 | **Donut** (`naver-clova-ix/donut-base`) | Encoder‚ÄìDecoder | Full precision | HuggingFace Transformers |
| 2 | **Florence-2** (`microsoft/Florence-2-large`) | Encoder‚ÄìDecoder | Full precision | HuggingFace Transformers |
| 3 | **PaddleOCR-VL** (`PaddlePaddle/PaddleOCR-VL`) | Decoder-only | bfloat16 | HuggingFace Transformers |
| 4 | **Gemma-3-4B-IT** (`google/gemma-3-4b-it`) | Decoder-only | 4-bit QLoRA (Unsloth) | Unsloth + TRL SFTTrainer |
| 5 | **Nanonets-OCR-s** | Decoder-only | 4-bit QLoRA (Unsloth) | Unsloth + TRL SFTTrainer |
| 6 | **Phi-3.5-Vision** (`microsoft/Phi-3.5-vision-instruct`) | Decoder-only | 4-bit QLoRA (BitsAndBytes) | HuggingFace Trainer + PEFT |
| 7 | **Qwen2.5-VL-7B** (`Qwen/Qwen2.5-VL-7B-Instruct`) | Decoder-only | 4-bit QLoRA (Unsloth) | Unsloth + TRL SFTTrainer |
| 8 | **Nanonets** (few-shot capable) | Decoder-only | 4-bit QLoRA (Unsloth) | Unsloth + TRL SFTTrainer |

---

## üìä Datasets

All datasets follow the same format:

- **Images**: Scanned document pages (`.jpg` / `.png`)
- **Labels**: JSONL files (`train.jsonl`, `val.jsonl`, `test.jsonl`) where each line is a JSON object containing an `image_name` field and the structured ground-truth fields.

Example ground-truth (Inventory dataset):

```json
{
  "image_name": "inventarbuch-042.jpg",
  "√úberschrift": "Inventar des Museums",
  "Inventarnummer": "J= 1234",
  "Ma√üe": { "L": "12 cm", "B": "8 cm", "D": "3 cm" },
  "Objektbezeichnung": "Bronzefibel",
  "Fundort": "Erlangen, Burgberg",
  "Fundzeit": "1923",
  "Beschreibungstext": "Gut erhaltene Fibel mit Spiralkonstruktion..."
}
```

---

## üî¨ Experiment Pipeline

Each model is evaluated through a progressive pipeline of experiments:

```
Zero-Shot ‚îÄ‚îÄ‚ñ∫ Few-Shot ‚îÄ‚îÄ‚ñ∫ Fine-Tuning (base hyperparams)
                                ‚îÇ
                                ‚îú‚îÄ‚îÄ‚ñ∫ HPO (Optuna) ‚îÄ‚îÄ‚ñ∫ Fine-Tuning w/ Best HPs
                                ‚îÇ
                                ‚îú‚îÄ‚îÄ‚ñ∫ Image Preprocessing Study   (uses base hyperparams)
                                ‚îú‚îÄ‚îÄ‚ñ∫ Multi-Stage Training        (uses base hyperparams)
                                ‚îî‚îÄ‚îÄ‚ñ∫ Multi-Dataset Training      (uses base hyperparams)
```

All secondary studies (HPO, Image Preprocessing, Multi-Stage, and Multi-Dataset) are conducted **on top of the base fine-tuning code** using the initial (non-optimized) hyperparameters. Only the "Fine-Tuning w/ Best HPs" branch uses the optimized hyperparameters discovered by Optuna.

| Experiment | Description |
|---|---|
| **Zero-Shot** | Inference with no training; the model is prompted with the JSON schema and asked to extract fields from a document image. |
| **Few-Shot** | One or more labelled examples are included in the prompt as demonstrations before asking the model to process a new image. |
| **Fine-Tuning** | The model is fine-tuned on the training split using QLoRA (4-bit) or full-parameter training, with CER-based best-model selection on the validation set. All subsequent experiments branch from this baseline. |
| **HPO (Optuna)** | A secondary study on top of base fine-tuning. Uses Optuna with an SQLite backend to tune learning rate, LoRA rank/alpha/dropout, weight decay, gradient accumulation, and epochs. The scripts in the `hpo/` folder **only search for the best hyperparameters** ‚Äî they do not produce a final trained model. The discovered parameters are exported to a JSON file. |
| **Fine-Tuning with Best HPs** | Re-runs the base fine-tuning code using the optimized hyperparameters found by Optuna. The `*_finetune_hpo.py` files in the `finetune/` folder are identical to the base fine-tuning scripts but with the Optuna-discovered hyperparameters plugged in. This is the **only** experiment that uses non-default hyperparameters. |
| **Image Preprocessing** | A secondary study on top of base fine-tuning (using base hyperparams). Applies four image preprocessing steps (brightness, contrast, sharpness, rotation) during training to determine whether augmentation benefits VLMs. |
| **Multi-Stage Training** | A secondary study on top of base fine-tuning (using base hyperparams). Two-stage training: Stage 1 (warm-up) uses teacher forcing only with higher LR; Stage 2 uses evaluation-based best-model selection with generation CER. |
| **Multi-Dataset Training** | A secondary study on top of base fine-tuning (using base hyperparams). A single model and LoRA adapter is trained on the combined training data of all three datasets, then evaluated per-dataset. |

---

## üìÅ Project Structure

```
Uddipan-Thesis/
‚îÇ
‚îú‚îÄ‚îÄ data_labels_creation/               <- Ground-truth label generation
‚îÇ   ‚îú‚îÄ‚îÄ labelling_code1.py              <- Llama-4-Maverick via Groq API (few-shot labelling)
‚îÇ   ‚îú‚îÄ‚îÄ labelling_code2_tesseract.py    <- Llama-4-Maverick + Tesseract OCR pre-pass
‚îÇ   ‚îî‚îÄ‚îÄ inventory.py                    <- Gemini-3-Flash for inventory label extraction
‚îÇ
‚îú‚îÄ‚îÄ DONUT/                              <- Donut (naver-clova-ix/donut-base)
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Full fine-tuning scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inven_new_base.py           <- Inventory dataset fine-tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inven_new_cord.py           <- Inventory dataset finetune (using donut cord)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_base.py             <- Schmuck dataset fine-tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_cord.py             <- Schmuck dataset finetune(using donut cord)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_base_cord.py          <- Staircase dataset (donut Base +  donut cord)
‚îÇ   ‚îî‚îÄ‚îÄ zero_shot/                      <- Zero-shot inference scripts
‚îÇ       ‚îú‚îÄ‚îÄ zero_inven_new.py           <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_inven_cord.py          <- Inventory (CORD) zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck_base.py        <- Schmuck zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck_cord.py        <- Schmuck (CORD) zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_stair_base.py          <- Staircase zero-shot
‚îÇ       ‚îî‚îÄ‚îÄ zero_stair_cord.py          <- Staircase (CORD) zero-shot
‚îÇ
‚îú‚îÄ‚îÄ florence2/                          <- Florence-2 (microsoft/Florence-2-large)
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Full-parameter fine-tuning scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_finetune.py           <- Staircase dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune2.py          <- Staircase (variant)
‚îÇ   ‚îî‚îÄ‚îÄ zero_shots/                     <- Zero-shot inference scripts
‚îÇ       ‚îú‚îÄ‚îÄ zero_shot_inventory.py      <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck.py             <- Schmuck zero-shot
‚îÇ       ‚îî‚îÄ‚îÄ zero_stair.py              <- Staircase zero-shot
‚îÇ
‚îú‚îÄ‚îÄ paddleocr/                          <- PaddleOCR-VL
‚îÇ   ‚îú‚îÄ‚îÄ finetune_code/                  <- Fine-tuning scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune.py           <- Staircase dataset
‚îÇ   ‚îî‚îÄ‚îÄ zero_shot_code/                 <- Zero-shot inference scripts
‚îÇ       ‚îú‚îÄ‚îÄ inven_new_inference.py      <- Inventory inference
‚îÇ       ‚îú‚îÄ‚îÄ schmuck_inference.py        <- Schmuck inference
‚îÇ       ‚îî‚îÄ‚îÄ stair_inference.py          <- Staircase inference
‚îÇ
‚îú‚îÄ‚îÄ gemma3/                             <- Gemma-3-4B-IT (google/gemma-3-4b-it)
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Fine-tuning (base + HPO-optimized)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune_hpo.py   <- Inventory (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune_hpo.py     <- Schmuck (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_finetune.py           <- Staircase (base hyperparams)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune_hpo.py       <- Staircase (Optuna-optimized HPs)
‚îÇ   ‚îú‚îÄ‚îÄ hpo/                            <- Optuna HPO (outputs best HPs only, no final model)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_hpo.py            <- HPO for inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_hpo.py              <- HPO for schmuck dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_hpo.py                <- HPO for staircase dataset
‚îÇ   ‚îú‚îÄ‚îÄ image_preprocess/               <- Image preprocessing study
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_preprocess.py     <- Inventory + augmentation pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_preprocess.py       <- Schmuck + augmentation pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_preprocess.py         <- Staircase + augmentation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ multistage_and_multidataset/    <- Advanced training strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_multistage.py     <- Multi-stage training (inventory)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_multistage.py       <- Multi-stage training (schmuck)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_multistage.py         <- Multi-stage training (staircase)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multidataset_finetune.py    <- Combined multi-dataset training
‚îÇ   ‚îî‚îÄ‚îÄ zero_few_shots/                 <- Zero-shot & few-shot inference
‚îÇ       ‚îú‚îÄ‚îÄ zero_shot_inventory.py      <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_shot_stair.py          <- Staircase zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck.py             <- Schmuck zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_shot_inventory.py       <- Inventory few-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_shot_schmuck.py         <- Schmuck few-shot
‚îÇ       ‚îî‚îÄ‚îÄ few_shots_stair.py          <- Staircase few-shot
‚îÇ
‚îú‚îÄ‚îÄ nanonets/                           <- Nanonets-OCR-s
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Fine-tuning scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune.py           <- Staircase dataset
‚îÇ   ‚îî‚îÄ‚îÄ zero_few_shots/                 <- Zero-shot & few-shot inference
‚îÇ       ‚îú‚îÄ‚îÄ zero_inven.py               <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck.py             <- Schmuck zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_stair.py               <- Staircase zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_inven.py                <- Inventory few-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_schmuck.py              <- Schmuck few-shot
‚îÇ       ‚îî‚îÄ‚îÄ few_stair.py                <- Staircase few-shot
‚îÇ
‚îú‚îÄ‚îÄ phivision/                          <- Phi-3.5-Vision (microsoft/Phi-3.5-vision-instruct)
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Fine-tuning (base + HPO-optimized)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune_hpo.py   <- Inventory (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune_hpo.py     <- Schmuck (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_finetune.py           <- Staircase (base hyperparams)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune_hpo.py       <- Staircase (Optuna-optimized HPs)
‚îÇ   ‚îú‚îÄ‚îÄ hpo/                            <- Optuna HPO (outputs best HPs only, no final model)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_hpo.py            <- HPO for inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_hpo.py              <- HPO for schmuck dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_with_hpo.py           <- HPO for staircase dataset
‚îÇ   ‚îú‚îÄ‚îÄ image_preprocess/               <- Image preprocessing study
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_preprocess.py     <- Inventory + augmentation pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_preprocess.py       <- Schmuck + augmentation pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_preprocess.py         <- Staircase + augmentation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ multistage_and_multidataset/    <- Advanced training strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_multistage.py     <- Multi-stage training (inventory)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_multistage.py       <- Multi-stage training (schmuck)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_multistage.py         <- Multi-stage training (staircase)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multidataset_finetune.py    <- Combined multi-dataset training
‚îÇ   ‚îî‚îÄ‚îÄ shots/                          <- Zero-shot & few-shot inference
‚îÇ       ‚îú‚îÄ‚îÄ zero_inventory.py           <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck.py             <- Schmuck zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_stair.py               <- Staircase zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_inventory.py            <- Inventory few-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_schmuck.py              <- Schmuck few-shot
‚îÇ       ‚îî‚îÄ‚îÄ few_stair.py                <- Staircase few-shot
‚îÇ
‚îú‚îÄ‚îÄ qwen/                              <- Qwen2.5-VL-7B (Qwen/Qwen2.5-VL-7B-Instruct)
‚îÇ   ‚îú‚îÄ‚îÄ finetune/                       <- Fine-tuning (base + HPO-optimized)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune.py       <- Inventory (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_finetune_hpo.py   <- Inventory (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune.py         <- Schmuck (base hyperparams)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_finetune_hpo.py     <- Schmuck (Optuna-optimized HPs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staircase_finetune.py       <- Staircase (base hyperparams)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_finetune_hpo.py       <- Staircase (Optuna-optimized HPs)
‚îÇ   ‚îú‚îÄ‚îÄ hpo/                            <- Optuna HPO (outputs best HPs only, no final model)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_hpo.py            <- HPO for inventory dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_hpo.py              <- HPO for schmuck dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_hpo.py                <- HPO for staircase dataset
‚îÇ   ‚îú‚îÄ‚îÄ image_preprocess/               <- Image preprocessing study
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_preprocess.py     <- Inventory + augmentation pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_preprocess.py       <- Schmuck + augmentation pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stair_preprocess.py         <- Staircase + augmentation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ multistage_and_multidataset/    <- Advanced training strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_multistage.py     <- Multi-stage training (inventory)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schmuck_multistage.py       <- Multi-stage training (schmuck)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stair_multistage.py         <- Multi-stage training (staircase)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multidataset_finetune.py    <- Combined multi-dataset training
‚îÇ   ‚îî‚îÄ‚îÄ zero_and_few_shot/              <- Zero-shot & few-shot inference
‚îÇ       ‚îú‚îÄ‚îÄ zero_shot_inventory.py      <- Inventory zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zeroshot_stair.py           <- Staircase zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ zero_schmuck.py             <- Schmuck zero-shot
‚îÇ       ‚îú‚îÄ‚îÄ few_shots_inventory.py      <- Inventory few-shot
‚îÇ       ‚îú‚îÄ‚îÄ fewshots_schmuck.py         <- Schmuck few-shot
‚îÇ       ‚îî‚îÄ‚îÄ staircase_few_shot.py       <- Staircase few-shot
‚îÇ
‚îú‚îÄ‚îÄ map_f1_and_anls/                    <- Evaluation metric scripts
‚îÇ   ‚îú‚îÄ‚îÄ anls_all.py                     <- ANLS* metric computation (anls_star library)
‚îÇ   ‚îî‚îÄ‚îÄ map_f1_all.py                   <- mAP-style Precision/Recall/F1 via per-field CER
‚îÇ
‚îú‚îÄ‚îÄ UI_codes/                           <- Interactive Gradio + FastAPI demo application
‚îÇ   ‚îú‚îÄ‚îÄ phi_ui.py                       <- OCR inference UI (model selection, JSON + Markdown output)
‚îÇ   ‚îú‚îÄ‚îÄ image1.png                      <- UI screenshot 1
‚îÇ   ‚îî‚îÄ‚îÄ image2.png                      <- UI screenshot 2
‚îÇ
‚îú‚îÄ‚îÄ jobfiles/                           <- SLURM job scripts for FAU HPC cluster
‚îÇ   ‚îú‚îÄ‚îÄ job_gemma.sh, job_phi.sh, ...   <- Training/inference job submissions
‚îÇ   ‚îú‚îÄ‚îÄ GOT/                            <- GOT-OCR related job files
‚îÇ   ‚îú‚îÄ‚îÄ no_post_process/                <- Jobs without post-processing
‚îÇ   ‚îú‚îÄ‚îÄ post_process/                   <- Jobs with post-processing
‚îÇ   ‚îî‚îÄ‚îÄ utils/                          <- Utility job scripts (JSON conversion, UI, saliency)
‚îÇ
‚îú‚îÄ‚îÄ logs/                               <- SLURM output logs (stdout/stderr per job ID)
‚îÇ   ‚îú‚îÄ‚îÄ DONUT/                          <- Donut experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ Florence2/                      <- Florence-2 experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ gemma/                          <- Gemma-3 experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ nano/                           <- Nanonets experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ NANONETS/                       <- Nanonets (additional) logs
‚îÇ   ‚îú‚îÄ‚îÄ paddle/                         <- PaddleOCR-VL experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ phi/                            <- Phi-3.5-Vision experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ QWEN/                           <- Qwen2.5-VL experiment logs
‚îÇ   ‚îú‚îÄ‚îÄ UI/                             <- UI-related logs
‚îÇ   ‚îî‚îÄ‚îÄ utils/                          <- Utility script logs
‚îÇ
‚îú‚îÄ‚îÄ optuna db/                          <- Optuna study database
‚îÇ   ‚îî‚îÄ‚îÄ vlmmodels.db                    <- SQLite DB for all HPO trials
‚îÇ
‚îú‚îÄ‚îÄ yml_env_files/                      <- Conda environment files
‚îÇ   ‚îú‚îÄ‚îÄ gemma_vision.yml                <- Gemma-3 environment
‚îÇ   ‚îú‚îÄ‚îÄ phi_ocr.yml                     <- Phi-3.5-Vision environment
‚îÇ   ‚îú‚îÄ‚îÄ qwen_vision.yml                 <- Qwen2.5-VL environment
‚îÇ   ‚îú‚îÄ‚îÄ florence2_env.yml               <- Florence-2 environment
‚îÇ   ‚îú‚îÄ‚îÄ donut118.yml                    <- Donut environment
‚îÇ   ‚îú‚îÄ‚îÄ paddleocr.yml                   <- PaddleOCR-VL environment
‚îÇ   ‚îú‚îÄ‚îÄ nanonets.yml                    <- Nanonets environment
‚îÇ   ‚îú‚îÄ‚îÄ got_ocr.yml                     <- GOT-OCR environment
‚îÇ   ‚îú‚îÄ‚îÄ uddipan_thesis.yml              <- General thesis environment
‚îÇ   ‚îî‚îÄ‚îÄ ...                             <- Additional environment files
‚îÇ
‚îî‚îÄ‚îÄ README.md                           <- This file
```

---

## üöÄ Setup

### Prerequisites

- Python 3.11+
- CUDA-enabled GPU (NVIDIA A100 / V100 recommended)
- Conda (Miniconda or Anaconda)

### Environment Installation

Each model has its own Conda environment file in `yml_env_files/`. Create the environment for the model you want to run:

```bash
# Example: Gemma-3
conda env create -f yml_env_files/gemma_vision.yml
conda activate gemma_vision

# Example: Phi-3.5-Vision
conda env create -f yml_env_files/phi_ocr.yml
conda activate phi_ocr

# Example: Qwen2.5-VL
conda env create -f yml_env_files/qwen_vision.yml
conda activate qwen_vision

# Example: Donut
conda env create -f yml_env_files/donut118.yml
conda activate donut118

# Example: Florence-2
conda env create -f yml_env_files/florence2_env.yml
conda activate florence2_env

# Example: PaddleOCR-VL
conda env create -f yml_env_files/paddleocr.yml
conda activate paddleocr

# Example: Nanonets
conda env create -f yml_env_files/nanonets.yml
conda activate nanonets
```

### Data Preparation

Organize your data in the following format:

```
your_data_dir/
‚îú‚îÄ‚îÄ train.jsonl       <- Training split (one JSON object per line)
‚îú‚îÄ‚îÄ val.jsonl         <- Validation split
‚îú‚îÄ‚îÄ test.jsonl        <- Test split
‚îî‚îÄ‚îÄ images/           <- Document images referenced by image_name in JSONL
    ‚îú‚îÄ‚îÄ doc-001.jpg
    ‚îú‚îÄ‚îÄ doc-002.jpg
    ‚îî‚îÄ‚îÄ ...
```

---

## üè∑Ô∏è Data Label Creation

Ground-truth labels were generated semi-automatically using two LLM-based pipelines:

| Script | Model | Method |
|---|---|---|
| `labelling_code1.py` | **Llama-4-Maverick-17B** (via Groq API) | Few-shot prompting with example images encoded as base64 |
| `labelling_code2_tesseract.py` | **Llama-4-Maverick-17B** + **Tesseract OCR** | Two-pass: Tesseract extracts raw text first, then Llama structures it into JSON |
| `inventory.py` | **Gemini-3-Flash** (Google GenAI API) | Direct image-to-JSON extraction with structured prompt |

All labels were manually reviewed and corrected after automatic generation.

---

## ‚ñ∂Ô∏è Running Experiments

### Zero-Shot / Few-Shot Inference

```bash
# Activate the appropriate environment
conda activate gemma_vision

# Run zero-shot inference
python gemma3/zero_few_shots/zero_shot_inventory.py

# Run few-shot inference
python gemma3/zero_few_shots/few_shot_inventory.py
```

### Fine-Tuning

```bash
# Base fine-tuning
python gemma3/finetune/inventory_finetune.py

# Fine-tuning with Optuna-optimized hyperparameters
python gemma3/finetune/inventory_finetune_hpo.py
```

### Hyperparameter Optimization (Optuna)

```bash
# Run HPO ‚Äî results are saved to optuna db/vlmmodels.db
python gemma3/hpo/inventory_hpo.py
```

The HPO script uses Optuna with an SQLite backend (`vlmmodels.db`) to tune:
- Learning rate, weight decay, max gradient norm
- LoRA rank (`r`), alpha
- Gradient accumulation steps
- Warmup ratio

Best hyperparameters are exported to a JSON file in the run output directory.

### Image Preprocessing Study

```bash
python gemma3/image_preprocess/inventory_preprocess.py
```

Applies four preprocessing during training to check if VLM benefits:
- Illumination flattening (grayscale)
- Gentle denoising
- CLAHE on luminance
- Letterbox resize to 1024√ó1024

### Multi-Stage Training

```bash
python gemma3/multistage_and_multidataset/inventory_multistage.py
```

Two-stage training:
1. **Stage 1 (Warm-up)**: Teacher forcing only, higher learning rate, no evaluation checkpoints
2. **Stage 2 (Main)**: Evaluation each epoch with autoregressive generation CER, best model saved

### Multi-Dataset Training

```bash
python gemma3/multistage_and_multidataset/multidataset_finetune.py
```

Trains a **single shared model + LoRA adapter** on the combined training data of all three datasets (Inventory + Schmuck + Staircase), then evaluates each dataset independently.

---

## üìè Evaluation Metrics

All models are evaluated using three complementary metrics:

| Metric | Script | Description |
|---|---|---|
| **CER** (Character Error Rate) | Built into each training script | Primary metric for best-model selection during training. Computed via autoregressive generation on validation subset using `jiwer` and `editdistance`. |
| **ANLS\*** | `map_f1_and_anls/anls_all.py` | Universal document processing metric from [Peer et al., 2024](https://arxiv.org/abs/2402.03848). Compares predicted vs. ground-truth JSON dicts field-by-field using the `anls_star` library. |
| **mAP-F1** | `map_f1_and_anls/map_f1_all.py` | mAP-style Precision / Recall / F1 using per-field CER overlap at multiple thresholds (0.8, 0.7, 0.5, 0.3). Penalizes models that fail to produce valid JSON. |

### Running Evaluation

```bash
# Edit INPUT_JSONL_PATH and OUTPUT_PATH in the script, then:
python map_f1_and_anls/anls_all.py
python map_f1_and_anls/map_f1_all.py
```

Both scripts accept any prediction JSONL file (with universal key detection) and produce a detailed metrics JSON report.

---

## üñ•Ô∏è Interactive UI

A **Gradio + FastAPI** web application is provided for interactive OCR inference:

<p align="center">
  <img src="UI_codes/image1.png" width="90%" alt="UI Screenshot 1">
</p>

<p align="center">
  <img src="UI_codes/image2.png" width="90%" alt="UI Screenshot 2">
</p>

### Features

- **Model Selection**: Dropdown to choose between available fine-tuned models
- **Dual Output**: Results displayed as both raw JSON and a formatted Markdown table
- **Latency Tracking**: Inference time measured and displayed per request
- **In-Memory Caching**: Repeated images return cached results instantly
- **REST API**: FastAPI endpoints alongside the Gradio UI for programmatic access

### Running the UI

```bash
conda activate phi_ocr
python UI_codes/phi_ui.py

# Or via uvicorn:
uvicorn UI_codes.phi_ui:app --host 0.0.0.0 --port 8000
```

Access the UI at: `http://localhost:8000/ui`

---

## üñß HPC Job Scheduling

All experiments were run on the **FAU HPC cluster** using SLURM. Job scripts are in `jobfiles/`:

```bash
# Submit a training job
sbatch jobfiles/job_gemma.sh

# Submit an inference job
sbatch jobfiles/job_gemma_stair.sh
```

Each `.sh` file specifies:
- GPU resources (A100 / A40)
- Conda environment activation
- Python script execution with appropriate paths
- Output log redirection to `logs/`

---

## üß∞ Built With

- [PyTorch](https://pytorch.org/) ‚Äî Deep learning framework
- [HuggingFace Transformers](https://huggingface.co/docs/transformers) ‚Äî Model loading, tokenization, and training
- [Unsloth](https://github.com/unslothai/unsloth) ‚Äî Memory-efficient 4-bit QLoRA fine-tuning for Gemma, Qwen, and Nanonets
- [PEFT](https://github.com/huggingface/peft) ‚Äî Parameter-Efficient Fine-Tuning (LoRA adapters for Phi-3.5-Vision)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) ‚Äî 4-bit quantization
- [TRL](https://github.com/huggingface/trl) ‚Äî SFTTrainer for supervised fine-tuning
- [Optuna](https://optuna.org/) ‚Äî Hyperparameter optimization with SQLite storage
- [Gradio](https://gradio.app/) ‚Äî Interactive web UI
- [FastAPI](https://fastapi.tiangolo.com/) ‚Äî REST API backend for the UI
- [jiwer](https://github.com/jitsi/jiwer) ‚Äî CER computation
- [anls_star](https://github.com/deepopinion/anls_star_metric) ‚Äî ANLS* metric
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) ‚Äî OCR pre-pass for label generation
- [Google GenAI](https://ai.google.dev/) ‚Äî Gemini API for label generation
- [Groq](https://groq.com/) ‚Äî Llama-4-Maverick API for label generation
- [FAU HPC](https://hpc.fau.de/) ‚Äî High-performance computing cluster at FAU Erlangen-N√ºrnberg

---

## üë§ Author

**Uddipan Basu Bir**

- Email: [uddipan.bir.basu@fau.de](mailto:uddipan.bir.basu@fau.de) ¬∑ [uddipanbb95@gmail.com](mailto:uddipanbb95@gmail.com)
- GitHub: [@uddipan77](https://github.com/uddipan77)

---

## ‚ö†Ô∏è Model Weights & Fine-tuned Checkpoints

This repository contains **only the source code, scripts, and configuration files**. The fine-tuned model weights, LoRA adapters, and trained checkpoints are **not included** due to their large size.

If you need access to the fine-tuned weights or trained model checkpoints, please contact:

> **Uddipan Basu Bir** ‚Äî [uddipan.bir.basu@fau.de](mailto:uddipan.bir.basu@fau.de) / [uddipanbb95@gmail.com](mailto:uddipanbb95@gmail.com)

---

<p align="center">
  <sub>Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg ‚Äî Master's Thesis, 2026</sub>
</p>
